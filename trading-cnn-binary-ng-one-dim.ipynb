{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sheet I will apply Convolutional Neural Networks for stock prediction of Apple stock price. I will divide data from last 15 years into chunks. Every day contains one opening value. Predicted output has two classes: whether stock will go up or down or stay constant X days after last input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TIME = 200\n",
    "PREDICTION_AFTER_DAYS = 20\n",
    "EPOCHS = 150\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SPLIT = False\n",
    "NORMALIZED_CHUNKS = True\n",
    "PREDICTION_LABEL = 'Open'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** Selecting test/valiations chunks randomly from the whole set of observation chunks seems to be a big mistake. It causes situation where network actually seen chunks from the future, therefore the more overfitted network, the better results will be. Network should be trained incrementally - with data from time x to x+1 and validated with data from x+1 to x+2. Then learned with data from x to x+2 and validated with data from x+3 to x+3 and so on..\n",
    "\n",
    "Parameter RANDOM_SPLIT is responsible for that feature. If True, then data set is split randomly into train/test set (network knows sets from the future). When False, then split is chronological - network works like in real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-05-18</td>\n",
       "      <td>5.064286</td>\n",
       "      <td>5.365714</td>\n",
       "      <td>4.998571</td>\n",
       "      <td>5.120000</td>\n",
       "      <td>4.432371</td>\n",
       "      <td>159180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-05-19</td>\n",
       "      <td>5.111429</td>\n",
       "      <td>5.382857</td>\n",
       "      <td>5.111429</td>\n",
       "      <td>5.364286</td>\n",
       "      <td>4.643848</td>\n",
       "      <td>198290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-05-20</td>\n",
       "      <td>5.321429</td>\n",
       "      <td>5.378572</td>\n",
       "      <td>5.312857</td>\n",
       "      <td>5.364286</td>\n",
       "      <td>4.643848</td>\n",
       "      <td>113162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-05-23</td>\n",
       "      <td>5.407143</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.407143</td>\n",
       "      <td>5.680000</td>\n",
       "      <td>4.917161</td>\n",
       "      <td>260643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-05-24</td>\n",
       "      <td>5.635714</td>\n",
       "      <td>5.712857</td>\n",
       "      <td>5.575714</td>\n",
       "      <td>5.671429</td>\n",
       "      <td>4.909742</td>\n",
       "      <td>148365000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close     Volume\n",
       "0  2005-05-18  5.064286  5.365714  4.998571  5.120000   4.432371  159180700\n",
       "1  2005-05-19  5.111429  5.382857  5.111429  5.364286   4.643848  198290400\n",
       "2  2005-05-20  5.321429  5.378572  5.312857  5.364286   4.643848  113162700\n",
       "3  2005-05-23  5.407143  5.700000  5.407143  5.680000   4.917161  260643600\n",
       "4  2005-05-24  5.635714  5.712857  5.575714  5.671429   4.909742  148365000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/AAPL.csv')\n",
    "df.reindex(index=df.index[::-1])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "def percentage_to_label(percentage_value):\n",
    "    return 0 if percentage_value < 0 else 1\n",
    "\n",
    "def investor_observes_stocks_for(x_days=60, then_buy_stocks=True, and_sells_them=True, after_y_days=10, dataframe=df):\n",
    "    assert then_buy_stocks\n",
    "    assert and_sells_them\n",
    "    observe_buy_sell_process_length = x_days + after_y_days\n",
    "    \n",
    "    observed_chunks = []\n",
    "    observation_results = []\n",
    "    \n",
    "    for first_day_of_observation in range(len(dataframe) - observe_buy_sell_process_length):\n",
    "        buyout_day = first_day_of_observation + x_days\n",
    "        sell_day = buyout_day + after_y_days\n",
    "        \n",
    "        observed_chunk = dataframe[first_day_of_observation:buyout_day].reset_index()\n",
    "        observation_result = dataframe.iloc[sell_day]\n",
    "        \n",
    "        closing_price_on_buyout_day = dataframe.iloc[buyout_day]['Close']\n",
    "        opening_price_on_sell_day = dataframe.iloc[sell_day]['Open']\n",
    "        \n",
    "        relative_price_change_as_percentage = (opening_price_on_sell_day - closing_price_on_buyout_day) / closing_price_on_buyout_day * 100\n",
    "        \n",
    "        observed_chunks += [observed_chunk]\n",
    "        observation_results += [percentage_to_label(relative_price_change_as_percentage)]\n",
    "    \n",
    "    return observed_chunks, observation_results\n",
    "\n",
    "observed_chunks, observation_results = investor_observes_stocks_for(x_days=OBSERVATION_TIME, then_buy_stocks=True, and_sells_them=True, after_y_days=PREDICTION_AFTER_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2005-05-18</td>\n",
       "      <td>5.064286</td>\n",
       "      <td>5.365714</td>\n",
       "      <td>4.998571</td>\n",
       "      <td>5.120000</td>\n",
       "      <td>4.432371</td>\n",
       "      <td>159180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2005-05-19</td>\n",
       "      <td>5.111429</td>\n",
       "      <td>5.382857</td>\n",
       "      <td>5.111429</td>\n",
       "      <td>5.364286</td>\n",
       "      <td>4.643848</td>\n",
       "      <td>198290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2005-05-20</td>\n",
       "      <td>5.321429</td>\n",
       "      <td>5.378572</td>\n",
       "      <td>5.312857</td>\n",
       "      <td>5.364286</td>\n",
       "      <td>4.643848</td>\n",
       "      <td>113162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2005-05-23</td>\n",
       "      <td>5.407143</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.407143</td>\n",
       "      <td>5.680000</td>\n",
       "      <td>4.917161</td>\n",
       "      <td>260643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2005-05-24</td>\n",
       "      <td>5.635714</td>\n",
       "      <td>5.712857</td>\n",
       "      <td>5.575714</td>\n",
       "      <td>5.671429</td>\n",
       "      <td>4.909742</td>\n",
       "      <td>148365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>2006-02-27</td>\n",
       "      <td>10.284286</td>\n",
       "      <td>10.302857</td>\n",
       "      <td>10.092857</td>\n",
       "      <td>10.141429</td>\n",
       "      <td>8.779409</td>\n",
       "      <td>197810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>2006-02-28</td>\n",
       "      <td>10.225715</td>\n",
       "      <td>10.342857</td>\n",
       "      <td>9.728572</td>\n",
       "      <td>9.784286</td>\n",
       "      <td>8.470230</td>\n",
       "      <td>316745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>9.834286</td>\n",
       "      <td>9.927143</td>\n",
       "      <td>9.717143</td>\n",
       "      <td>9.871428</td>\n",
       "      <td>8.545672</td>\n",
       "      <td>190954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>2006-03-02</td>\n",
       "      <td>9.855714</td>\n",
       "      <td>9.998571</td>\n",
       "      <td>9.810000</td>\n",
       "      <td>9.944285</td>\n",
       "      <td>8.608742</td>\n",
       "      <td>156318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>2006-03-03</td>\n",
       "      <td>9.914286</td>\n",
       "      <td>9.987143</td>\n",
       "      <td>9.647142</td>\n",
       "      <td>9.674286</td>\n",
       "      <td>8.375002</td>\n",
       "      <td>184417100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index        Date       Open       High        Low      Close  Adj Close  \\\n",
       "0        0  2005-05-18   5.064286   5.365714   4.998571   5.120000   4.432371   \n",
       "1        1  2005-05-19   5.111429   5.382857   5.111429   5.364286   4.643848   \n",
       "2        2  2005-05-20   5.321429   5.378572   5.312857   5.364286   4.643848   \n",
       "3        3  2005-05-23   5.407143   5.700000   5.407143   5.680000   4.917161   \n",
       "4        4  2005-05-24   5.635714   5.712857   5.575714   5.671429   4.909742   \n",
       "..     ...         ...        ...        ...        ...        ...        ...   \n",
       "195    195  2006-02-27  10.284286  10.302857  10.092857  10.141429   8.779409   \n",
       "196    196  2006-02-28  10.225715  10.342857   9.728572   9.784286   8.470230   \n",
       "197    197  2006-03-01   9.834286   9.927143   9.717143   9.871428   8.545672   \n",
       "198    198  2006-03-02   9.855714   9.998571   9.810000   9.944285   8.608742   \n",
       "199    199  2006-03-03   9.914286   9.987143   9.647142   9.674286   8.375002   \n",
       "\n",
       "        Volume  \n",
       "0    159180700  \n",
       "1    198290400  \n",
       "2    113162700  \n",
       "3    260643600  \n",
       "4    148365000  \n",
       "..         ...  \n",
       "195  197810200  \n",
       "196  316745100  \n",
       "197  190954400  \n",
       "198  156318400  \n",
       "199  184417100  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    2248\n",
       "0    1307\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(observation_results)\n",
    "plt.show()\n",
    "\n",
    "pd.Series(observation_results).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "\n",
    "def normalize_chunk(chunk):\n",
    "    df = chunk.copy()\n",
    "    for label_to_normalize in ['Open', 'High', 'Low', 'Close', 'Adj Close']:\n",
    "        np_arr = df[label_to_normalize].to_numpy()\n",
    "        df[label_to_normalize] = pd.Series((np.array(np_arr) - np.mean(np_arr)) / np.std(np_arr))\n",
    "    return df\n",
    "\n",
    "def normalize_chunks(chunks):\n",
    "    return list(map(lambda x: normalize_chunk(x), chunks))\n",
    "\n",
    "observed_chunks = normalize_chunks(observed_chunks) if NORMALIZED_CHUNKS else observed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2844, 200, 1), (2844,), (711, 200, 1), (711,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN TEST SPLIT\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(list(map(lambda df: df[[PREDICTION_LABEL]].to_numpy(), observed_chunks)))\n",
    "Y = np.array(observation_results).astype('float32')\n",
    "\n",
    "def chronological_split(X_data, Y_data, test_size=0.25):\n",
    "    training_test_split_index = int((1 - test_size) * len(X_data))\n",
    "    X_train = X_data[:training_test_split_index]\n",
    "    Y_train = Y_data[:training_test_split_index]\n",
    "    X_test = X_data[training_test_split_index:]\n",
    "    Y_test = Y_data[training_test_split_index:]\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "def random_split(X_data, Y_data, test_size=0.25):\n",
    "    return train_test_split(X_data, Y_data, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = random_split(X, Y, TEST_SIZE) if RANDOM_SPLIT else chronological_split(X, Y, TEST_SIZE)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0    1074\n",
       "1.0    1074\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNDERSAMPLING TRAINING SET\n",
    "\n",
    "import random\n",
    "\n",
    "def undersample_and_shuffle(X, Y):\n",
    "    count_class_1, count_class_0 = pd.Series(Y).value_counts()\n",
    "\n",
    "    lower_count = count_class_0 if count_class_0 < count_class_1 else count_class_1\n",
    "    lower_class = 0 if count_class_0 < count_class_1 else 1\n",
    "    \n",
    "    diff = abs(count_class_0 - count_class_1)\n",
    "    \n",
    "    vec0 = random.sample([(x, y) for x, y in zip(X, Y) if y == 0], lower_count)\n",
    "    vec1 = random.sample([(x, y) for x, y in zip(X, Y) if y == 1], lower_count)\n",
    "\n",
    "    \n",
    "    vec = vec0 + vec1\n",
    "    random.shuffle(vec)\n",
    "    return np.array(list(map(lambda x: x[0], vec))), np.array(list(map(lambda x: x[1], vec)))\n",
    "    \n",
    "X_train_balanced, Y_train_balanced = undersample_and_shuffle(X_train, Y_train)\n",
    "\n",
    "plt.hist(Y_train_balanced)\n",
    "plt.show()\n",
    "\n",
    "pd.Series(Y_train_balanced).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 200, 16)           96        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 16)           64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 200, 8)            648       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200, 8)            32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 200, 8)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 8)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 103,625\n",
      "Trainable params: 103,449\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (OBSERVATION_TIME, 1), filters=16, kernel_size=5, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution1D(filters=8, kernel_size=5, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2148 samples, validate on 711 samples\n",
      "Epoch 1/150\n",
      "2148/2148 [==============================] - 1s 529us/step - loss: 0.8339 - tp: 754.0000 - fp: 716.0000 - tn: 358.0000 - fn: 320.0000 - accuracy: 0.5177 - precision: 0.5129 - recall: 0.7020 - auc: 0.5204 - val_loss: 0.6941 - val_tp: 273.0000 - val_fp: 174.0000 - val_tn: 59.0000 - val_fn: 205.0000 - val_accuracy: 0.4669 - val_precision: 0.6107 - val_recall: 0.5711 - val_auc: 0.4376\n",
      "Epoch 2/150\n",
      "2148/2148 [==============================] - 0s 189us/step - loss: 0.7747 - tp: 685.0000 - fp: 611.0000 - tn: 463.0000 - fn: 389.0000 - accuracy: 0.5345 - precision: 0.5285 - recall: 0.6378 - auc: 0.5472 - val_loss: 0.7042 - val_tp: 288.0000 - val_fp: 180.0000 - val_tn: 53.0000 - val_fn: 190.0000 - val_accuracy: 0.4796 - val_precision: 0.6154 - val_recall: 0.6025 - val_auc: 0.4514\n",
      "Epoch 3/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.7251 - tp: 652.0000 - fp: 499.0000 - tn: 575.0000 - fn: 422.0000 - accuracy: 0.5712 - precision: 0.5665 - recall: 0.6071 - auc: 0.5933 - val_loss: 0.7285 - val_tp: 164.0000 - val_fp: 54.0000 - val_tn: 179.0000 - val_fn: 314.0000 - val_accuracy: 0.4824 - val_precision: 0.7523 - val_recall: 0.3431 - val_auc: 0.5077\n",
      "Epoch 4/150\n",
      "2148/2148 [==============================] - 0s 161us/step - loss: 0.6963 - tp: 633.0000 - fp: 482.0000 - tn: 592.0000 - fn: 441.0000 - accuracy: 0.5703 - precision: 0.5677 - recall: 0.5894 - auc: 0.6067 - val_loss: 0.7355 - val_tp: 213.0000 - val_fp: 141.0000 - val_tn: 92.0000 - val_fn: 265.0000 - val_accuracy: 0.4290 - val_precision: 0.6017 - val_recall: 0.4456 - val_auc: 0.4684\n",
      "Epoch 5/150\n",
      "2148/2148 [==============================] - 0s 170us/step - loss: 0.6836 - tp: 648.0000 - fp: 451.0000 - tn: 623.0000 - fn: 426.0000 - accuracy: 0.5917 - precision: 0.5896 - recall: 0.6034 - auc: 0.6315 - val_loss: 0.7775 - val_tp: 114.0000 - val_fp: 34.0000 - val_tn: 199.0000 - val_fn: 364.0000 - val_accuracy: 0.4402 - val_precision: 0.7703 - val_recall: 0.2385 - val_auc: 0.5087\n",
      "Epoch 6/150\n",
      "2148/2148 [==============================] - 0s 172us/step - loss: 0.6698 - tp: 656.0000 - fp: 422.0000 - tn: 652.0000 - fn: 418.0000 - accuracy: 0.6089 - precision: 0.6085 - recall: 0.6108 - auc: 0.6495 - val_loss: 0.7345 - val_tp: 207.0000 - val_fp: 80.0000 - val_tn: 153.0000 - val_fn: 271.0000 - val_accuracy: 0.5063 - val_precision: 0.7213 - val_recall: 0.4331 - val_auc: 0.5413\n",
      "Epoch 7/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.6633 - tp: 632.0000 - fp: 414.0000 - tn: 660.0000 - fn: 442.0000 - accuracy: 0.6015 - precision: 0.6042 - recall: 0.5885 - auc: 0.6486 - val_loss: 0.7234 - val_tp: 266.0000 - val_fp: 144.0000 - val_tn: 89.0000 - val_fn: 212.0000 - val_accuracy: 0.4993 - val_precision: 0.6488 - val_recall: 0.5565 - val_auc: 0.4967\n",
      "Epoch 8/150\n",
      "2148/2148 [==============================] - 0s 167us/step - loss: 0.6477 - tp: 677.0000 - fp: 406.0000 - tn: 668.0000 - fn: 397.0000 - accuracy: 0.6262 - precision: 0.6251 - recall: 0.6304 - auc: 0.6737 - val_loss: 0.7566 - val_tp: 239.0000 - val_fp: 104.0000 - val_tn: 129.0000 - val_fn: 239.0000 - val_accuracy: 0.5176 - val_precision: 0.6968 - val_recall: 0.5000 - val_auc: 0.5058\n",
      "Epoch 9/150\n",
      "2148/2148 [==============================] - 0s 166us/step - loss: 0.6484 - tp: 672.0000 - fp: 397.0000 - tn: 677.0000 - fn: 402.0000 - accuracy: 0.6280 - precision: 0.6286 - recall: 0.6257 - auc: 0.6734 - val_loss: 0.7396 - val_tp: 234.0000 - val_fp: 86.0000 - val_tn: 147.0000 - val_fn: 244.0000 - val_accuracy: 0.5359 - val_precision: 0.7312 - val_recall: 0.4895 - val_auc: 0.5423\n",
      "Epoch 10/150\n",
      "2148/2148 [==============================] - 0s 163us/step - loss: 0.6376 - tp: 661.0000 - fp: 373.0000 - tn: 701.0000 - fn: 413.0000 - accuracy: 0.6341 - precision: 0.6393 - recall: 0.6155 - auc: 0.6875 - val_loss: 0.7343 - val_tp: 289.0000 - val_fp: 146.0000 - val_tn: 87.0000 - val_fn: 189.0000 - val_accuracy: 0.5288 - val_precision: 0.6644 - val_recall: 0.6046 - val_auc: 0.5093\n",
      "Epoch 11/150\n",
      "2148/2148 [==============================] - 0s 170us/step - loss: 0.6233 - tp: 682.0000 - fp: 395.0000 - tn: 679.0000 - fn: 392.0000 - accuracy: 0.6336 - precision: 0.6332 - recall: 0.6350 - auc: 0.7047 - val_loss: 0.7378 - val_tp: 269.0000 - val_fp: 125.0000 - val_tn: 108.0000 - val_fn: 209.0000 - val_accuracy: 0.5302 - val_precision: 0.6827 - val_recall: 0.5628 - val_auc: 0.5248\n",
      "Epoch 12/150\n",
      "2148/2148 [==============================] - 0s 162us/step - loss: 0.6188 - tp: 682.0000 - fp: 359.0000 - tn: 715.0000 - fn: 392.0000 - accuracy: 0.6504 - precision: 0.6551 - recall: 0.6350 - auc: 0.7129 - val_loss: 0.7575 - val_tp: 259.0000 - val_fp: 103.0000 - val_tn: 130.0000 - val_fn: 219.0000 - val_accuracy: 0.5471 - val_precision: 0.7155 - val_recall: 0.5418 - val_auc: 0.5276\n",
      "Epoch 13/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.6078 - tp: 730.0000 - fp: 361.0000 - tn: 713.0000 - fn: 344.0000 - accuracy: 0.6718 - precision: 0.6691 - recall: 0.6797 - auc: 0.7294 - val_loss: 0.7513 - val_tp: 324.0000 - val_fp: 171.0000 - val_tn: 62.0000 - val_fn: 154.0000 - val_accuracy: 0.5429 - val_precision: 0.6545 - val_recall: 0.6778 - val_auc: 0.5024\n",
      "Epoch 14/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.6120 - tp: 694.0000 - fp: 356.0000 - tn: 718.0000 - fn: 380.0000 - accuracy: 0.6574 - precision: 0.6610 - recall: 0.6462 - auc: 0.7223 - val_loss: 0.7516 - val_tp: 246.0000 - val_fp: 99.0000 - val_tn: 134.0000 - val_fn: 232.0000 - val_accuracy: 0.5345 - val_precision: 0.7130 - val_recall: 0.5146 - val_auc: 0.5413\n",
      "Epoch 15/150\n",
      "2148/2148 [==============================] - 0s 148us/step - loss: 0.5950 - tp: 743.0000 - fp: 348.0000 - tn: 726.0000 - fn: 331.0000 - accuracy: 0.6839 - precision: 0.6810 - recall: 0.6918 - auc: 0.7466 - val_loss: 0.7657 - val_tp: 263.0000 - val_fp: 119.0000 - val_tn: 114.0000 - val_fn: 215.0000 - val_accuracy: 0.5302 - val_precision: 0.6885 - val_recall: 0.5502 - val_auc: 0.5322\n",
      "Epoch 16/150\n",
      "2148/2148 [==============================] - 0s 155us/step - loss: 0.5965 - tp: 704.0000 - fp: 341.0000 - tn: 733.0000 - fn: 370.0000 - accuracy: 0.6690 - precision: 0.6737 - recall: 0.6555 - auc: 0.7411 - val_loss: 0.7529 - val_tp: 269.0000 - val_fp: 152.0000 - val_tn: 81.0000 - val_fn: 209.0000 - val_accuracy: 0.4923 - val_precision: 0.6390 - val_recall: 0.5628 - val_auc: 0.5299\n",
      "Epoch 17/150\n",
      "2148/2148 [==============================] - 0s 169us/step - loss: 0.5938 - tp: 724.0000 - fp: 350.0000 - tn: 724.0000 - fn: 350.0000 - accuracy: 0.6741 - precision: 0.6741 - recall: 0.6741 - auc: 0.7435 - val_loss: 0.7353 - val_tp: 259.0000 - val_fp: 121.0000 - val_tn: 112.0000 - val_fn: 219.0000 - val_accuracy: 0.5218 - val_precision: 0.6816 - val_recall: 0.5418 - val_auc: 0.5514\n",
      "Epoch 18/150\n",
      "2148/2148 [==============================] - 0s 161us/step - loss: 0.5852 - tp: 748.0000 - fp: 345.0000 - tn: 729.0000 - fn: 326.0000 - accuracy: 0.6876 - precision: 0.6844 - recall: 0.6965 - auc: 0.7558 - val_loss: 0.7679 - val_tp: 255.0000 - val_fp: 89.0000 - val_tn: 144.0000 - val_fn: 223.0000 - val_accuracy: 0.5612 - val_precision: 0.7413 - val_recall: 0.5335 - val_auc: 0.5534\n",
      "Epoch 19/150\n",
      "2148/2148 [==============================] - 0s 164us/step - loss: 0.5814 - tp: 730.0000 - fp: 322.0000 - tn: 752.0000 - fn: 344.0000 - accuracy: 0.6899 - precision: 0.6939 - recall: 0.6797 - auc: 0.7623 - val_loss: 0.7887 - val_tp: 171.0000 - val_fp: 49.0000 - val_tn: 184.0000 - val_fn: 307.0000 - val_accuracy: 0.4993 - val_precision: 0.7773 - val_recall: 0.3577 - val_auc: 0.6099\n",
      "Epoch 20/150\n",
      "2148/2148 [==============================] - 0s 166us/step - loss: 0.5768 - tp: 727.0000 - fp: 312.0000 - tn: 762.0000 - fn: 347.0000 - accuracy: 0.6932 - precision: 0.6997 - recall: 0.6769 - auc: 0.7665 - val_loss: 0.7481 - val_tp: 250.0000 - val_fp: 100.0000 - val_tn: 133.0000 - val_fn: 228.0000 - val_accuracy: 0.5387 - val_precision: 0.7143 - val_recall: 0.5230 - val_auc: 0.5674\n",
      "Epoch 21/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148/2148 [==============================] - 0s 167us/step - loss: 0.5670 - tp: 743.0000 - fp: 314.0000 - tn: 760.0000 - fn: 331.0000 - accuracy: 0.6997 - precision: 0.7029 - recall: 0.6918 - auc: 0.7753 - val_loss: 0.7423 - val_tp: 292.0000 - val_fp: 156.0000 - val_tn: 77.0000 - val_fn: 186.0000 - val_accuracy: 0.5190 - val_precision: 0.6518 - val_recall: 0.6109 - val_auc: 0.5362\n",
      "Epoch 22/150\n",
      "2148/2148 [==============================] - ETA: 0s - loss: 0.5625 - tp: 655.0000 - fp: 257.0000 - tn: 627.0000 - fn: 253.0000 - accuracy: 0.7154 - precision: 0.7182 - recall: 0.7214 - auc: 0.782 - 0s 157us/step - loss: 0.5606 - tp: 771.0000 - fp: 310.0000 - tn: 764.0000 - fn: 303.0000 - accuracy: 0.7146 - precision: 0.7132 - recall: 0.7179 - auc: 0.7844 - val_loss: 0.7554 - val_tp: 287.0000 - val_fp: 113.0000 - val_tn: 120.0000 - val_fn: 191.0000 - val_accuracy: 0.5724 - val_precision: 0.7175 - val_recall: 0.6004 - val_auc: 0.5567\n",
      "Epoch 23/150\n",
      "2148/2148 [==============================] - 0s 162us/step - loss: 0.5544 - tp: 762.0000 - fp: 314.0000 - tn: 760.0000 - fn: 312.0000 - accuracy: 0.7086 - precision: 0.7082 - recall: 0.7095 - auc: 0.7880 - val_loss: 0.7358 - val_tp: 250.0000 - val_fp: 102.0000 - val_tn: 131.0000 - val_fn: 228.0000 - val_accuracy: 0.5359 - val_precision: 0.7102 - val_recall: 0.5230 - val_auc: 0.5773\n",
      "Epoch 24/150\n",
      "2148/2148 [==============================] - 0s 151us/step - loss: 0.5638 - tp: 768.0000 - fp: 331.0000 - tn: 743.0000 - fn: 306.0000 - accuracy: 0.7034 - precision: 0.6988 - recall: 0.7151 - auc: 0.7804 - val_loss: 0.7377 - val_tp: 244.0000 - val_fp: 99.0000 - val_tn: 134.0000 - val_fn: 234.0000 - val_accuracy: 0.5316 - val_precision: 0.7114 - val_recall: 0.5105 - val_auc: 0.5914\n",
      "Epoch 25/150\n",
      "2148/2148 [==============================] - 0s 156us/step - loss: 0.5601 - tp: 762.0000 - fp: 302.0000 - tn: 772.0000 - fn: 312.0000 - accuracy: 0.7142 - precision: 0.7162 - recall: 0.7095 - auc: 0.7838 - val_loss: 0.7469 - val_tp: 230.0000 - val_fp: 85.0000 - val_tn: 148.0000 - val_fn: 248.0000 - val_accuracy: 0.5316 - val_precision: 0.7302 - val_recall: 0.4812 - val_auc: 0.6092\n",
      "Epoch 26/150\n",
      "2148/2148 [==============================] - 0s 157us/step - loss: 0.5504 - tp: 769.0000 - fp: 318.0000 - tn: 756.0000 - fn: 305.0000 - accuracy: 0.7100 - precision: 0.7075 - recall: 0.7160 - auc: 0.7912 - val_loss: 0.7342 - val_tp: 263.0000 - val_fp: 100.0000 - val_tn: 133.0000 - val_fn: 215.0000 - val_accuracy: 0.5570 - val_precision: 0.7245 - val_recall: 0.5502 - val_auc: 0.5898\n",
      "Epoch 27/150\n",
      "2148/2148 [==============================] - 0s 157us/step - loss: 0.5407 - tp: 769.0000 - fp: 306.0000 - tn: 768.0000 - fn: 305.0000 - accuracy: 0.7155 - precision: 0.7153 - recall: 0.7160 - auc: 0.8002 - val_loss: 0.7546 - val_tp: 259.0000 - val_fp: 106.0000 - val_tn: 127.0000 - val_fn: 219.0000 - val_accuracy: 0.5429 - val_precision: 0.7096 - val_recall: 0.5418 - val_auc: 0.5767\n",
      "Epoch 28/150\n",
      "2148/2148 [==============================] - 0s 151us/step - loss: 0.5309 - tp: 799.0000 - fp: 280.0000 - tn: 794.0000 - fn: 275.0000 - accuracy: 0.7416 - precision: 0.7405 - recall: 0.7439 - auc: 0.8145 - val_loss: 0.7844 - val_tp: 305.0000 - val_fp: 145.0000 - val_tn: 88.0000 - val_fn: 173.0000 - val_accuracy: 0.5527 - val_precision: 0.6778 - val_recall: 0.6381 - val_auc: 0.5483\n",
      "Epoch 29/150\n",
      "2148/2148 [==============================] - 0s 139us/step - loss: 0.5390 - tp: 776.0000 - fp: 303.0000 - tn: 771.0000 - fn: 298.0000 - accuracy: 0.7202 - precision: 0.7192 - recall: 0.7225 - auc: 0.8024 - val_loss: 0.7795 - val_tp: 257.0000 - val_fp: 82.0000 - val_tn: 151.0000 - val_fn: 221.0000 - val_accuracy: 0.5738 - val_precision: 0.7581 - val_recall: 0.5377 - val_auc: 0.5968\n",
      "Epoch 30/150\n",
      "2148/2148 [==============================] - 0s 147us/step - loss: 0.5314 - tp: 781.0000 - fp: 277.0000 - tn: 797.0000 - fn: 293.0000 - accuracy: 0.7346 - precision: 0.7382 - recall: 0.7272 - auc: 0.8123 - val_loss: 0.7851 - val_tp: 239.0000 - val_fp: 82.0000 - val_tn: 151.0000 - val_fn: 239.0000 - val_accuracy: 0.5485 - val_precision: 0.7445 - val_recall: 0.5000 - val_auc: 0.5960\n",
      "Epoch 31/150\n",
      "2148/2148 [==============================] - 0s 147us/step - loss: 0.5280 - tp: 798.0000 - fp: 300.0000 - tn: 774.0000 - fn: 276.0000 - accuracy: 0.7318 - precision: 0.7268 - recall: 0.7430 - auc: 0.8123 - val_loss: 0.7834 - val_tp: 228.0000 - val_fp: 73.0000 - val_tn: 160.0000 - val_fn: 250.0000 - val_accuracy: 0.5457 - val_precision: 0.7575 - val_recall: 0.4770 - val_auc: 0.6013\n",
      "Epoch 32/150\n",
      "2148/2148 [==============================] - 0s 178us/step - loss: 0.5258 - tp: 793.0000 - fp: 298.0000 - tn: 776.0000 - fn: 281.0000 - accuracy: 0.7304 - precision: 0.7269 - recall: 0.7384 - auc: 0.8144 - val_loss: 0.8079 - val_tp: 194.0000 - val_fp: 55.0000 - val_tn: 178.0000 - val_fn: 284.0000 - val_accuracy: 0.5232 - val_precision: 0.7791 - val_recall: 0.4059 - val_auc: 0.6136\n",
      "Epoch 33/150\n",
      "2148/2148 [==============================] - 0s 186us/step - loss: 0.5231 - tp: 789.0000 - fp: 296.0000 - tn: 778.0000 - fn: 285.0000 - accuracy: 0.7295 - precision: 0.7272 - recall: 0.7346 - auc: 0.8188 - val_loss: 0.7893 - val_tp: 248.0000 - val_fp: 83.0000 - val_tn: 150.0000 - val_fn: 230.0000 - val_accuracy: 0.5598 - val_precision: 0.7492 - val_recall: 0.5188 - val_auc: 0.5925\n",
      "Epoch 34/150\n",
      "2148/2148 [==============================] - 0s 169us/step - loss: 0.5056 - tp: 815.0000 - fp: 278.0000 - tn: 796.0000 - fn: 259.0000 - accuracy: 0.7500 - precision: 0.7457 - recall: 0.7588 - auc: 0.8341 - val_loss: 0.7987 - val_tp: 226.0000 - val_fp: 78.0000 - val_tn: 155.0000 - val_fn: 252.0000 - val_accuracy: 0.5359 - val_precision: 0.7434 - val_recall: 0.4728 - val_auc: 0.6031\n",
      "Epoch 35/150\n",
      "2148/2148 [==============================] - 0s 166us/step - loss: 0.5140 - tp: 795.0000 - fp: 269.0000 - tn: 805.0000 - fn: 279.0000 - accuracy: 0.7449 - precision: 0.7472 - recall: 0.7402 - auc: 0.8261 - val_loss: 0.7735 - val_tp: 279.0000 - val_fp: 122.0000 - val_tn: 111.0000 - val_fn: 199.0000 - val_accuracy: 0.5485 - val_precision: 0.6958 - val_recall: 0.5837 - val_auc: 0.5768\n",
      "Epoch 36/150\n",
      "2148/2148 [==============================] - 0s 169us/step - loss: 0.4959 - tp: 814.0000 - fp: 262.0000 - tn: 812.0000 - fn: 260.0000 - accuracy: 0.7570 - precision: 0.7565 - recall: 0.7579 - auc: 0.8429 - val_loss: 0.7694 - val_tp: 255.0000 - val_fp: 82.0000 - val_tn: 151.0000 - val_fn: 223.0000 - val_accuracy: 0.5710 - val_precision: 0.7567 - val_recall: 0.5335 - val_auc: 0.6118\n",
      "Epoch 37/150\n",
      "2148/2148 [==============================] - 0s 170us/step - loss: 0.4905 - tp: 825.0000 - fp: 245.0000 - tn: 829.0000 - fn: 249.0000 - accuracy: 0.7700 - precision: 0.7710 - recall: 0.7682 - auc: 0.8479 - val_loss: 0.7983 - val_tp: 248.0000 - val_fp: 79.0000 - val_tn: 154.0000 - val_fn: 230.0000 - val_accuracy: 0.5654 - val_precision: 0.7584 - val_recall: 0.5188 - val_auc: 0.6072\n",
      "Epoch 38/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.4944 - tp: 812.0000 - fp: 254.0000 - tn: 820.0000 - fn: 262.0000 - accuracy: 0.7598 - precision: 0.7617 - recall: 0.7561 - auc: 0.8435 - val_loss: 0.7858 - val_tp: 263.0000 - val_fp: 91.0000 - val_tn: 142.0000 - val_fn: 215.0000 - val_accuracy: 0.5696 - val_precision: 0.7429 - val_recall: 0.5502 - val_auc: 0.6045\n",
      "Epoch 39/150\n",
      "2148/2148 [==============================] - 0s 166us/step - loss: 0.4929 - tp: 814.0000 - fp: 264.0000 - tn: 810.0000 - fn: 260.0000 - accuracy: 0.7561 - precision: 0.7551 - recall: 0.7579 - auc: 0.8440 - val_loss: 0.7782 - val_tp: 268.0000 - val_fp: 103.0000 - val_tn: 130.0000 - val_fn: 210.0000 - val_accuracy: 0.5598 - val_precision: 0.7224 - val_recall: 0.5607 - val_auc: 0.6042\n",
      "Epoch 40/150\n",
      "2148/2148 [==============================] - 0s 159us/step - loss: 0.4860 - tp: 800.0000 - fp: 251.0000 - tn: 823.0000 - fn: 274.0000 - accuracy: 0.7556 - precision: 0.7612 - recall: 0.7449 - auc: 0.8472 - val_loss: 0.7583 - val_tp: 277.0000 - val_fp: 113.0000 - val_tn: 120.0000 - val_fn: 201.0000 - val_accuracy: 0.5584 - val_precision: 0.7103 - val_recall: 0.5795 - val_auc: 0.6040\n",
      "Epoch 41/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148/2148 [==============================] - 0s 164us/step - loss: 0.4767 - tp: 827.0000 - fp: 254.0000 - tn: 820.0000 - fn: 247.0000 - accuracy: 0.7668 - precision: 0.7650 - recall: 0.7700 - auc: 0.8582 - val_loss: 0.8167 - val_tp: 219.0000 - val_fp: 66.0000 - val_tn: 167.0000 - val_fn: 259.0000 - val_accuracy: 0.5429 - val_precision: 0.7684 - val_recall: 0.4582 - val_auc: 0.6346\n",
      "Epoch 42/150\n",
      "2148/2148 [==============================] - 0s 165us/step - loss: 0.4818 - tp: 802.0000 - fp: 252.0000 - tn: 822.0000 - fn: 272.0000 - accuracy: 0.7561 - precision: 0.7609 - recall: 0.7467 - auc: 0.8495 - val_loss: 0.7802 - val_tp: 256.0000 - val_fp: 85.0000 - val_tn: 148.0000 - val_fn: 222.0000 - val_accuracy: 0.5682 - val_precision: 0.7507 - val_recall: 0.5356 - val_auc: 0.6186\n",
      "Epoch 43/150\n",
      "2148/2148 [==============================] - 0s 176us/step - loss: 0.4855 - tp: 815.0000 - fp: 245.0000 - tn: 829.0000 - fn: 259.0000 - accuracy: 0.7654 - precision: 0.7689 - recall: 0.7588 - auc: 0.8490 - val_loss: 0.7633 - val_tp: 261.0000 - val_fp: 85.0000 - val_tn: 148.0000 - val_fn: 217.0000 - val_accuracy: 0.5752 - val_precision: 0.7543 - val_recall: 0.5460 - val_auc: 0.6303\n",
      "Epoch 44/150\n",
      "2148/2148 [==============================] - 0s 192us/step - loss: 0.4619 - tp: 842.0000 - fp: 230.0000 - tn: 844.0000 - fn: 232.0000 - accuracy: 0.7849 - precision: 0.7854 - recall: 0.7840 - auc: 0.8669 - val_loss: 0.8324 - val_tp: 213.0000 - val_fp: 58.0000 - val_tn: 175.0000 - val_fn: 265.0000 - val_accuracy: 0.5457 - val_precision: 0.7860 - val_recall: 0.4456 - val_auc: 0.6345\n",
      "Epoch 45/150\n",
      "2148/2148 [==============================] - 0s 193us/step - loss: 0.4696 - tp: 832.0000 - fp: 235.0000 - tn: 839.0000 - fn: 242.0000 - accuracy: 0.7779 - precision: 0.7798 - recall: 0.7747 - auc: 0.8614 - val_loss: 0.7907 - val_tp: 271.0000 - val_fp: 103.0000 - val_tn: 130.0000 - val_fn: 207.0000 - val_accuracy: 0.5640 - val_precision: 0.7246 - val_recall: 0.5669 - val_auc: 0.6084\n",
      "Epoch 46/150\n",
      "2148/2148 [==============================] - 0s 180us/step - loss: 0.4623 - tp: 834.0000 - fp: 243.0000 - tn: 831.0000 - fn: 240.0000 - accuracy: 0.7751 - precision: 0.7744 - recall: 0.7765 - auc: 0.8657 - val_loss: 0.8359 - val_tp: 208.0000 - val_fp: 54.0000 - val_tn: 179.0000 - val_fn: 270.0000 - val_accuracy: 0.5443 - val_precision: 0.7939 - val_recall: 0.4351 - val_auc: 0.6256\n",
      "Epoch 47/150\n",
      "2148/2148 [==============================] - 0s 158us/step - loss: 0.4535 - tp: 836.0000 - fp: 220.0000 - tn: 854.0000 - fn: 238.0000 - accuracy: 0.7868 - precision: 0.7917 - recall: 0.7784 - auc: 0.8736 - val_loss: 0.7997 - val_tp: 234.0000 - val_fp: 62.0000 - val_tn: 171.0000 - val_fn: 244.0000 - val_accuracy: 0.5696 - val_precision: 0.7905 - val_recall: 0.4895 - val_auc: 0.6303\n",
      "Epoch 48/150\n",
      "2148/2148 [==============================] - 0s 148us/step - loss: 0.4663 - tp: 822.0000 - fp: 243.0000 - tn: 831.0000 - fn: 252.0000 - accuracy: 0.7696 - precision: 0.7718 - recall: 0.7654 - auc: 0.8602 - val_loss: 0.7773 - val_tp: 257.0000 - val_fp: 87.0000 - val_tn: 146.0000 - val_fn: 221.0000 - val_accuracy: 0.5668 - val_precision: 0.7471 - val_recall: 0.5377 - val_auc: 0.6222\n",
      "Epoch 49/150\n",
      "2148/2148 [==============================] - 0s 147us/step - loss: 0.4634 - tp: 828.0000 - fp: 221.0000 - tn: 853.0000 - fn: 246.0000 - accuracy: 0.7826 - precision: 0.7893 - recall: 0.7709 - auc: 0.8643 - val_loss: 0.7874 - val_tp: 262.0000 - val_fp: 85.0000 - val_tn: 148.0000 - val_fn: 216.0000 - val_accuracy: 0.5767 - val_precision: 0.7550 - val_recall: 0.5481 - val_auc: 0.6129\n",
      "Epoch 50/150\n",
      "2148/2148 [==============================] - 0s 147us/step - loss: 0.4538 - tp: 843.0000 - fp: 219.0000 - tn: 855.0000 - fn: 231.0000 - accuracy: 0.7905 - precision: 0.7938 - recall: 0.7849 - auc: 0.8719 - val_loss: 0.7700 - val_tp: 279.0000 - val_fp: 118.0000 - val_tn: 115.0000 - val_fn: 199.0000 - val_accuracy: 0.5541 - val_precision: 0.7028 - val_recall: 0.5837 - val_auc: 0.6030\n",
      "Epoch 51/150\n",
      "2148/2148 [==============================] - 0s 149us/step - loss: 0.4564 - tp: 824.0000 - fp: 235.0000 - tn: 839.0000 - fn: 250.0000 - accuracy: 0.7742 - precision: 0.7781 - recall: 0.7672 - auc: 0.8676 - val_loss: 0.8120 - val_tp: 228.0000 - val_fp: 58.0000 - val_tn: 175.0000 - val_fn: 250.0000 - val_accuracy: 0.5668 - val_precision: 0.7972 - val_recall: 0.4770 - val_auc: 0.6417\n",
      "Epoch 52/150\n",
      "2148/2148 [==============================] - 0s 150us/step - loss: 0.4423 - tp: 840.0000 - fp: 205.0000 - tn: 869.0000 - fn: 234.0000 - accuracy: 0.7956 - precision: 0.8038 - recall: 0.7821 - auc: 0.8803 - val_loss: 0.8099 - val_tp: 206.0000 - val_fp: 54.0000 - val_tn: 179.0000 - val_fn: 272.0000 - val_accuracy: 0.5415 - val_precision: 0.7923 - val_recall: 0.4310 - val_auc: 0.6450\n",
      "Epoch 53/150\n",
      "2148/2148 [==============================] - 0s 151us/step - loss: 0.4614 - tp: 840.0000 - fp: 233.0000 - tn: 841.0000 - fn: 234.0000 - accuracy: 0.7826 - precision: 0.7829 - recall: 0.7821 - auc: 0.8647 - val_loss: 0.7561 - val_tp: 276.0000 - val_fp: 104.0000 - val_tn: 129.0000 - val_fn: 202.0000 - val_accuracy: 0.5696 - val_precision: 0.7263 - val_recall: 0.5774 - val_auc: 0.6159\n",
      "Epoch 54/150\n",
      "2148/2148 [==============================] - 0s 156us/step - loss: 0.4420 - tp: 867.0000 - fp: 225.0000 - tn: 849.0000 - fn: 207.0000 - accuracy: 0.7989 - precision: 0.7940 - recall: 0.8073 - auc: 0.8810 - val_loss: 0.7975 - val_tp: 261.0000 - val_fp: 76.0000 - val_tn: 157.0000 - val_fn: 217.0000 - val_accuracy: 0.5879 - val_precision: 0.7745 - val_recall: 0.5460 - val_auc: 0.6243\n",
      "Epoch 55/150\n",
      "2148/2148 [==============================] - 0s 149us/step - loss: 0.4356 - tp: 857.0000 - fp: 224.0000 - tn: 850.0000 - fn: 217.0000 - accuracy: 0.7947 - precision: 0.7928 - recall: 0.7980 - auc: 0.8839 - val_loss: 0.8144 - val_tp: 268.0000 - val_fp: 104.0000 - val_tn: 129.0000 - val_fn: 210.0000 - val_accuracy: 0.5584 - val_precision: 0.7204 - val_recall: 0.5607 - val_auc: 0.6035\n",
      "Epoch 56/150\n",
      "2148/2148 [==============================] - 0s 151us/step - loss: 0.4360 - tp: 859.0000 - fp: 209.0000 - tn: 865.0000 - fn: 215.0000 - accuracy: 0.8026 - precision: 0.8043 - recall: 0.7998 - auc: 0.8847 - val_loss: 0.8043 - val_tp: 254.0000 - val_fp: 76.0000 - val_tn: 157.0000 - val_fn: 224.0000 - val_accuracy: 0.5781 - val_precision: 0.7697 - val_recall: 0.5314 - val_auc: 0.6215\n",
      "Epoch 57/150\n",
      "2148/2148 [==============================] - 0s 159us/step - loss: 0.4376 - tp: 860.0000 - fp: 193.0000 - tn: 881.0000 - fn: 214.0000 - accuracy: 0.8105 - precision: 0.8167 - recall: 0.8007 - auc: 0.8825 - val_loss: 0.8013 - val_tp: 238.0000 - val_fp: 73.0000 - val_tn: 160.0000 - val_fn: 240.0000 - val_accuracy: 0.5598 - val_precision: 0.7653 - val_recall: 0.4979 - val_auc: 0.6271\n",
      "Epoch 58/150\n",
      "2148/2148 [==============================] - 0s 164us/step - loss: 0.4269 - tp: 867.0000 - fp: 190.0000 - tn: 884.0000 - fn: 207.0000 - accuracy: 0.8152 - precision: 0.8202 - recall: 0.8073 - auc: 0.8914 - val_loss: 0.8756 - val_tp: 187.0000 - val_fp: 34.0000 - val_tn: 199.0000 - val_fn: 291.0000 - val_accuracy: 0.5429 - val_precision: 0.8462 - val_recall: 0.3912 - val_auc: 0.6797\n",
      "Epoch 59/150\n",
      "2148/2148 [==============================] - 0s 154us/step - loss: 0.4192 - tp: 887.0000 - fp: 200.0000 - tn: 874.0000 - fn: 187.0000 - accuracy: 0.8198 - precision: 0.8160 - recall: 0.8259 - auc: 0.8956 - val_loss: 0.7793 - val_tp: 265.0000 - val_fp: 91.0000 - val_tn: 142.0000 - val_fn: 213.0000 - val_accuracy: 0.5724 - val_precision: 0.7444 - val_recall: 0.5544 - val_auc: 0.6217\n",
      "Epoch 60/150\n",
      "2148/2148 [==============================] - 0s 150us/step - loss: 0.4177 - tp: 861.0000 - fp: 198.0000 - tn: 876.0000 - fn: 213.0000 - accuracy: 0.8087 - precision: 0.8130 - recall: 0.8017 - auc: 0.8969 - val_loss: 0.7440 - val_tp: 265.0000 - val_fp: 80.0000 - val_tn: 153.0000 - val_fn: 213.0000 - val_accuracy: 0.5879 - val_precision: 0.7681 - val_recall: 0.5544 - val_auc: 0.6463\n",
      "Epoch 61/150\n",
      "2148/2148 [==============================] - 0s 147us/step - loss: 0.4293 - tp: 852.0000 - fp: 214.0000 - tn: 860.0000 - fn: 222.0000 - accuracy: 0.7970 - precision: 0.7992 - recall: 0.7933 - auc: 0.8862 - val_loss: 0.7753 - val_tp: 261.0000 - val_fp: 80.0000 - val_tn: 153.0000 - val_fn: 217.0000 - val_accuracy: 0.5823 - val_precision: 0.7654 - val_recall: 0.5460 - val_auc: 0.6378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/150\n",
      "2148/2148 [==============================] - 0s 149us/step - loss: 0.4143 - tp: 888.0000 - fp: 192.0000 - tn: 882.0000 - fn: 186.0000 - accuracy: 0.8240 - precision: 0.8222 - recall: 0.8268 - auc: 0.8995 - val_loss: 0.8261 - val_tp: 193.0000 - val_fp: 52.0000 - val_tn: 181.0000 - val_fn: 285.0000 - val_accuracy: 0.5260 - val_precision: 0.7878 - val_recall: 0.4038 - val_auc: 0.6468\n",
      "Epoch 63/150\n",
      "2148/2148 [==============================] - 0s 151us/step - loss: 0.4218 - tp: 853.0000 - fp: 190.0000 - tn: 884.0000 - fn: 221.0000 - accuracy: 0.8087 - precision: 0.8178 - recall: 0.7942 - auc: 0.8910 - val_loss: 0.7948 - val_tp: 261.0000 - val_fp: 83.0000 - val_tn: 150.0000 - val_fn: 217.0000 - val_accuracy: 0.5781 - val_precision: 0.7587 - val_recall: 0.5460 - val_auc: 0.6246\n",
      "Epoch 64/150\n",
      "2148/2148 [==============================] - 0s 149us/step - loss: 0.4008 - tp: 875.0000 - fp: 182.0000 - tn: 892.0000 - fn: 199.0000 - accuracy: 0.8226 - precision: 0.8278 - recall: 0.8147 - auc: 0.9058 - val_loss: 0.8117 - val_tp: 262.0000 - val_fp: 82.0000 - val_tn: 151.0000 - val_fn: 216.0000 - val_accuracy: 0.5809 - val_precision: 0.7616 - val_recall: 0.5481 - val_auc: 0.6242\n",
      "Epoch 65/150\n",
      "2148/2148 [==============================] - 0s 151us/step - loss: 0.4093 - tp: 872.0000 - fp: 199.0000 - tn: 875.0000 - fn: 202.0000 - accuracy: 0.8133 - precision: 0.8142 - recall: 0.8119 - auc: 0.8986 - val_loss: 0.8167 - val_tp: 267.0000 - val_fp: 101.0000 - val_tn: 132.0000 - val_fn: 211.0000 - val_accuracy: 0.5612 - val_precision: 0.7255 - val_recall: 0.5586 - val_auc: 0.6137\n",
      "Epoch 66/150\n",
      "2148/2148 [==============================] - 0s 160us/step - loss: 0.4113 - tp: 877.0000 - fp: 207.0000 - tn: 867.0000 - fn: 197.0000 - accuracy: 0.8119 - precision: 0.8090 - recall: 0.8166 - auc: 0.8977 - val_loss: 0.8034 - val_tp: 254.0000 - val_fp: 81.0000 - val_tn: 152.0000 - val_fn: 224.0000 - val_accuracy: 0.5710 - val_precision: 0.7582 - val_recall: 0.5314 - val_auc: 0.6329\n",
      "Epoch 67/150\n",
      "2148/2148 [==============================] - 0s 168us/step - loss: 0.3973 - tp: 869.0000 - fp: 182.0000 - tn: 892.0000 - fn: 205.0000 - accuracy: 0.8198 - precision: 0.8268 - recall: 0.8091 - auc: 0.9069 - val_loss: 0.8553 - val_tp: 234.0000 - val_fp: 66.0000 - val_tn: 167.0000 - val_fn: 244.0000 - val_accuracy: 0.5640 - val_precision: 0.7800 - val_recall: 0.4895 - val_auc: 0.6439\n",
      "Epoch 68/150\n",
      "2148/2148 [==============================] - 0s 148us/step - loss: 0.3947 - tp: 876.0000 - fp: 197.0000 - tn: 877.0000 - fn: 198.0000 - accuracy: 0.8161 - precision: 0.8164 - recall: 0.8156 - auc: 0.9075 - val_loss: 0.8209 - val_tp: 267.0000 - val_fp: 83.0000 - val_tn: 150.0000 - val_fn: 211.0000 - val_accuracy: 0.5865 - val_precision: 0.7629 - val_recall: 0.5586 - val_auc: 0.6295\n",
      "Epoch 69/150\n",
      "2148/2148 [==============================] - 0s 157us/step - loss: 0.3879 - tp: 889.0000 - fp: 183.0000 - tn: 891.0000 - fn: 185.0000 - accuracy: 0.8287 - precision: 0.8293 - recall: 0.8277 - auc: 0.9117 - val_loss: 0.8276 - val_tp: 227.0000 - val_fp: 66.0000 - val_tn: 167.0000 - val_fn: 251.0000 - val_accuracy: 0.5541 - val_precision: 0.7747 - val_recall: 0.4749 - val_auc: 0.6446\n",
      "Epoch 70/150\n",
      "2148/2148 [==============================] - 0s 221us/step - loss: 0.3860 - tp: 883.0000 - fp: 178.0000 - tn: 896.0000 - fn: 191.0000 - accuracy: 0.8282 - precision: 0.8322 - recall: 0.8222 - auc: 0.9128 - val_loss: 0.8220 - val_tp: 272.0000 - val_fp: 104.0000 - val_tn: 129.0000 - val_fn: 206.0000 - val_accuracy: 0.5640 - val_precision: 0.7234 - val_recall: 0.5690 - val_auc: 0.6123\n",
      "Epoch 71/150\n",
      "2148/2148 [==============================] - 0s 181us/step - loss: 0.3831 - tp: 900.0000 - fp: 190.0000 - tn: 884.0000 - fn: 174.0000 - accuracy: 0.8305 - precision: 0.8257 - recall: 0.8380 - auc: 0.9141 - val_loss: 0.8334 - val_tp: 230.0000 - val_fp: 67.0000 - val_tn: 166.0000 - val_fn: 248.0000 - val_accuracy: 0.5570 - val_precision: 0.7744 - val_recall: 0.4812 - val_auc: 0.6288\n",
      "Epoch 72/150\n",
      "2148/2148 [==============================] - 0s 186us/step - loss: 0.3841 - tp: 895.0000 - fp: 175.0000 - tn: 899.0000 - fn: 179.0000 - accuracy: 0.8352 - precision: 0.8364 - recall: 0.8333 - auc: 0.9141 - val_loss: 0.8716 - val_tp: 271.0000 - val_fp: 101.0000 - val_tn: 132.0000 - val_fn: 207.0000 - val_accuracy: 0.5668 - val_precision: 0.7285 - val_recall: 0.5669 - val_auc: 0.6115\n",
      "Epoch 73/150\n",
      "2148/2148 [==============================] - 0s 193us/step - loss: 0.3800 - tp: 901.0000 - fp: 180.0000 - tn: 894.0000 - fn: 173.0000 - accuracy: 0.8357 - precision: 0.8335 - recall: 0.8389 - auc: 0.9161 - val_loss: 0.8738 - val_tp: 235.0000 - val_fp: 65.0000 - val_tn: 168.0000 - val_fn: 243.0000 - val_accuracy: 0.5668 - val_precision: 0.7833 - val_recall: 0.4916 - val_auc: 0.6466\n",
      "Epoch 74/150\n",
      "2148/2148 [==============================] - 0s 186us/step - loss: 0.3767 - tp: 896.0000 - fp: 163.0000 - tn: 911.0000 - fn: 178.0000 - accuracy: 0.8412 - precision: 0.8461 - recall: 0.8343 - auc: 0.9179 - val_loss: 0.8737 - val_tp: 208.0000 - val_fp: 61.0000 - val_tn: 172.0000 - val_fn: 270.0000 - val_accuracy: 0.5345 - val_precision: 0.7732 - val_recall: 0.4351 - val_auc: 0.6639\n",
      "Epoch 75/150\n",
      "2148/2148 [==============================] - 0s 194us/step - loss: 0.3800 - tp: 885.0000 - fp: 177.0000 - tn: 897.0000 - fn: 189.0000 - accuracy: 0.8296 - precision: 0.8333 - recall: 0.8240 - auc: 0.9140 - val_loss: 0.8346 - val_tp: 254.0000 - val_fp: 83.0000 - val_tn: 150.0000 - val_fn: 224.0000 - val_accuracy: 0.5682 - val_precision: 0.7537 - val_recall: 0.5314 - val_auc: 0.6255\n",
      "Epoch 76/150\n",
      "2148/2148 [==============================] - 1s 263us/step - loss: 0.3708 - tp: 908.0000 - fp: 185.0000 - tn: 889.0000 - fn: 166.0000 - accuracy: 0.8366 - precision: 0.8307 - recall: 0.8454 - auc: 0.9205 - val_loss: 0.8761 - val_tp: 239.0000 - val_fp: 69.0000 - val_tn: 164.0000 - val_fn: 239.0000 - val_accuracy: 0.5668 - val_precision: 0.7760 - val_recall: 0.5000 - val_auc: 0.6360\n",
      "Epoch 77/150\n",
      "2148/2148 [==============================] - 0s 211us/step - loss: 0.3654 - tp: 908.0000 - fp: 156.0000 - tn: 918.0000 - fn: 166.0000 - accuracy: 0.8501 - precision: 0.8534 - recall: 0.8454 - auc: 0.9249 - val_loss: 0.9097 - val_tp: 228.0000 - val_fp: 65.0000 - val_tn: 168.0000 - val_fn: 250.0000 - val_accuracy: 0.5570 - val_precision: 0.7782 - val_recall: 0.4770 - val_auc: 0.6346\n",
      "Epoch 78/150\n",
      "2148/2148 [==============================] - 0s 203us/step - loss: 0.3675 - tp: 902.0000 - fp: 179.0000 - tn: 895.0000 - fn: 172.0000 - accuracy: 0.8366 - precision: 0.8344 - recall: 0.8399 - auc: 0.9208 - val_loss: 0.8343 - val_tp: 231.0000 - val_fp: 62.0000 - val_tn: 171.0000 - val_fn: 247.0000 - val_accuracy: 0.5654 - val_precision: 0.7884 - val_recall: 0.4833 - val_auc: 0.6426\n",
      "Epoch 79/150\n",
      "2148/2148 [==============================] - 0s 164us/step - loss: 0.3656 - tp: 915.0000 - fp: 153.0000 - tn: 921.0000 - fn: 159.0000 - accuracy: 0.8547 - precision: 0.8567 - recall: 0.8520 - auc: 0.9231 - val_loss: 0.8610 - val_tp: 211.0000 - val_fp: 64.0000 - val_tn: 169.0000 - val_fn: 267.0000 - val_accuracy: 0.5345 - val_precision: 0.7673 - val_recall: 0.4414 - val_auc: 0.6366\n",
      "Epoch 80/150\n",
      "2148/2148 [==============================] - 0s 174us/step - loss: 0.3639 - tp: 910.0000 - fp: 169.0000 - tn: 905.0000 - fn: 164.0000 - accuracy: 0.8450 - precision: 0.8434 - recall: 0.8473 - auc: 0.9236 - val_loss: 0.8742 - val_tp: 221.0000 - val_fp: 57.0000 - val_tn: 176.0000 - val_fn: 257.0000 - val_accuracy: 0.5584 - val_precision: 0.7950 - val_recall: 0.4623 - val_auc: 0.6282\n",
      "Epoch 81/150\n",
      "2148/2148 [==============================] - 0s 185us/step - loss: 0.3586 - tp: 900.0000 - fp: 158.0000 - tn: 916.0000 - fn: 174.0000 - accuracy: 0.8454 - precision: 0.8507 - recall: 0.8380 - auc: 0.9262 - val_loss: 0.8547 - val_tp: 227.0000 - val_fp: 68.0000 - val_tn: 165.0000 - val_fn: 251.0000 - val_accuracy: 0.5513 - val_precision: 0.7695 - val_recall: 0.4749 - val_auc: 0.6356\n",
      "Epoch 82/150\n",
      "2148/2148 [==============================] - 0s 175us/step - loss: 0.3622 - tp: 918.0000 - fp: 173.0000 - tn: 901.0000 - fn: 156.0000 - accuracy: 0.8468 - precision: 0.8414 - recall: 0.8547 - auc: 0.9230 - val_loss: 0.7971 - val_tp: 236.0000 - val_fp: 64.0000 - val_tn: 169.0000 - val_fn: 242.0000 - val_accuracy: 0.5696 - val_precision: 0.7867 - val_recall: 0.4937 - val_auc: 0.6617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "2148/2148 [==============================] - 0s 163us/step - loss: 0.3614 - tp: 909.0000 - fp: 168.0000 - tn: 906.0000 - fn: 165.0000 - accuracy: 0.8450 - precision: 0.8440 - recall: 0.8464 - auc: 0.9242 - val_loss: 0.8095 - val_tp: 266.0000 - val_fp: 85.0000 - val_tn: 148.0000 - val_fn: 212.0000 - val_accuracy: 0.5823 - val_precision: 0.7578 - val_recall: 0.5565 - val_auc: 0.6368\n",
      "Epoch 84/150\n",
      "2148/2148 [==============================] - 0s 165us/step - loss: 0.3637 - tp: 898.0000 - fp: 166.0000 - tn: 908.0000 - fn: 176.0000 - accuracy: 0.8408 - precision: 0.8440 - recall: 0.8361 - auc: 0.9223 - val_loss: 0.8537 - val_tp: 227.0000 - val_fp: 63.0000 - val_tn: 170.0000 - val_fn: 251.0000 - val_accuracy: 0.5584 - val_precision: 0.7828 - val_recall: 0.4749 - val_auc: 0.6505\n",
      "Epoch 85/150\n",
      "2148/2148 [==============================] - 0s 168us/step - loss: 0.3535 - tp: 894.0000 - fp: 151.0000 - tn: 923.0000 - fn: 180.0000 - accuracy: 0.8459 - precision: 0.8555 - recall: 0.8324 - auc: 0.9271 - val_loss: 0.9241 - val_tp: 197.0000 - val_fp: 51.0000 - val_tn: 182.0000 - val_fn: 281.0000 - val_accuracy: 0.5331 - val_precision: 0.7944 - val_recall: 0.4121 - val_auc: 0.6551\n",
      "Epoch 86/150\n",
      "2148/2148 [==============================] - 0s 156us/step - loss: 0.3510 - tp: 903.0000 - fp: 148.0000 - tn: 926.0000 - fn: 171.0000 - accuracy: 0.8515 - precision: 0.8592 - recall: 0.8408 - auc: 0.9295 - val_loss: 0.8213 - val_tp: 263.0000 - val_fp: 87.0000 - val_tn: 146.0000 - val_fn: 215.0000 - val_accuracy: 0.5752 - val_precision: 0.7514 - val_recall: 0.5502 - val_auc: 0.6321\n",
      "Epoch 87/150\n",
      "2148/2148 [==============================] - 0s 179us/step - loss: 0.3395 - tp: 922.0000 - fp: 152.0000 - tn: 922.0000 - fn: 152.0000 - accuracy: 0.8585 - precision: 0.8585 - recall: 0.8585 - auc: 0.9352 - val_loss: 0.8216 - val_tp: 243.0000 - val_fp: 77.0000 - val_tn: 156.0000 - val_fn: 235.0000 - val_accuracy: 0.5612 - val_precision: 0.7594 - val_recall: 0.5084 - val_auc: 0.6409\n",
      "Epoch 88/150\n",
      "2148/2148 [==============================] - 0s 180us/step - loss: 0.3414 - tp: 912.0000 - fp: 140.0000 - tn: 934.0000 - fn: 162.0000 - accuracy: 0.8594 - precision: 0.8669 - recall: 0.8492 - auc: 0.9343 - val_loss: 0.9199 - val_tp: 209.0000 - val_fp: 68.0000 - val_tn: 165.0000 - val_fn: 269.0000 - val_accuracy: 0.5260 - val_precision: 0.7545 - val_recall: 0.4372 - val_auc: 0.6268\n",
      "Epoch 89/150\n",
      "2148/2148 [==============================] - 0s 169us/step - loss: 0.3359 - tp: 923.0000 - fp: 151.0000 - tn: 923.0000 - fn: 151.0000 - accuracy: 0.8594 - precision: 0.8594 - recall: 0.8594 - auc: 0.9375 - val_loss: 0.8947 - val_tp: 218.0000 - val_fp: 68.0000 - val_tn: 165.0000 - val_fn: 260.0000 - val_accuracy: 0.5387 - val_precision: 0.7622 - val_recall: 0.4561 - val_auc: 0.6482\n",
      "Epoch 90/150\n",
      "2148/2148 [==============================] - 0s 167us/step - loss: 0.3492 - tp: 912.0000 - fp: 158.0000 - tn: 916.0000 - fn: 162.0000 - accuracy: 0.8510 - precision: 0.8523 - recall: 0.8492 - auc: 0.9289 - val_loss: 0.9131 - val_tp: 204.0000 - val_fp: 40.0000 - val_tn: 193.0000 - val_fn: 274.0000 - val_accuracy: 0.5584 - val_precision: 0.8361 - val_recall: 0.4268 - val_auc: 0.6526\n",
      "Epoch 91/150\n",
      "2148/2148 [==============================] - 0s 179us/step - loss: 0.3362 - tp: 919.0000 - fp: 140.0000 - tn: 934.0000 - fn: 155.0000 - accuracy: 0.8627 - precision: 0.8678 - recall: 0.8557 - auc: 0.9369 - val_loss: 0.8595 - val_tp: 227.0000 - val_fp: 65.0000 - val_tn: 168.0000 - val_fn: 251.0000 - val_accuracy: 0.5556 - val_precision: 0.7774 - val_recall: 0.4749 - val_auc: 0.6325\n",
      "Epoch 92/150\n",
      "2148/2148 [==============================] - 0s 175us/step - loss: 0.3527 - tp: 919.0000 - fp: 163.0000 - tn: 911.0000 - fn: 155.0000 - accuracy: 0.8520 - precision: 0.8494 - recall: 0.8557 - auc: 0.9269 - val_loss: 0.7880 - val_tp: 284.0000 - val_fp: 89.0000 - val_tn: 144.0000 - val_fn: 194.0000 - val_accuracy: 0.6020 - val_precision: 0.7614 - val_recall: 0.5941 - val_auc: 0.6357\n",
      "Epoch 93/150\n",
      "2148/2148 [==============================] - 0s 183us/step - loss: 0.3339 - tp: 930.0000 - fp: 143.0000 - tn: 931.0000 - fn: 144.0000 - accuracy: 0.8664 - precision: 0.8667 - recall: 0.8659 - auc: 0.9370 - val_loss: 0.7815 - val_tp: 303.0000 - val_fp: 96.0000 - val_tn: 137.0000 - val_fn: 175.0000 - val_accuracy: 0.6188 - val_precision: 0.7594 - val_recall: 0.6339 - val_auc: 0.6419\n",
      "Epoch 94/150\n",
      "2148/2148 [==============================] - 0s 175us/step - loss: 0.3360 - tp: 921.0000 - fp: 155.0000 - tn: 919.0000 - fn: 153.0000 - accuracy: 0.8566 - precision: 0.8559 - recall: 0.8575 - auc: 0.9358 - val_loss: 0.8364 - val_tp: 283.0000 - val_fp: 90.0000 - val_tn: 143.0000 - val_fn: 195.0000 - val_accuracy: 0.5992 - val_precision: 0.7587 - val_recall: 0.5921 - val_auc: 0.6321\n",
      "Epoch 95/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.3240 - tp: 932.0000 - fp: 139.0000 - tn: 935.0000 - fn: 142.0000 - accuracy: 0.8692 - precision: 0.8702 - recall: 0.8678 - auc: 0.9410 - val_loss: 0.8206 - val_tp: 269.0000 - val_fp: 89.0000 - val_tn: 144.0000 - val_fn: 209.0000 - val_accuracy: 0.5809 - val_precision: 0.7514 - val_recall: 0.5628 - val_auc: 0.6334\n",
      "Epoch 96/150\n",
      "2148/2148 [==============================] - 0s 170us/step - loss: 0.3134 - tp: 939.0000 - fp: 136.0000 - tn: 938.0000 - fn: 135.0000 - accuracy: 0.8738 - precision: 0.8735 - recall: 0.8743 - auc: 0.9464 - val_loss: 0.8518 - val_tp: 231.0000 - val_fp: 68.0000 - val_tn: 165.0000 - val_fn: 247.0000 - val_accuracy: 0.5570 - val_precision: 0.7726 - val_recall: 0.4833 - val_auc: 0.6520\n",
      "Epoch 97/150\n",
      "2148/2148 [==============================] - 0s 173us/step - loss: 0.3202 - tp: 932.0000 - fp: 141.0000 - tn: 933.0000 - fn: 142.0000 - accuracy: 0.8682 - precision: 0.8686 - recall: 0.8678 - auc: 0.9433 - val_loss: 0.7946 - val_tp: 275.0000 - val_fp: 78.0000 - val_tn: 155.0000 - val_fn: 203.0000 - val_accuracy: 0.6048 - val_precision: 0.7790 - val_recall: 0.5753 - val_auc: 0.6579\n",
      "Epoch 98/150\n",
      "2148/2148 [==============================] - 0s 173us/step - loss: 0.3146 - tp: 930.0000 - fp: 121.0000 - tn: 953.0000 - fn: 144.0000 - accuracy: 0.8766 - precision: 0.8849 - recall: 0.8659 - auc: 0.9452 - val_loss: 0.8888 - val_tp: 245.0000 - val_fp: 70.0000 - val_tn: 163.0000 - val_fn: 233.0000 - val_accuracy: 0.5738 - val_precision: 0.7778 - val_recall: 0.5126 - val_auc: 0.6425\n",
      "Epoch 99/150\n",
      "2148/2148 [==============================] - 0s 174us/step - loss: 0.3138 - tp: 941.0000 - fp: 138.0000 - tn: 936.0000 - fn: 133.0000 - accuracy: 0.8738 - precision: 0.8721 - recall: 0.8762 - auc: 0.9455 - val_loss: 0.8683 - val_tp: 236.0000 - val_fp: 67.0000 - val_tn: 166.0000 - val_fn: 242.0000 - val_accuracy: 0.5654 - val_precision: 0.7789 - val_recall: 0.4937 - val_auc: 0.6527\n",
      "Epoch 100/150\n",
      "2148/2148 [==============================] - 0s 172us/step - loss: 0.3187 - tp: 936.0000 - fp: 138.0000 - tn: 936.0000 - fn: 138.0000 - accuracy: 0.8715 - precision: 0.8715 - recall: 0.8715 - auc: 0.9423 - val_loss: 0.9013 - val_tp: 226.0000 - val_fp: 59.0000 - val_tn: 174.0000 - val_fn: 252.0000 - val_accuracy: 0.5626 - val_precision: 0.7930 - val_recall: 0.4728 - val_auc: 0.6593\n",
      "Epoch 101/150\n",
      "2148/2148 [==============================] - 0s 169us/step - loss: 0.3052 - tp: 934.0000 - fp: 125.0000 - tn: 949.0000 - fn: 140.0000 - accuracy: 0.8766 - precision: 0.8820 - recall: 0.8696 - auc: 0.9504 - val_loss: 0.8576 - val_tp: 258.0000 - val_fp: 77.0000 - val_tn: 156.0000 - val_fn: 220.0000 - val_accuracy: 0.5823 - val_precision: 0.7701 - val_recall: 0.5397 - val_auc: 0.6249\n",
      "Epoch 102/150\n",
      "2148/2148 [==============================] - 0s 170us/step - loss: 0.3085 - tp: 944.0000 - fp: 135.0000 - tn: 939.0000 - fn: 130.0000 - accuracy: 0.8766 - precision: 0.8749 - recall: 0.8790 - auc: 0.9463 - val_loss: 0.9094 - val_tp: 234.0000 - val_fp: 76.0000 - val_tn: 157.0000 - val_fn: 244.0000 - val_accuracy: 0.5499 - val_precision: 0.7548 - val_recall: 0.4895 - val_auc: 0.6350\n",
      "Epoch 103/150\n",
      "2148/2148 [==============================] - 0s 168us/step - loss: 0.3073 - tp: 942.0000 - fp: 140.0000 - tn: 934.0000 - fn: 132.0000 - accuracy: 0.8734 - precision: 0.8706 - recall: 0.8771 - auc: 0.9472 - val_loss: 0.9325 - val_tp: 213.0000 - val_fp: 59.0000 - val_tn: 174.0000 - val_fn: 265.0000 - val_accuracy: 0.5443 - val_precision: 0.7831 - val_recall: 0.4456 - val_auc: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/150\n",
      "2148/2148 [==============================] - 0s 161us/step - loss: 0.2885 - tp: 946.0000 - fp: 115.0000 - tn: 959.0000 - fn: 128.0000 - accuracy: 0.8869 - precision: 0.8916 - recall: 0.8808 - auc: 0.9581 - val_loss: 0.9218 - val_tp: 244.0000 - val_fp: 71.0000 - val_tn: 162.0000 - val_fn: 234.0000 - val_accuracy: 0.5710 - val_precision: 0.7746 - val_recall: 0.5105 - val_auc: 0.6385\n",
      "Epoch 105/150\n",
      "2148/2148 [==============================] - 0s 173us/step - loss: 0.3100 - tp: 927.0000 - fp: 143.0000 - tn: 931.0000 - fn: 147.0000 - accuracy: 0.8650 - precision: 0.8664 - recall: 0.8631 - auc: 0.9453 - val_loss: 0.8623 - val_tp: 264.0000 - val_fp: 77.0000 - val_tn: 156.0000 - val_fn: 214.0000 - val_accuracy: 0.5907 - val_precision: 0.7742 - val_recall: 0.5523 - val_auc: 0.6438\n",
      "Epoch 106/150\n",
      "2148/2148 [==============================] - 0s 163us/step - loss: 0.3066 - tp: 939.0000 - fp: 126.0000 - tn: 948.0000 - fn: 135.0000 - accuracy: 0.8785 - precision: 0.8817 - recall: 0.8743 - auc: 0.9466 - val_loss: 0.8736 - val_tp: 318.0000 - val_fp: 136.0000 - val_tn: 97.0000 - val_fn: 160.0000 - val_accuracy: 0.5837 - val_precision: 0.7004 - val_recall: 0.6653 - val_auc: 0.5981\n",
      "Epoch 107/150\n",
      "2148/2148 [==============================] - 0s 167us/step - loss: 0.3100 - tp: 942.0000 - fp: 146.0000 - tn: 928.0000 - fn: 132.0000 - accuracy: 0.8706 - precision: 0.8658 - recall: 0.8771 - auc: 0.9453 - val_loss: 0.8656 - val_tp: 281.0000 - val_fp: 99.0000 - val_tn: 134.0000 - val_fn: 197.0000 - val_accuracy: 0.5837 - val_precision: 0.7395 - val_recall: 0.5879 - val_auc: 0.6132\n",
      "Epoch 108/150\n",
      "2148/2148 [==============================] - 0s 167us/step - loss: 0.3051 - tp: 936.0000 - fp: 142.0000 - tn: 932.0000 - fn: 138.0000 - accuracy: 0.8696 - precision: 0.8683 - recall: 0.8715 - auc: 0.9469 - val_loss: 0.8882 - val_tp: 243.0000 - val_fp: 70.0000 - val_tn: 163.0000 - val_fn: 235.0000 - val_accuracy: 0.5710 - val_precision: 0.7764 - val_recall: 0.5084 - val_auc: 0.6414\n",
      "Epoch 109/150\n",
      "2148/2148 [==============================] - 0s 168us/step - loss: 0.3025 - tp: 921.0000 - fp: 133.0000 - tn: 941.0000 - fn: 153.0000 - accuracy: 0.8669 - precision: 0.8738 - recall: 0.8575 - auc: 0.9487 - val_loss: 0.8706 - val_tp: 272.0000 - val_fp: 96.0000 - val_tn: 137.0000 - val_fn: 206.0000 - val_accuracy: 0.5752 - val_precision: 0.7391 - val_recall: 0.5690 - val_auc: 0.6210\n",
      "Epoch 110/150\n",
      "2148/2148 [==============================] - 0s 166us/step - loss: 0.2795 - tp: 968.0000 - fp: 128.0000 - tn: 946.0000 - fn: 106.0000 - accuracy: 0.8911 - precision: 0.8832 - recall: 0.9013 - auc: 0.9582 - val_loss: 0.8975 - val_tp: 252.0000 - val_fp: 70.0000 - val_tn: 163.0000 - val_fn: 226.0000 - val_accuracy: 0.5837 - val_precision: 0.7826 - val_recall: 0.5272 - val_auc: 0.6451\n",
      "Epoch 111/150\n",
      "2148/2148 [==============================] - 0s 166us/step - loss: 0.2869 - tp: 945.0000 - fp: 117.0000 - tn: 957.0000 - fn: 129.0000 - accuracy: 0.8855 - precision: 0.8898 - recall: 0.8799 - auc: 0.9552 - val_loss: 0.8923 - val_tp: 244.0000 - val_fp: 78.0000 - val_tn: 155.0000 - val_fn: 234.0000 - val_accuracy: 0.5612 - val_precision: 0.7578 - val_recall: 0.5105 - val_auc: 0.6351\n",
      "Epoch 112/150\n",
      "2148/2148 [==============================] - 0s 167us/step - loss: 0.2971 - tp: 950.0000 - fp: 131.0000 - tn: 943.0000 - fn: 124.0000 - accuracy: 0.8813 - precision: 0.8788 - recall: 0.8845 - auc: 0.9501 - val_loss: 0.8930 - val_tp: 267.0000 - val_fp: 81.0000 - val_tn: 152.0000 - val_fn: 211.0000 - val_accuracy: 0.5893 - val_precision: 0.7672 - val_recall: 0.5586 - val_auc: 0.6404\n",
      "Epoch 113/150\n",
      "2148/2148 [==============================] - 0s 161us/step - loss: 0.2865 - tp: 957.0000 - fp: 120.0000 - tn: 954.0000 - fn: 117.0000 - accuracy: 0.8897 - precision: 0.8886 - recall: 0.8911 - auc: 0.9549 - val_loss: 0.8920 - val_tp: 262.0000 - val_fp: 101.0000 - val_tn: 132.0000 - val_fn: 216.0000 - val_accuracy: 0.5541 - val_precision: 0.7218 - val_recall: 0.5481 - val_auc: 0.6131\n",
      "Epoch 114/150\n",
      "2148/2148 [==============================] - 0s 162us/step - loss: 0.2876 - tp: 958.0000 - fp: 125.0000 - tn: 949.0000 - fn: 116.0000 - accuracy: 0.8878 - precision: 0.8846 - recall: 0.8920 - auc: 0.9546 - val_loss: 0.8636 - val_tp: 238.0000 - val_fp: 69.0000 - val_tn: 164.0000 - val_fn: 240.0000 - val_accuracy: 0.5654 - val_precision: 0.7752 - val_recall: 0.4979 - val_auc: 0.6631\n",
      "Epoch 115/150\n",
      "2148/2148 [==============================] - 0s 164us/step - loss: 0.2982 - tp: 942.0000 - fp: 136.0000 - tn: 938.0000 - fn: 132.0000 - accuracy: 0.8752 - precision: 0.8738 - recall: 0.8771 - auc: 0.9499 - val_loss: 0.8815 - val_tp: 263.0000 - val_fp: 82.0000 - val_tn: 151.0000 - val_fn: 215.0000 - val_accuracy: 0.5823 - val_precision: 0.7623 - val_recall: 0.5502 - val_auc: 0.6328\n",
      "Epoch 116/150\n",
      "2148/2148 [==============================] - 0s 162us/step - loss: 0.2790 - tp: 949.0000 - fp: 121.0000 - tn: 953.0000 - fn: 125.0000 - accuracy: 0.8855 - precision: 0.8869 - recall: 0.8836 - auc: 0.9585 - val_loss: 0.9015 - val_tp: 242.0000 - val_fp: 77.0000 - val_tn: 156.0000 - val_fn: 236.0000 - val_accuracy: 0.5598 - val_precision: 0.7586 - val_recall: 0.5063 - val_auc: 0.6307\n",
      "Epoch 117/150\n",
      "2148/2148 [==============================] - 0s 169us/step - loss: 0.2786 - tp: 953.0000 - fp: 125.0000 - tn: 949.0000 - fn: 121.0000 - accuracy: 0.8855 - precision: 0.8840 - recall: 0.8873 - auc: 0.9581 - val_loss: 0.8871 - val_tp: 236.0000 - val_fp: 69.0000 - val_tn: 164.0000 - val_fn: 242.0000 - val_accuracy: 0.5626 - val_precision: 0.7738 - val_recall: 0.4937 - val_auc: 0.6628\n",
      "Epoch 118/150\n",
      "2148/2148 [==============================] - 0s 164us/step - loss: 0.2801 - tp: 940.0000 - fp: 104.0000 - tn: 970.0000 - fn: 134.0000 - accuracy: 0.8892 - precision: 0.9004 - recall: 0.8752 - auc: 0.9571 - val_loss: 0.8772 - val_tp: 261.0000 - val_fp: 84.0000 - val_tn: 149.0000 - val_fn: 217.0000 - val_accuracy: 0.5767 - val_precision: 0.7565 - val_recall: 0.5460 - val_auc: 0.6327\n",
      "Epoch 119/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.2917 - tp: 948.0000 - fp: 131.0000 - tn: 943.0000 - fn: 126.0000 - accuracy: 0.8804 - precision: 0.8786 - recall: 0.8827 - auc: 0.9507 - val_loss: 0.8608 - val_tp: 277.0000 - val_fp: 90.0000 - val_tn: 143.0000 - val_fn: 201.0000 - val_accuracy: 0.5907 - val_precision: 0.7548 - val_recall: 0.5795 - val_auc: 0.6267\n",
      "Epoch 120/150\n",
      "2148/2148 [==============================] - 0s 173us/step - loss: 0.2889 - tp: 955.0000 - fp: 126.0000 - tn: 948.0000 - fn: 119.0000 - accuracy: 0.8859 - precision: 0.8834 - recall: 0.8892 - auc: 0.9524 - val_loss: 0.9684 - val_tp: 225.0000 - val_fp: 66.0000 - val_tn: 167.0000 - val_fn: 253.0000 - val_accuracy: 0.5513 - val_precision: 0.7732 - val_recall: 0.4707 - val_auc: 0.6403\n",
      "Epoch 121/150\n",
      "2148/2148 [==============================] - 0s 171us/step - loss: 0.2850 - tp: 952.0000 - fp: 136.0000 - tn: 938.0000 - fn: 122.0000 - accuracy: 0.8799 - precision: 0.8750 - recall: 0.8864 - auc: 0.9546 - val_loss: 0.8411 - val_tp: 290.0000 - val_fp: 104.0000 - val_tn: 129.0000 - val_fn: 188.0000 - val_accuracy: 0.5893 - val_precision: 0.7360 - val_recall: 0.6067 - val_auc: 0.6303\n",
      "Epoch 122/150\n",
      "2148/2148 [==============================] - 0s 174us/step - loss: 0.2734 - tp: 960.0000 - fp: 111.0000 - tn: 963.0000 - fn: 114.0000 - accuracy: 0.8953 - precision: 0.8964 - recall: 0.8939 - auc: 0.9586 - val_loss: 0.8721 - val_tp: 282.0000 - val_fp: 95.0000 - val_tn: 138.0000 - val_fn: 196.0000 - val_accuracy: 0.5907 - val_precision: 0.7480 - val_recall: 0.5900 - val_auc: 0.6304\n",
      "Epoch 123/150\n",
      "2148/2148 [==============================] - 0s 168us/step - loss: 0.2637 - tp: 961.0000 - fp: 109.0000 - tn: 965.0000 - fn: 113.0000 - accuracy: 0.8966 - precision: 0.8981 - recall: 0.8948 - auc: 0.9644 - val_loss: 0.8846 - val_tp: 262.0000 - val_fp: 88.0000 - val_tn: 145.0000 - val_fn: 216.0000 - val_accuracy: 0.5724 - val_precision: 0.7486 - val_recall: 0.5481 - val_auc: 0.6297\n",
      "Epoch 124/150\n",
      "2148/2148 [==============================] - 0s 170us/step - loss: 0.2687 - tp: 955.0000 - fp: 105.0000 - tn: 969.0000 - fn: 119.0000 - accuracy: 0.8957 - precision: 0.9009 - recall: 0.8892 - auc: 0.9605 - val_loss: 0.8785 - val_tp: 274.0000 - val_fp: 102.0000 - val_tn: 131.0000 - val_fn: 204.0000 - val_accuracy: 0.5696 - val_precision: 0.7287 - val_recall: 0.5732 - val_auc: 0.6211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150\n",
      "2148/2148 [==============================] - 0s 170us/step - loss: 0.2683 - tp: 950.0000 - fp: 124.0000 - tn: 950.0000 - fn: 124.0000 - accuracy: 0.8845 - precision: 0.8845 - recall: 0.8845 - auc: 0.9601 - val_loss: 0.9478 - val_tp: 233.0000 - val_fp: 72.0000 - val_tn: 161.0000 - val_fn: 245.0000 - val_accuracy: 0.5541 - val_precision: 0.7639 - val_recall: 0.4874 - val_auc: 0.6457\n",
      "Epoch 126/150\n",
      "2148/2148 [==============================] - 0s 168us/step - loss: 0.2705 - tp: 944.0000 - fp: 118.0000 - tn: 956.0000 - fn: 130.0000 - accuracy: 0.8845 - precision: 0.8889 - recall: 0.8790 - auc: 0.9601 - val_loss: 0.9757 - val_tp: 240.0000 - val_fp: 80.0000 - val_tn: 153.0000 - val_fn: 238.0000 - val_accuracy: 0.5527 - val_precision: 0.7500 - val_recall: 0.5021 - val_auc: 0.6274\n",
      "Epoch 127/150\n",
      "2148/2148 [==============================] - 0s 167us/step - loss: 0.2661 - tp: 950.0000 - fp: 117.0000 - tn: 957.0000 - fn: 124.0000 - accuracy: 0.8878 - precision: 0.8903 - recall: 0.8845 - auc: 0.9615 - val_loss: 0.9490 - val_tp: 262.0000 - val_fp: 93.0000 - val_tn: 140.0000 - val_fn: 216.0000 - val_accuracy: 0.5654 - val_precision: 0.7380 - val_recall: 0.5481 - val_auc: 0.6230\n",
      "Epoch 128/150\n",
      "2148/2148 [==============================] - 0s 163us/step - loss: 0.2660 - tp: 966.0000 - fp: 106.0000 - tn: 968.0000 - fn: 108.0000 - accuracy: 0.9004 - precision: 0.9011 - recall: 0.8994 - auc: 0.9611 - val_loss: 0.9468 - val_tp: 257.0000 - val_fp: 83.0000 - val_tn: 150.0000 - val_fn: 221.0000 - val_accuracy: 0.5724 - val_precision: 0.7559 - val_recall: 0.5377 - val_auc: 0.6253\n",
      "Epoch 129/150\n",
      "2148/2148 [==============================] - 0s 161us/step - loss: 0.2583 - tp: 964.0000 - fp: 103.0000 - tn: 971.0000 - fn: 110.0000 - accuracy: 0.9008 - precision: 0.9035 - recall: 0.8976 - auc: 0.9646 - val_loss: 0.9782 - val_tp: 226.0000 - val_fp: 68.0000 - val_tn: 165.0000 - val_fn: 252.0000 - val_accuracy: 0.5499 - val_precision: 0.7687 - val_recall: 0.4728 - val_auc: 0.6576\n",
      "Epoch 130/150\n",
      " 128/2148 [>.............................] - ETA: 0s - loss: 0.2920 - tp: 55.0000 - fp: 8.0000 - tn: 58.0000 - fn: 7.0000 - accuracy: 0.8828 - precision: 0.8730 - recall: 0.8871 - auc: 0.9494"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=X_train_balanced, \n",
    "                    y=Y_train_balanced, \n",
    "                    batch_size=128, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    shuffle=True)\n",
    "\n",
    "model.save(\"model-binary.h5\")\n",
    "model.save_weights('model-binary-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "plotted_metrics = ['precision', 'recall', 'accuracy']\n",
    "\n",
    "fig = plt.figure(figsize=(18, 4 * len(plotted_metrics)))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "\n",
    "for idx, metric in enumerate(plotted_metrics):\n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+1)\n",
    "    plt.title(metric)\n",
    "    plt.plot(history_dict[metric])\n",
    "    \n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+2)\n",
    "    plt.title('val_{}'.format(metric))\n",
    "    plt.plot(history_dict['val_{}'.format(metric)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_test)\n",
    "plt.show()\n",
    "\n",
    "pd.Series(Y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE VISUALIZATION** Now, as the model is ready, lets try to explain it a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model_viz = Model(inputs=model.inputs, outputs=[model.layers[i].output for i in [0, 4]])\n",
    "model_viz.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = X_test[0]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "fig.suptitle('Original sample', fontsize=16)\n",
    "plt.plot(sample, 'r-')\n",
    "plt.show()\n",
    "\n",
    "prediction = model_viz.predict(np.array([sample]))\n",
    "\n",
    "square = 4\n",
    "idx = 1\n",
    "\n",
    "fig = plt.figure(figsize=(18, 16))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "fig.suptitle('Feature maps from first convolutional layer', fontsize=16)\n",
    "    \n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        ax = plt.subplot(square, square, idx)\n",
    "        plt.plot(prediction[0][:,:,idx-1][0], 'b-')\n",
    "        idx += 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "idx = 1\n",
    "\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "fig.suptitle('Feature maps from second convolutional layer', fontsize=16)\n",
    "    \n",
    "for _ in range(2):\n",
    "    for _ in range(4):\n",
    "        ax = plt.subplot(2, 4, idx)\n",
    "        plt.plot(prediction[1][:,:,idx-1][0], 'g-')\n",
    "        idx += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
