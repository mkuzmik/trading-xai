{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sheet I will apply Convolutional Neural Networks for stock prediction of S&P500 index. I will divide data from last 20 years into chunks. Every day contains four values: opening, low, high and closing ones - therefore input data for 30-day chunk has size 30x4. Predicted output has two classes: whether stock will go up or down or stay constant X days after last input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TIME = 120\n",
    "PREDICTION_AFTER_DAYS = 5\n",
    "EPOCHS = 200\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_SPLIT = True\n",
    "NORMALIZED_CHUNKS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>1346.089966</td>\n",
       "      <td>1358.109985</td>\n",
       "      <td>1331.880005</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>980000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1370.109985</td>\n",
       "      <td>1342.439941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>993700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1364.800049</td>\n",
       "      <td>1329.880005</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1215000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1362.140015</td>\n",
       "      <td>1329.150024</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1065200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1360.819946</td>\n",
       "      <td>1325.069946</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1026500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2000-02-22  1346.089966  1358.109985  1331.880005  1352.170044   \n",
       "1  2000-02-23  1352.170044  1370.109985  1342.439941  1360.689941   \n",
       "2  2000-02-24  1360.689941  1364.800049  1329.880005  1353.430054   \n",
       "3  2000-02-25  1353.430054  1362.140015  1329.150024  1333.359985   \n",
       "4  2000-02-28  1333.359985  1360.819946  1325.069946  1348.050049   \n",
       "\n",
       "     Adj Close      Volume  \n",
       "0  1352.170044   980000000  \n",
       "1  1360.689941   993700000  \n",
       "2  1353.430054  1215000000  \n",
       "3  1333.359985  1065200000  \n",
       "4  1348.050049  1026500000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/sp500_20Y_without_covid.csv')\n",
    "df.reindex(index=df.index[::-1])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_to_label(percentage_value):\n",
    "    return 0 if percentage_value < 0 else 1\n",
    "\n",
    "def investor_observes_stocks_for(x_days=60, then_buy_stocks=True, and_sells_them=True, after_y_days=10, dataframe=df):\n",
    "    assert then_buy_stocks\n",
    "    assert and_sells_them\n",
    "    observe_buy_sell_process_length = x_days + after_y_days\n",
    "    \n",
    "    observed_chunks = []\n",
    "    observation_results = []\n",
    "    \n",
    "    for first_day_of_observation in range(len(dataframe) - observe_buy_sell_process_length):\n",
    "        buyout_day = first_day_of_observation + x_days\n",
    "        sell_day = buyout_day + after_y_days\n",
    "        \n",
    "        observed_chunk = dataframe[first_day_of_observation:buyout_day].reset_index()\n",
    "        observation_result = dataframe.iloc[sell_day]\n",
    "        \n",
    "        closing_price_on_buyout_day = dataframe.iloc[buyout_day]['Close']\n",
    "        opening_price_on_sell_day = dataframe.iloc[sell_day]['Open']\n",
    "        \n",
    "        relative_price_change_as_percentage = (opening_price_on_sell_day - closing_price_on_buyout_day) / closing_price_on_buyout_day * 100\n",
    "        \n",
    "        observed_chunks += [observed_chunk]\n",
    "        observation_results += [percentage_to_label(relative_price_change_as_percentage)]\n",
    "    \n",
    "    return observed_chunks, observation_results\n",
    "\n",
    "observed_chunks, observation_results = investor_observes_stocks_for(x_days=OBSERVATION_TIME, then_buy_stocks=True, and_sells_them=True, after_y_days=PREDICTION_AFTER_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>1346.089966</td>\n",
       "      <td>1358.109985</td>\n",
       "      <td>1331.880005</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>980000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1370.109985</td>\n",
       "      <td>1342.439941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>993700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1364.800049</td>\n",
       "      <td>1329.880005</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1215000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1362.140015</td>\n",
       "      <td>1329.150024</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1065200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1360.819946</td>\n",
       "      <td>1325.069946</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1026500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>2000-08-04</td>\n",
       "      <td>1452.560059</td>\n",
       "      <td>1462.930054</td>\n",
       "      <td>1451.310059</td>\n",
       "      <td>1462.930054</td>\n",
       "      <td>1462.930054</td>\n",
       "      <td>956000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>2000-08-07</td>\n",
       "      <td>1462.930054</td>\n",
       "      <td>1480.800049</td>\n",
       "      <td>1460.719971</td>\n",
       "      <td>1479.319946</td>\n",
       "      <td>1479.319946</td>\n",
       "      <td>854800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>2000-08-08</td>\n",
       "      <td>1479.319946</td>\n",
       "      <td>1484.520020</td>\n",
       "      <td>1472.609985</td>\n",
       "      <td>1482.800049</td>\n",
       "      <td>1482.800049</td>\n",
       "      <td>992200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>2000-08-09</td>\n",
       "      <td>1482.800049</td>\n",
       "      <td>1490.329956</td>\n",
       "      <td>1471.160034</td>\n",
       "      <td>1472.869995</td>\n",
       "      <td>1472.869995</td>\n",
       "      <td>1054000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>2000-08-10</td>\n",
       "      <td>1472.869995</td>\n",
       "      <td>1475.150024</td>\n",
       "      <td>1459.890015</td>\n",
       "      <td>1460.250000</td>\n",
       "      <td>1460.250000</td>\n",
       "      <td>940800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index        Date         Open         High          Low        Close  \\\n",
       "0        0  2000-02-22  1346.089966  1358.109985  1331.880005  1352.170044   \n",
       "1        1  2000-02-23  1352.170044  1370.109985  1342.439941  1360.689941   \n",
       "2        2  2000-02-24  1360.689941  1364.800049  1329.880005  1353.430054   \n",
       "3        3  2000-02-25  1353.430054  1362.140015  1329.150024  1333.359985   \n",
       "4        4  2000-02-28  1333.359985  1360.819946  1325.069946  1348.050049   \n",
       "..     ...         ...          ...          ...          ...          ...   \n",
       "115    115  2000-08-04  1452.560059  1462.930054  1451.310059  1462.930054   \n",
       "116    116  2000-08-07  1462.930054  1480.800049  1460.719971  1479.319946   \n",
       "117    117  2000-08-08  1479.319946  1484.520020  1472.609985  1482.800049   \n",
       "118    118  2000-08-09  1482.800049  1490.329956  1471.160034  1472.869995   \n",
       "119    119  2000-08-10  1472.869995  1475.150024  1459.890015  1460.250000   \n",
       "\n",
       "       Adj Close      Volume  \n",
       "0    1352.170044   980000000  \n",
       "1    1360.689941   993700000  \n",
       "2    1353.430054  1215000000  \n",
       "3    1333.359985  1065200000  \n",
       "4    1348.050049  1026500000  \n",
       "..           ...         ...  \n",
       "115  1462.930054   956000000  \n",
       "116  1479.319946   854800000  \n",
       "117  1482.800049   992200000  \n",
       "118  1472.869995  1054000000  \n",
       "119  1460.250000   940800000  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chunk(chunk):\n",
    "    df = chunk.copy()\n",
    "    for label_to_normalize in ['Open', 'High', 'Low', 'Close', 'Adj Close']:\n",
    "        np_arr = df[label_to_normalize].to_numpy()\n",
    "        df[label_to_normalize] = pd.Series((np.array(np_arr) - np.mean(np_arr)) / np.std(np_arr))\n",
    "    return df\n",
    "\n",
    "def normalize_chunks(chunks):\n",
    "    return list(map(lambda x: normalize_chunk(x), chunks))\n",
    "\n",
    "observed_chunks = normalize_chunks(observed_chunks) if NORMALIZED_CHUNKS else observed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    2761\n",
       "0    2144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(observation_results)\n",
    "plt.show()\n",
    "\n",
    "pd.Series(observation_results).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** Selecting test/valiations chunks randomly from the whole set of observation chunks seems to be a big mistake. It causes situation where network actually seen chunks from the future, therefore the more overfitted network, the better results will be. Network should be trained incrementally - with data from time x to x+1 and validated with data from x+1 to x+2. Then learned with data from x to x+2 and validated with data from x+3 to x+3 and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 120, 16)           336       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 120, 16)           64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 120, 8)            520       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 120, 8)            32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 120, 8)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 120, 8)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                61504     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 62,777\n",
      "Trainable params: 62,601\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (OBSERVATION_TIME, 5), filters=16, kernel_size=4, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution1D(filters=8, kernel_size=4, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4414, 120, 5), (4414,), (491, 120, 5), (491,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(list(map(lambda df: df[['Open', 'High', 'Low', 'Close', 'Adj Close']].to_numpy(), observed_chunks)))\n",
    "Y = np.array(observation_results).astype('float32')\n",
    "\n",
    "def chronological_split(X_data, Y_data, test_size=0.25):\n",
    "    training_test_split_index = int((1 - test_size) * len(X_data))\n",
    "    X_train = X_data[:training_test_split_index]\n",
    "    Y_train = Y_data[:training_test_split_index]\n",
    "    X_test = X_data[training_test_split_index:]\n",
    "    Y_test = Y_data[training_test_split_index:]\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "def random_split(X_data, Y_data, test_size=0.25):\n",
    "    return train_test_split(X_data, Y_data, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = random_split(X, Y, TEST_SIZE) if RANDOM_SPLIT else chronological_split(X, Y, TEST_SIZE)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def undersample_and_shuffle(X, Y):\n",
    "    count_class_1, count_class_0 = pd.Series(Y).value_counts()\n",
    "\n",
    "    lower_count = count_class_0 if count_class_0 < count_class_1 else count_class_1\n",
    "    lower_class = 0 if count_class_0 < count_class_1 else 1\n",
    "    \n",
    "    diff = abs(count_class_0 - count_class_1)\n",
    "    \n",
    "    vec0 = random.sample([(x, y) for x, y in zip(X, Y) if y == 0], lower_count)\n",
    "    vec1 = random.sample([(x, y) for x, y in zip(X, Y) if y == 1], lower_count)\n",
    "\n",
    "    \n",
    "    vec = vec0 + vec1\n",
    "    random.shuffle(vec)\n",
    "    return np.array(list(map(lambda x: x[0], vec))), np.array(list(map(lambda x: x[1], vec)))\n",
    "    \n",
    "X_train_balanced, Y_train_balanced = undersample_and_shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    299\n",
       "0.0    192\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(Y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3904 samples, validate on 491 samples\n",
      "Epoch 1/200\n",
      "3904/3904 [==============================] - 1s 283us/step - loss: 0.7714 - tp: 1085.0000 - fp: 1085.0000 - tn: 867.0000 - fn: 867.0000 - accuracy: 0.5000 - precision: 0.5000 - recall: 0.5558 - auc: 0.4979 - val_loss: 0.7365 - val_tp: 99.0000 - val_fp: 59.0000 - val_tn: 133.0000 - val_fn: 200.0000 - val_accuracy: 0.4725 - val_precision: 0.6266 - val_recall: 0.3311 - val_auc: 0.5484\n",
      "Epoch 2/200\n",
      "3904/3904 [==============================] - 0s 118us/step - loss: 0.7213 - tp: 1052.0000 - fp: 933.0000 - tn: 1019.0000 - fn: 900.0000 - accuracy: 0.5305 - precision: 0.5300 - recall: 0.5389 - auc: 0.5380 - val_loss: 0.6982 - val_tp: 168.0000 - val_fp: 103.0000 - val_tn: 89.0000 - val_fn: 131.0000 - val_accuracy: 0.5234 - val_precision: 0.6199 - val_recall: 0.5619 - val_auc: 0.5386\n",
      "Epoch 3/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.7186 - tp: 988.0000 - fp: 932.0000 - tn: 1020.0000 - fn: 964.0000 - accuracy: 0.5143 - precision: 0.5146 - recall: 0.5061 - auc: 0.5245 - val_loss: 0.6955 - val_tp: 122.0000 - val_fp: 64.0000 - val_tn: 128.0000 - val_fn: 177.0000 - val_accuracy: 0.5092 - val_precision: 0.6559 - val_recall: 0.4080 - val_auc: 0.5628\n",
      "Epoch 4/200\n",
      "3904/3904 [==============================] - 0s 114us/step - loss: 0.7039 - tp: 1060.0000 - fp: 915.0000 - tn: 1037.0000 - fn: 892.0000 - accuracy: 0.5371 - precision: 0.5367 - recall: 0.5430 - auc: 0.5477 - val_loss: 0.7011 - val_tp: 179.0000 - val_fp: 103.0000 - val_tn: 89.0000 - val_fn: 120.0000 - val_accuracy: 0.5458 - val_precision: 0.6348 - val_recall: 0.5987 - val_auc: 0.5376\n",
      "Epoch 5/200\n",
      "3904/3904 [==============================] - 0s 120us/step - loss: 0.6978 - tp: 1086.0000 - fp: 928.0000 - tn: 1024.0000 - fn: 866.0000 - accuracy: 0.5405 - precision: 0.5392 - recall: 0.5564 - auc: 0.5586 - val_loss: 0.6963 - val_tp: 148.0000 - val_fp: 86.0000 - val_tn: 106.0000 - val_fn: 151.0000 - val_accuracy: 0.5173 - val_precision: 0.6325 - val_recall: 0.4950 - val_auc: 0.5225\n",
      "Epoch 6/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.6896 - tp: 1055.0000 - fp: 864.0000 - tn: 1088.0000 - fn: 897.0000 - accuracy: 0.5489 - precision: 0.5498 - recall: 0.5405 - auc: 0.5748 - val_loss: 0.6929 - val_tp: 187.0000 - val_fp: 109.0000 - val_tn: 83.0000 - val_fn: 112.0000 - val_accuracy: 0.5499 - val_precision: 0.6318 - val_recall: 0.6254 - val_auc: 0.5255\n",
      "Epoch 7/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.6916 - tp: 1038.0000 - fp: 849.0000 - tn: 1103.0000 - fn: 914.0000 - accuracy: 0.5484 - precision: 0.5501 - recall: 0.5318 - auc: 0.5680 - val_loss: 0.6904 - val_tp: 171.0000 - val_fp: 97.0000 - val_tn: 95.0000 - val_fn: 128.0000 - val_accuracy: 0.5418 - val_precision: 0.6381 - val_recall: 0.5719 - val_auc: 0.5446\n",
      "Epoch 8/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.6935 - tp: 1050.0000 - fp: 920.0000 - tn: 1032.0000 - fn: 902.0000 - accuracy: 0.5333 - precision: 0.5330 - recall: 0.5379 - auc: 0.5567 - val_loss: 0.7065 - val_tp: 176.0000 - val_fp: 106.0000 - val_tn: 86.0000 - val_fn: 123.0000 - val_accuracy: 0.5336 - val_precision: 0.6241 - val_recall: 0.5886 - val_auc: 0.5309\n",
      "Epoch 9/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.6879 - tp: 1098.0000 - fp: 910.0000 - tn: 1042.0000 - fn: 854.0000 - accuracy: 0.5482 - precision: 0.5468 - recall: 0.5625 - auc: 0.5715 - val_loss: 0.6974 - val_tp: 164.0000 - val_fp: 95.0000 - val_tn: 97.0000 - val_fn: 135.0000 - val_accuracy: 0.5316 - val_precision: 0.6332 - val_recall: 0.5485 - val_auc: 0.5412\n",
      "Epoch 10/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6895 - tp: 1084.0000 - fp: 903.0000 - tn: 1049.0000 - fn: 868.0000 - accuracy: 0.5464 - precision: 0.5455 - recall: 0.5553 - auc: 0.5648 - val_loss: 0.7017 - val_tp: 135.0000 - val_fp: 81.0000 - val_tn: 111.0000 - val_fn: 164.0000 - val_accuracy: 0.5010 - val_precision: 0.6250 - val_recall: 0.4515 - val_auc: 0.5336\n",
      "Epoch 11/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.6803 - tp: 1086.0000 - fp: 838.0000 - tn: 1114.0000 - fn: 866.0000 - accuracy: 0.5635 - precision: 0.5644 - recall: 0.5564 - auc: 0.5946 - val_loss: 0.6783 - val_tp: 217.0000 - val_fp: 121.0000 - val_tn: 71.0000 - val_fn: 82.0000 - val_accuracy: 0.5866 - val_precision: 0.6420 - val_recall: 0.7258 - val_auc: 0.5550\n",
      "Epoch 12/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.6768 - tp: 1156.0000 - fp: 864.0000 - tn: 1088.0000 - fn: 796.0000 - accuracy: 0.5748 - precision: 0.5723 - recall: 0.5922 - auc: 0.6017 - val_loss: 0.6919 - val_tp: 163.0000 - val_fp: 93.0000 - val_tn: 99.0000 - val_fn: 136.0000 - val_accuracy: 0.5336 - val_precision: 0.6367 - val_recall: 0.5452 - val_auc: 0.5422\n",
      "Epoch 13/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6801 - tp: 1079.0000 - fp: 833.0000 - tn: 1119.0000 - fn: 873.0000 - accuracy: 0.5630 - precision: 0.5643 - recall: 0.5528 - auc: 0.5946 - val_loss: 0.6909 - val_tp: 150.0000 - val_fp: 83.0000 - val_tn: 109.0000 - val_fn: 149.0000 - val_accuracy: 0.5275 - val_precision: 0.6438 - val_recall: 0.5017 - val_auc: 0.5517\n",
      "Epoch 14/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.6793 - tp: 1107.0000 - fp: 880.0000 - tn: 1072.0000 - fn: 845.0000 - accuracy: 0.5581 - precision: 0.5571 - recall: 0.5671 - auc: 0.5926 - val_loss: 0.6873 - val_tp: 156.0000 - val_fp: 96.0000 - val_tn: 96.0000 - val_fn: 143.0000 - val_accuracy: 0.5132 - val_precision: 0.6190 - val_recall: 0.5217 - val_auc: 0.5600\n",
      "Epoch 15/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.6783 - tp: 1108.0000 - fp: 853.0000 - tn: 1099.0000 - fn: 844.0000 - accuracy: 0.5653 - precision: 0.5650 - recall: 0.5676 - auc: 0.5983 - val_loss: 0.7031 - val_tp: 158.0000 - val_fp: 87.0000 - val_tn: 105.0000 - val_fn: 141.0000 - val_accuracy: 0.5356 - val_precision: 0.6449 - val_recall: 0.5284 - val_auc: 0.5450\n",
      "Epoch 16/200\n",
      "3904/3904 [==============================] - 0s 109us/step - loss: 0.6776 - tp: 1123.0000 - fp: 860.0000 - tn: 1092.0000 - fn: 829.0000 - accuracy: 0.5674 - precision: 0.5663 - recall: 0.5753 - auc: 0.5975 - val_loss: 0.7032 - val_tp: 135.0000 - val_fp: 75.0000 - val_tn: 117.0000 - val_fn: 164.0000 - val_accuracy: 0.5132 - val_precision: 0.6429 - val_recall: 0.4515 - val_auc: 0.5523\n",
      "Epoch 17/200\n",
      "3904/3904 [==============================] - 0s 109us/step - loss: 0.6725 - tp: 1150.0000 - fp: 865.0000 - tn: 1087.0000 - fn: 802.0000 - accuracy: 0.5730 - precision: 0.5707 - recall: 0.5891 - auc: 0.6115 - val_loss: 0.7046 - val_tp: 120.0000 - val_fp: 65.0000 - val_tn: 127.0000 - val_fn: 179.0000 - val_accuracy: 0.5031 - val_precision: 0.6486 - val_recall: 0.4013 - val_auc: 0.5549\n",
      "Epoch 18/200\n",
      "3904/3904 [==============================] - 0s 109us/step - loss: 0.6734 - tp: 1186.0000 - fp: 873.0000 - tn: 1079.0000 - fn: 766.0000 - accuracy: 0.5802 - precision: 0.5760 - recall: 0.6076 - auc: 0.6119 - val_loss: 0.7052 - val_tp: 107.0000 - val_fp: 52.0000 - val_tn: 140.0000 - val_fn: 192.0000 - val_accuracy: 0.5031 - val_precision: 0.6730 - val_recall: 0.3579 - val_auc: 0.5633\n",
      "Epoch 19/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.6701 - tp: 1113.0000 - fp: 791.0000 - tn: 1161.0000 - fn: 839.0000 - accuracy: 0.5825 - precision: 0.5846 - recall: 0.5702 - auc: 0.6179 - val_loss: 0.6955 - val_tp: 131.0000 - val_fp: 66.0000 - val_tn: 126.0000 - val_fn: 168.0000 - val_accuracy: 0.5234 - val_precision: 0.6650 - val_recall: 0.4381 - val_auc: 0.5722\n",
      "Epoch 20/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.6636 - tp: 1130.0000 - fp: 771.0000 - tn: 1181.0000 - fn: 822.0000 - accuracy: 0.5920 - precision: 0.5944 - recall: 0.5789 - auc: 0.6327 - val_loss: 0.7079 - val_tp: 116.0000 - val_fp: 65.0000 - val_tn: 127.0000 - val_fn: 183.0000 - val_accuracy: 0.4949 - val_precision: 0.6409 - val_recall: 0.3880 - val_auc: 0.5607\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.6705 - tp: 1131.0000 - fp: 811.0000 - tn: 1141.0000 - fn: 821.0000 - accuracy: 0.5820 - precision: 0.5824 - recall: 0.5794 - auc: 0.6190 - val_loss: 0.6978 - val_tp: 144.0000 - val_fp: 78.0000 - val_tn: 114.0000 - val_fn: 155.0000 - val_accuracy: 0.5255 - val_precision: 0.6486 - val_recall: 0.4816 - val_auc: 0.5670\n",
      "Epoch 22/200\n",
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.6667 - tp: 1172.0000 - fp: 841.0000 - tn: 1111.0000 - fn: 780.0000 - accuracy: 0.5848 - precision: 0.5822 - recall: 0.6004 - auc: 0.6264 - val_loss: 0.6944 - val_tp: 121.0000 - val_fp: 58.0000 - val_tn: 134.0000 - val_fn: 178.0000 - val_accuracy: 0.5193 - val_precision: 0.6760 - val_recall: 0.4047 - val_auc: 0.5694\n",
      "Epoch 23/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.6647 - tp: 1155.0000 - fp: 799.0000 - tn: 1153.0000 - fn: 797.0000 - accuracy: 0.5912 - precision: 0.5911 - recall: 0.5917 - auc: 0.6290 - val_loss: 0.6938 - val_tp: 134.0000 - val_fp: 65.0000 - val_tn: 127.0000 - val_fn: 165.0000 - val_accuracy: 0.5316 - val_precision: 0.6734 - val_recall: 0.4482 - val_auc: 0.5596\n",
      "Epoch 24/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.6607 - tp: 1170.0000 - fp: 775.0000 - tn: 1177.0000 - fn: 782.0000 - accuracy: 0.6012 - precision: 0.6015 - recall: 0.5994 - auc: 0.6425 - val_loss: 0.6927 - val_tp: 136.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 163.0000 - val_accuracy: 0.5295 - val_precision: 0.6667 - val_recall: 0.4548 - val_auc: 0.5713\n",
      "Epoch 25/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6599 - tp: 1170.0000 - fp: 774.0000 - tn: 1178.0000 - fn: 782.0000 - accuracy: 0.6014 - precision: 0.6019 - recall: 0.5994 - auc: 0.6430 - val_loss: 0.6949 - val_tp: 168.0000 - val_fp: 90.0000 - val_tn: 102.0000 - val_fn: 131.0000 - val_accuracy: 0.5499 - val_precision: 0.6512 - val_recall: 0.5619 - val_auc: 0.5574\n",
      "Epoch 26/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.6606 - tp: 1157.0000 - fp: 793.0000 - tn: 1159.0000 - fn: 795.0000 - accuracy: 0.5932 - precision: 0.5933 - recall: 0.5927 - auc: 0.6355 - val_loss: 0.6974 - val_tp: 153.0000 - val_fp: 82.0000 - val_tn: 110.0000 - val_fn: 146.0000 - val_accuracy: 0.5356 - val_precision: 0.6511 - val_recall: 0.5117 - val_auc: 0.5626\n",
      "Epoch 27/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.6601 - tp: 1146.0000 - fp: 746.0000 - tn: 1206.0000 - fn: 806.0000 - accuracy: 0.6025 - precision: 0.6057 - recall: 0.5871 - auc: 0.6440 - val_loss: 0.6857 - val_tp: 192.0000 - val_fp: 106.0000 - val_tn: 86.0000 - val_fn: 107.0000 - val_accuracy: 0.5662 - val_precision: 0.6443 - val_recall: 0.6421 - val_auc: 0.5626\n",
      "Epoch 28/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.6563 - tp: 1226.0000 - fp: 809.0000 - tn: 1143.0000 - fn: 726.0000 - accuracy: 0.6068 - precision: 0.6025 - recall: 0.6281 - auc: 0.6517 - val_loss: 0.6808 - val_tp: 174.0000 - val_fp: 86.0000 - val_tn: 106.0000 - val_fn: 125.0000 - val_accuracy: 0.5703 - val_precision: 0.6692 - val_recall: 0.5819 - val_auc: 0.5845\n",
      "Epoch 29/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6591 - tp: 1220.0000 - fp: 822.0000 - tn: 1130.0000 - fn: 732.0000 - accuracy: 0.6019 - precision: 0.5975 - recall: 0.6250 - auc: 0.6431 - val_loss: 0.6868 - val_tp: 156.0000 - val_fp: 71.0000 - val_tn: 121.0000 - val_fn: 143.0000 - val_accuracy: 0.5642 - val_precision: 0.6872 - val_recall: 0.5217 - val_auc: 0.5810\n",
      "Epoch 30/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.6554 - tp: 1202.0000 - fp: 777.0000 - tn: 1175.0000 - fn: 750.0000 - accuracy: 0.6089 - precision: 0.6074 - recall: 0.6158 - auc: 0.6528 - val_loss: 0.6791 - val_tp: 197.0000 - val_fp: 98.0000 - val_tn: 94.0000 - val_fn: 102.0000 - val_accuracy: 0.5927 - val_precision: 0.6678 - val_recall: 0.6589 - val_auc: 0.5739\n",
      "Epoch 31/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6515 - tp: 1226.0000 - fp: 811.0000 - tn: 1141.0000 - fn: 726.0000 - accuracy: 0.6063 - precision: 0.6019 - recall: 0.6281 - auc: 0.6583 - val_loss: 0.6779 - val_tp: 171.0000 - val_fp: 96.0000 - val_tn: 96.0000 - val_fn: 128.0000 - val_accuracy: 0.5438 - val_precision: 0.6404 - val_recall: 0.5719 - val_auc: 0.5894\n",
      "Epoch 32/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6487 - tp: 1210.0000 - fp: 760.0000 - tn: 1192.0000 - fn: 742.0000 - accuracy: 0.6153 - precision: 0.6142 - recall: 0.6199 - auc: 0.6647 - val_loss: 0.6857 - val_tp: 158.0000 - val_fp: 82.0000 - val_tn: 110.0000 - val_fn: 141.0000 - val_accuracy: 0.5458 - val_precision: 0.6583 - val_recall: 0.5284 - val_auc: 0.5872\n",
      "Epoch 33/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6529 - tp: 1187.0000 - fp: 756.0000 - tn: 1196.0000 - fn: 765.0000 - accuracy: 0.6104 - precision: 0.6109 - recall: 0.6081 - auc: 0.6579 - val_loss: 0.6930 - val_tp: 126.0000 - val_fp: 53.0000 - val_tn: 139.0000 - val_fn: 173.0000 - val_accuracy: 0.5397 - val_precision: 0.7039 - val_recall: 0.4214 - val_auc: 0.5881\n",
      "Epoch 34/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.6486 - tp: 1181.0000 - fp: 729.0000 - tn: 1223.0000 - fn: 771.0000 - accuracy: 0.6158 - precision: 0.6183 - recall: 0.6050 - auc: 0.6649 - val_loss: 0.6835 - val_tp: 157.0000 - val_fp: 75.0000 - val_tn: 117.0000 - val_fn: 142.0000 - val_accuracy: 0.5580 - val_precision: 0.6767 - val_recall: 0.5251 - val_auc: 0.5975\n",
      "Epoch 35/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.6402 - tp: 1246.0000 - fp: 705.0000 - tn: 1247.0000 - fn: 706.0000 - accuracy: 0.6386 - precision: 0.6386 - recall: 0.6383 - auc: 0.6847 - val_loss: 0.6860 - val_tp: 162.0000 - val_fp: 85.0000 - val_tn: 107.0000 - val_fn: 137.0000 - val_accuracy: 0.5479 - val_precision: 0.6559 - val_recall: 0.5418 - val_auc: 0.5915\n",
      "Epoch 36/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6440 - tp: 1223.0000 - fp: 762.0000 - tn: 1190.0000 - fn: 729.0000 - accuracy: 0.6181 - precision: 0.6161 - recall: 0.6265 - auc: 0.6719 - val_loss: 0.6834 - val_tp: 163.0000 - val_fp: 83.0000 - val_tn: 109.0000 - val_fn: 136.0000 - val_accuracy: 0.5540 - val_precision: 0.6626 - val_recall: 0.5452 - val_auc: 0.5922\n",
      "Epoch 37/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.6390 - tp: 1244.0000 - fp: 708.0000 - tn: 1244.0000 - fn: 708.0000 - accuracy: 0.6373 - precision: 0.6373 - recall: 0.6373 - auc: 0.6859 - val_loss: 0.6841 - val_tp: 165.0000 - val_fp: 84.0000 - val_tn: 108.0000 - val_fn: 134.0000 - val_accuracy: 0.5560 - val_precision: 0.6627 - val_recall: 0.5518 - val_auc: 0.5913\n",
      "Epoch 38/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.6445 - tp: 1197.0000 - fp: 719.0000 - tn: 1233.0000 - fn: 755.0000 - accuracy: 0.6224 - precision: 0.6247 - recall: 0.6132 - auc: 0.6721 - val_loss: 0.6783 - val_tp: 159.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 140.0000 - val_accuracy: 0.5764 - val_precision: 0.7004 - val_recall: 0.5318 - val_auc: 0.6041\n",
      "Epoch 39/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6389 - tp: 1222.0000 - fp: 723.0000 - tn: 1229.0000 - fn: 730.0000 - accuracy: 0.6278 - precision: 0.6283 - recall: 0.6260 - auc: 0.6829 - val_loss: 0.6800 - val_tp: 160.0000 - val_fp: 66.0000 - val_tn: 126.0000 - val_fn: 139.0000 - val_accuracy: 0.5825 - val_precision: 0.7080 - val_recall: 0.5351 - val_auc: 0.6034\n",
      "Epoch 40/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6395 - tp: 1233.0000 - fp: 725.0000 - tn: 1227.0000 - fn: 719.0000 - accuracy: 0.6301 - precision: 0.6297 - recall: 0.6317 - auc: 0.6810 - val_loss: 0.6745 - val_tp: 165.0000 - val_fp: 79.0000 - val_tn: 113.0000 - val_fn: 134.0000 - val_accuracy: 0.5662 - val_precision: 0.6762 - val_recall: 0.5518 - val_auc: 0.6085\n",
      "Epoch 41/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.6363 - tp: 1253.0000 - fp: 740.0000 - tn: 1212.0000 - fn: 699.0000 - accuracy: 0.6314 - precision: 0.6287 - recall: 0.6419 - auc: 0.6843 - val_loss: 0.6828 - val_tp: 162.0000 - val_fp: 72.0000 - val_tn: 120.0000 - val_fn: 137.0000 - val_accuracy: 0.5743 - val_precision: 0.6923 - val_recall: 0.5418 - val_auc: 0.5984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "3904/3904 [==============================] - 0s 102us/step - loss: 0.6333 - tp: 1251.0000 - fp: 693.0000 - tn: 1259.0000 - fn: 701.0000 - accuracy: 0.6429 - precision: 0.6435 - recall: 0.6409 - auc: 0.6940 - val_loss: 0.6804 - val_tp: 165.0000 - val_fp: 82.0000 - val_tn: 110.0000 - val_fn: 134.0000 - val_accuracy: 0.5601 - val_precision: 0.6680 - val_recall: 0.5518 - val_auc: 0.5982\n",
      "Epoch 43/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6344 - tp: 1219.0000 - fp: 715.0000 - tn: 1237.0000 - fn: 733.0000 - accuracy: 0.6291 - precision: 0.6303 - recall: 0.6245 - auc: 0.6883 - val_loss: 0.6865 - val_tp: 154.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 145.0000 - val_accuracy: 0.5642 - val_precision: 0.6906 - val_recall: 0.5151 - val_auc: 0.5980\n",
      "Epoch 44/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.6313 - tp: 1247.0000 - fp: 687.0000 - tn: 1265.0000 - fn: 705.0000 - accuracy: 0.6434 - precision: 0.6448 - recall: 0.6388 - auc: 0.6969 - val_loss: 0.6815 - val_tp: 166.0000 - val_fp: 86.0000 - val_tn: 106.0000 - val_fn: 133.0000 - val_accuracy: 0.5540 - val_precision: 0.6587 - val_recall: 0.5552 - val_auc: 0.6033\n",
      "Epoch 45/200\n",
      "3904/3904 [==============================] - 0s 102us/step - loss: 0.6332 - tp: 1259.0000 - fp: 720.0000 - tn: 1232.0000 - fn: 693.0000 - accuracy: 0.6381 - precision: 0.6362 - recall: 0.6450 - auc: 0.6929 - val_loss: 0.6773 - val_tp: 157.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 142.0000 - val_accuracy: 0.5723 - val_precision: 0.6978 - val_recall: 0.5251 - val_auc: 0.6172\n",
      "Epoch 46/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6258 - tp: 1288.0000 - fp: 714.0000 - tn: 1238.0000 - fn: 664.0000 - accuracy: 0.6470 - precision: 0.6434 - recall: 0.6598 - auc: 0.7045 - val_loss: 0.6822 - val_tp: 172.0000 - val_fp: 88.0000 - val_tn: 104.0000 - val_fn: 127.0000 - val_accuracy: 0.5621 - val_precision: 0.6615 - val_recall: 0.5753 - val_auc: 0.6062\n",
      "Epoch 47/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6280 - tp: 1261.0000 - fp: 718.0000 - tn: 1234.0000 - fn: 691.0000 - accuracy: 0.6391 - precision: 0.6372 - recall: 0.6460 - auc: 0.7002 - val_loss: 0.6979 - val_tp: 139.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 160.0000 - val_accuracy: 0.5336 - val_precision: 0.6683 - val_recall: 0.4649 - val_auc: 0.5921\n",
      "Epoch 48/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6256 - tp: 1274.0000 - fp: 692.0000 - tn: 1260.0000 - fn: 678.0000 - accuracy: 0.6491 - precision: 0.6480 - recall: 0.6527 - auc: 0.7074 - val_loss: 0.6935 - val_tp: 146.0000 - val_fp: 71.0000 - val_tn: 121.0000 - val_fn: 153.0000 - val_accuracy: 0.5438 - val_precision: 0.6728 - val_recall: 0.4883 - val_auc: 0.5967\n",
      "Epoch 49/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6203 - tp: 1271.0000 - fp: 662.0000 - tn: 1290.0000 - fn: 681.0000 - accuracy: 0.6560 - precision: 0.6575 - recall: 0.6511 - auc: 0.7139 - val_loss: 0.6845 - val_tp: 150.0000 - val_fp: 56.0000 - val_tn: 136.0000 - val_fn: 149.0000 - val_accuracy: 0.5825 - val_precision: 0.7282 - val_recall: 0.5017 - val_auc: 0.6129\n",
      "Epoch 50/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.6227 - tp: 1230.0000 - fp: 674.0000 - tn: 1278.0000 - fn: 722.0000 - accuracy: 0.6424 - precision: 0.6460 - recall: 0.6301 - auc: 0.7071 - val_loss: 0.6877 - val_tp: 161.0000 - val_fp: 71.0000 - val_tn: 121.0000 - val_fn: 138.0000 - val_accuracy: 0.5743 - val_precision: 0.6940 - val_recall: 0.5385 - val_auc: 0.5995\n",
      "Epoch 51/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.6143 - tp: 1310.0000 - fp: 704.0000 - tn: 1248.0000 - fn: 642.0000 - accuracy: 0.6552 - precision: 0.6504 - recall: 0.6711 - auc: 0.7203 - val_loss: 0.6901 - val_tp: 151.0000 - val_fp: 72.0000 - val_tn: 120.0000 - val_fn: 148.0000 - val_accuracy: 0.5519 - val_precision: 0.6771 - val_recall: 0.5050 - val_auc: 0.6021\n",
      "Epoch 52/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6201 - tp: 1266.0000 - fp: 688.0000 - tn: 1264.0000 - fn: 686.0000 - accuracy: 0.6481 - precision: 0.6479 - recall: 0.6486 - auc: 0.7091 - val_loss: 0.6861 - val_tp: 161.0000 - val_fp: 75.0000 - val_tn: 117.0000 - val_fn: 138.0000 - val_accuracy: 0.5662 - val_precision: 0.6822 - val_recall: 0.5385 - val_auc: 0.6074\n",
      "Epoch 53/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6112 - tp: 1292.0000 - fp: 664.0000 - tn: 1288.0000 - fn: 660.0000 - accuracy: 0.6609 - precision: 0.6605 - recall: 0.6619 - auc: 0.7262 - val_loss: 0.6768 - val_tp: 167.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 132.0000 - val_accuracy: 0.5906 - val_precision: 0.7076 - val_recall: 0.5585 - val_auc: 0.6261\n",
      "Epoch 54/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.6151 - tp: 1277.0000 - fp: 658.0000 - tn: 1294.0000 - fn: 675.0000 - accuracy: 0.6586 - precision: 0.6599 - recall: 0.6542 - auc: 0.7209 - val_loss: 0.6868 - val_tp: 162.0000 - val_fp: 73.0000 - val_tn: 119.0000 - val_fn: 137.0000 - val_accuracy: 0.5723 - val_precision: 0.6894 - val_recall: 0.5418 - val_auc: 0.6127\n",
      "Epoch 55/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.6128 - tp: 1314.0000 - fp: 682.0000 - tn: 1270.0000 - fn: 638.0000 - accuracy: 0.6619 - precision: 0.6583 - recall: 0.6732 - auc: 0.7236 - val_loss: 0.6959 - val_tp: 141.0000 - val_fp: 52.0000 - val_tn: 140.0000 - val_fn: 158.0000 - val_accuracy: 0.5723 - val_precision: 0.7306 - val_recall: 0.4716 - val_auc: 0.6169\n",
      "Epoch 56/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.6085 - tp: 1251.0000 - fp: 627.0000 - tn: 1325.0000 - fn: 701.0000 - accuracy: 0.6598 - precision: 0.6661 - recall: 0.6409 - auc: 0.7283 - val_loss: 0.6778 - val_tp: 157.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 142.0000 - val_accuracy: 0.5723 - val_precision: 0.6978 - val_recall: 0.5251 - val_auc: 0.6247\n",
      "Epoch 57/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6052 - tp: 1299.0000 - fp: 648.0000 - tn: 1304.0000 - fn: 653.0000 - accuracy: 0.6668 - precision: 0.6672 - recall: 0.6655 - auc: 0.7327 - val_loss: 0.6796 - val_tp: 165.0000 - val_fp: 66.0000 - val_tn: 126.0000 - val_fn: 134.0000 - val_accuracy: 0.5927 - val_precision: 0.7143 - val_recall: 0.5518 - val_auc: 0.6165\n",
      "Epoch 58/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.6127 - tp: 1271.0000 - fp: 685.0000 - tn: 1267.0000 - fn: 681.0000 - accuracy: 0.6501 - precision: 0.6498 - recall: 0.6511 - auc: 0.7215 - val_loss: 0.6788 - val_tp: 165.0000 - val_fp: 78.0000 - val_tn: 114.0000 - val_fn: 134.0000 - val_accuracy: 0.5682 - val_precision: 0.6790 - val_recall: 0.5518 - val_auc: 0.6038\n",
      "Epoch 59/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6060 - tp: 1289.0000 - fp: 629.0000 - tn: 1323.0000 - fn: 663.0000 - accuracy: 0.6691 - precision: 0.6721 - recall: 0.6603 - auc: 0.7323 - val_loss: 0.6811 - val_tp: 165.0000 - val_fp: 70.0000 - val_tn: 122.0000 - val_fn: 134.0000 - val_accuracy: 0.5845 - val_precision: 0.7021 - val_recall: 0.5518 - val_auc: 0.6158\n",
      "Epoch 60/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.6043 - tp: 1314.0000 - fp: 675.0000 - tn: 1277.0000 - fn: 638.0000 - accuracy: 0.6637 - precision: 0.6606 - recall: 0.6732 - auc: 0.7326 - val_loss: 0.6918 - val_tp: 154.0000 - val_fp: 65.0000 - val_tn: 127.0000 - val_fn: 145.0000 - val_accuracy: 0.5723 - val_precision: 0.7032 - val_recall: 0.5151 - val_auc: 0.6037\n",
      "Epoch 61/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.5981 - tp: 1317.0000 - fp: 634.0000 - tn: 1318.0000 - fn: 635.0000 - accuracy: 0.6749 - precision: 0.6750 - recall: 0.6747 - auc: 0.7447 - val_loss: 0.6891 - val_tp: 149.0000 - val_fp: 61.0000 - val_tn: 131.0000 - val_fn: 150.0000 - val_accuracy: 0.5703 - val_precision: 0.7095 - val_recall: 0.4983 - val_auc: 0.6139\n",
      "Epoch 62/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.6026 - tp: 1318.0000 - fp: 664.0000 - tn: 1288.0000 - fn: 634.0000 - accuracy: 0.6675 - precision: 0.6650 - recall: 0.6752 - auc: 0.7343 - val_loss: 0.6865 - val_tp: 155.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 144.0000 - val_accuracy: 0.5682 - val_precision: 0.6951 - val_recall: 0.5184 - val_auc: 0.6116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.5971 - tp: 1282.0000 - fp: 615.0000 - tn: 1337.0000 - fn: 670.0000 - accuracy: 0.6709 - precision: 0.6758 - recall: 0.6568 - auc: 0.7409 - val_loss: 0.6824 - val_tp: 158.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 141.0000 - val_accuracy: 0.5743 - val_precision: 0.6991 - val_recall: 0.5284 - val_auc: 0.6198\n",
      "Epoch 64/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.5992 - tp: 1322.0000 - fp: 637.0000 - tn: 1315.0000 - fn: 630.0000 - accuracy: 0.6755 - precision: 0.6748 - recall: 0.6773 - auc: 0.7411 - val_loss: 0.6759 - val_tp: 172.0000 - val_fp: 67.0000 - val_tn: 125.0000 - val_fn: 127.0000 - val_accuracy: 0.6049 - val_precision: 0.7197 - val_recall: 0.5753 - val_auc: 0.6327\n",
      "Epoch 65/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.5956 - tp: 1343.0000 - fp: 644.0000 - tn: 1308.0000 - fn: 609.0000 - accuracy: 0.6790 - precision: 0.6759 - recall: 0.6880 - auc: 0.7467 - val_loss: 0.6773 - val_tp: 167.0000 - val_fp: 75.0000 - val_tn: 117.0000 - val_fn: 132.0000 - val_accuracy: 0.5784 - val_precision: 0.6901 - val_recall: 0.5585 - val_auc: 0.6364\n",
      "Epoch 66/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.5876 - tp: 1331.0000 - fp: 624.0000 - tn: 1328.0000 - fn: 621.0000 - accuracy: 0.6811 - precision: 0.6808 - recall: 0.6819 - auc: 0.7555 - val_loss: 0.6736 - val_tp: 169.0000 - val_fp: 72.0000 - val_tn: 120.0000 - val_fn: 130.0000 - val_accuracy: 0.5886 - val_precision: 0.7012 - val_recall: 0.5652 - val_auc: 0.6243\n",
      "Epoch 67/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.5833 - tp: 1363.0000 - fp: 615.0000 - tn: 1337.0000 - fn: 589.0000 - accuracy: 0.6916 - precision: 0.6891 - recall: 0.6983 - auc: 0.7629 - val_loss: 0.6776 - val_tp: 170.0000 - val_fp: 80.0000 - val_tn: 112.0000 - val_fn: 129.0000 - val_accuracy: 0.5743 - val_precision: 0.6800 - val_recall: 0.5686 - val_auc: 0.6190\n",
      "Epoch 68/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.5916 - tp: 1309.0000 - fp: 616.0000 - tn: 1336.0000 - fn: 643.0000 - accuracy: 0.6775 - precision: 0.6800 - recall: 0.6706 - auc: 0.7508 - val_loss: 0.6856 - val_tp: 154.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 145.0000 - val_accuracy: 0.5825 - val_precision: 0.7196 - val_recall: 0.5151 - val_auc: 0.6240\n",
      "Epoch 69/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.5917 - tp: 1343.0000 - fp: 648.0000 - tn: 1304.0000 - fn: 609.0000 - accuracy: 0.6780 - precision: 0.6745 - recall: 0.6880 - auc: 0.7513 - val_loss: 0.6843 - val_tp: 149.0000 - val_fp: 53.0000 - val_tn: 139.0000 - val_fn: 150.0000 - val_accuracy: 0.5866 - val_precision: 0.7376 - val_recall: 0.4983 - val_auc: 0.6384\n",
      "Epoch 70/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5847 - tp: 1347.0000 - fp: 605.0000 - tn: 1347.0000 - fn: 605.0000 - accuracy: 0.6901 - precision: 0.6901 - recall: 0.6901 - auc: 0.7575 - val_loss: 0.6736 - val_tp: 177.0000 - val_fp: 75.0000 - val_tn: 117.0000 - val_fn: 122.0000 - val_accuracy: 0.5988 - val_precision: 0.7024 - val_recall: 0.5920 - val_auc: 0.6315\n",
      "Epoch 71/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5822 - tp: 1332.0000 - fp: 615.0000 - tn: 1337.0000 - fn: 620.0000 - accuracy: 0.6837 - precision: 0.6841 - recall: 0.6824 - auc: 0.7606 - val_loss: 0.6743 - val_tp: 162.0000 - val_fp: 66.0000 - val_tn: 126.0000 - val_fn: 137.0000 - val_accuracy: 0.5866 - val_precision: 0.7105 - val_recall: 0.5418 - val_auc: 0.6378\n",
      "Epoch 72/200\n",
      "3904/3904 [==============================] - 0s 102us/step - loss: 0.5751 - tp: 1340.0000 - fp: 571.0000 - tn: 1381.0000 - fn: 612.0000 - accuracy: 0.6970 - precision: 0.7012 - recall: 0.6865 - auc: 0.7704 - val_loss: 0.6899 - val_tp: 163.0000 - val_fp: 76.0000 - val_tn: 116.0000 - val_fn: 136.0000 - val_accuracy: 0.5682 - val_precision: 0.6820 - val_recall: 0.5452 - val_auc: 0.6177\n",
      "Epoch 73/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5771 - tp: 1337.0000 - fp: 602.0000 - tn: 1350.0000 - fn: 615.0000 - accuracy: 0.6883 - precision: 0.6895 - recall: 0.6849 - auc: 0.7676 - val_loss: 0.6710 - val_tp: 184.0000 - val_fp: 97.0000 - val_tn: 95.0000 - val_fn: 115.0000 - val_accuracy: 0.5682 - val_precision: 0.6548 - val_recall: 0.6154 - val_auc: 0.6164\n",
      "Epoch 74/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.5761 - tp: 1377.0000 - fp: 604.0000 - tn: 1348.0000 - fn: 575.0000 - accuracy: 0.6980 - precision: 0.6951 - recall: 0.7054 - auc: 0.7692 - val_loss: 0.6819 - val_tp: 164.0000 - val_fp: 75.0000 - val_tn: 117.0000 - val_fn: 135.0000 - val_accuracy: 0.5723 - val_precision: 0.6862 - val_recall: 0.5485 - val_auc: 0.6252\n",
      "Epoch 75/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.5771 - tp: 1356.0000 - fp: 605.0000 - tn: 1347.0000 - fn: 596.0000 - accuracy: 0.6924 - precision: 0.6915 - recall: 0.6947 - auc: 0.7665 - val_loss: 0.6699 - val_tp: 180.0000 - val_fp: 80.0000 - val_tn: 112.0000 - val_fn: 119.0000 - val_accuracy: 0.5947 - val_precision: 0.6923 - val_recall: 0.6020 - val_auc: 0.6320\n",
      "Epoch 76/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5691 - tp: 1381.0000 - fp: 587.0000 - tn: 1365.0000 - fn: 571.0000 - accuracy: 0.7034 - precision: 0.7017 - recall: 0.7075 - auc: 0.7758 - val_loss: 0.6718 - val_tp: 177.0000 - val_fp: 76.0000 - val_tn: 116.0000 - val_fn: 122.0000 - val_accuracy: 0.5967 - val_precision: 0.6996 - val_recall: 0.5920 - val_auc: 0.6338\n",
      "Epoch 77/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5610 - tp: 1413.0000 - fp: 587.0000 - tn: 1365.0000 - fn: 539.0000 - accuracy: 0.7116 - precision: 0.7065 - recall: 0.7239 - auc: 0.7870 - val_loss: 0.6862 - val_tp: 163.0000 - val_fp: 71.0000 - val_tn: 121.0000 - val_fn: 136.0000 - val_accuracy: 0.5784 - val_precision: 0.6966 - val_recall: 0.5452 - val_auc: 0.6185\n",
      "Epoch 78/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.5691 - tp: 1316.0000 - fp: 575.0000 - tn: 1377.0000 - fn: 636.0000 - accuracy: 0.6898 - precision: 0.6959 - recall: 0.6742 - auc: 0.7721 - val_loss: 0.6836 - val_tp: 175.0000 - val_fp: 76.0000 - val_tn: 116.0000 - val_fn: 124.0000 - val_accuracy: 0.5927 - val_precision: 0.6972 - val_recall: 0.5853 - val_auc: 0.6136\n",
      "Epoch 79/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.5693 - tp: 1372.0000 - fp: 579.0000 - tn: 1373.0000 - fn: 580.0000 - accuracy: 0.7031 - precision: 0.7032 - recall: 0.7029 - auc: 0.7751 - val_loss: 0.6814 - val_tp: 162.0000 - val_fp: 74.0000 - val_tn: 118.0000 - val_fn: 137.0000 - val_accuracy: 0.5703 - val_precision: 0.6864 - val_recall: 0.5418 - val_auc: 0.6168\n",
      "Epoch 80/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.5653 - tp: 1390.0000 - fp: 579.0000 - tn: 1373.0000 - fn: 562.0000 - accuracy: 0.7077 - precision: 0.7059 - recall: 0.7121 - auc: 0.7816 - val_loss: 0.6723 - val_tp: 172.0000 - val_fp: 76.0000 - val_tn: 116.0000 - val_fn: 127.0000 - val_accuracy: 0.5866 - val_precision: 0.6935 - val_recall: 0.5753 - val_auc: 0.6384\n",
      "Epoch 81/200\n",
      "3904/3904 [==============================] - 0s 109us/step - loss: 0.5602 - tp: 1388.0000 - fp: 569.0000 - tn: 1383.0000 - fn: 564.0000 - accuracy: 0.7098 - precision: 0.7092 - recall: 0.7111 - auc: 0.7858 - val_loss: 0.6748 - val_tp: 162.0000 - val_fp: 67.0000 - val_tn: 125.0000 - val_fn: 137.0000 - val_accuracy: 0.5845 - val_precision: 0.7074 - val_recall: 0.5418 - val_auc: 0.6341\n",
      "Epoch 82/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.5582 - tp: 1381.0000 - fp: 574.0000 - tn: 1378.0000 - fn: 571.0000 - accuracy: 0.7067 - precision: 0.7064 - recall: 0.7075 - auc: 0.7876 - val_loss: 0.6943 - val_tp: 165.0000 - val_fp: 70.0000 - val_tn: 122.0000 - val_fn: 134.0000 - val_accuracy: 0.5845 - val_precision: 0.7021 - val_recall: 0.5518 - val_auc: 0.6230\n",
      "Epoch 83/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.5612 - tp: 1404.0000 - fp: 560.0000 - tn: 1392.0000 - fn: 548.0000 - accuracy: 0.7162 - precision: 0.7149 - recall: 0.7193 - auc: 0.7852 - val_loss: 0.6956 - val_tp: 157.0000 - val_fp: 70.0000 - val_tn: 122.0000 - val_fn: 142.0000 - val_accuracy: 0.5682 - val_precision: 0.6916 - val_recall: 0.5251 - val_auc: 0.6211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.5517 - tp: 1404.0000 - fp: 566.0000 - tn: 1386.0000 - fn: 548.0000 - accuracy: 0.7147 - precision: 0.7127 - recall: 0.7193 - auc: 0.7954 - val_loss: 0.6813 - val_tp: 171.0000 - val_fp: 65.0000 - val_tn: 127.0000 - val_fn: 128.0000 - val_accuracy: 0.6069 - val_precision: 0.7246 - val_recall: 0.5719 - val_auc: 0.6303\n",
      "Epoch 85/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.5510 - tp: 1432.0000 - fp: 553.0000 - tn: 1399.0000 - fn: 520.0000 - accuracy: 0.7252 - precision: 0.7214 - recall: 0.7336 - auc: 0.7966 - val_loss: 0.6858 - val_tp: 163.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 136.0000 - val_accuracy: 0.5947 - val_precision: 0.7212 - val_recall: 0.5452 - val_auc: 0.6342\n",
      "Epoch 86/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.5490 - tp: 1400.0000 - fp: 539.0000 - tn: 1413.0000 - fn: 552.0000 - accuracy: 0.7205 - precision: 0.7220 - recall: 0.7172 - auc: 0.7977 - val_loss: 0.6814 - val_tp: 175.0000 - val_fp: 74.0000 - val_tn: 118.0000 - val_fn: 124.0000 - val_accuracy: 0.5967 - val_precision: 0.7028 - val_recall: 0.5853 - val_auc: 0.6281\n",
      "Epoch 87/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.5478 - tp: 1432.0000 - fp: 590.0000 - tn: 1362.0000 - fn: 520.0000 - accuracy: 0.7157 - precision: 0.7082 - recall: 0.7336 - auc: 0.7982 - val_loss: 0.6753 - val_tp: 163.0000 - val_fp: 61.0000 - val_tn: 131.0000 - val_fn: 136.0000 - val_accuracy: 0.5988 - val_precision: 0.7277 - val_recall: 0.5452 - val_auc: 0.6501\n",
      "Epoch 88/200\n",
      "3904/3904 [==============================] - 0s 109us/step - loss: 0.5519 - tp: 1404.0000 - fp: 579.0000 - tn: 1373.0000 - fn: 548.0000 - accuracy: 0.7113 - precision: 0.7080 - recall: 0.7193 - auc: 0.7916 - val_loss: 0.7208 - val_tp: 137.0000 - val_fp: 53.0000 - val_tn: 139.0000 - val_fn: 162.0000 - val_accuracy: 0.5621 - val_precision: 0.7211 - val_recall: 0.4582 - val_auc: 0.6240\n",
      "Epoch 89/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.5483 - tp: 1378.0000 - fp: 527.0000 - tn: 1425.0000 - fn: 574.0000 - accuracy: 0.7180 - precision: 0.7234 - recall: 0.7059 - auc: 0.7973 - val_loss: 0.6852 - val_tp: 163.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 136.0000 - val_accuracy: 0.5947 - val_precision: 0.7212 - val_recall: 0.5452 - val_auc: 0.6442\n",
      "Epoch 90/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.5425 - tp: 1402.0000 - fp: 539.0000 - tn: 1413.0000 - fn: 550.0000 - accuracy: 0.7211 - precision: 0.7223 - recall: 0.7182 - auc: 0.8034 - val_loss: 0.6759 - val_tp: 168.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 131.0000 - val_accuracy: 0.5927 - val_precision: 0.7089 - val_recall: 0.5619 - val_auc: 0.6454\n",
      "Epoch 91/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.5480 - tp: 1409.0000 - fp: 556.0000 - tn: 1396.0000 - fn: 543.0000 - accuracy: 0.7185 - precision: 0.7170 - recall: 0.7218 - auc: 0.7977 - val_loss: 0.6728 - val_tp: 168.0000 - val_fp: 76.0000 - val_tn: 116.0000 - val_fn: 131.0000 - val_accuracy: 0.5784 - val_precision: 0.6885 - val_recall: 0.5619 - val_auc: 0.6393\n",
      "Epoch 92/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.5482 - tp: 1406.0000 - fp: 545.0000 - tn: 1407.0000 - fn: 546.0000 - accuracy: 0.7205 - precision: 0.7207 - recall: 0.7203 - auc: 0.7964 - val_loss: 0.6781 - val_tp: 162.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 137.0000 - val_accuracy: 0.5825 - val_precision: 0.7043 - val_recall: 0.5418 - val_auc: 0.6503\n",
      "Epoch 93/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.5392 - tp: 1400.0000 - fp: 513.0000 - tn: 1439.0000 - fn: 552.0000 - accuracy: 0.7272 - precision: 0.7318 - recall: 0.7172 - auc: 0.8069 - val_loss: 0.6810 - val_tp: 158.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 141.0000 - val_accuracy: 0.5723 - val_precision: 0.6960 - val_recall: 0.5284 - val_auc: 0.6384\n",
      "Epoch 94/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5398 - tp: 1434.0000 - fp: 537.0000 - tn: 1415.0000 - fn: 518.0000 - accuracy: 0.7298 - precision: 0.7275 - recall: 0.7346 - auc: 0.8084 - val_loss: 0.6793 - val_tp: 158.0000 - val_fp: 70.0000 - val_tn: 122.0000 - val_fn: 141.0000 - val_accuracy: 0.5703 - val_precision: 0.6930 - val_recall: 0.5284 - val_auc: 0.6339\n",
      "Epoch 95/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5352 - tp: 1449.0000 - fp: 545.0000 - tn: 1407.0000 - fn: 503.0000 - accuracy: 0.7316 - precision: 0.7267 - recall: 0.7423 - auc: 0.8118 - val_loss: 0.6990 - val_tp: 149.0000 - val_fp: 57.0000 - val_tn: 135.0000 - val_fn: 150.0000 - val_accuracy: 0.5784 - val_precision: 0.7233 - val_recall: 0.4983 - val_auc: 0.6370\n",
      "Epoch 96/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.5261 - tp: 1443.0000 - fp: 492.0000 - tn: 1460.0000 - fn: 509.0000 - accuracy: 0.7436 - precision: 0.7457 - recall: 0.7392 - auc: 0.8215 - val_loss: 0.6719 - val_tp: 160.0000 - val_fp: 70.0000 - val_tn: 122.0000 - val_fn: 139.0000 - val_accuracy: 0.5743 - val_precision: 0.6957 - val_recall: 0.5351 - val_auc: 0.6528\n",
      "Epoch 97/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.5375 - tp: 1424.0000 - fp: 540.0000 - tn: 1412.0000 - fn: 528.0000 - accuracy: 0.7264 - precision: 0.7251 - recall: 0.7295 - auc: 0.8055 - val_loss: 0.6818 - val_tp: 166.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 133.0000 - val_accuracy: 0.5886 - val_precision: 0.7064 - val_recall: 0.5552 - val_auc: 0.6447\n",
      "Epoch 98/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.5318 - tp: 1449.0000 - fp: 538.0000 - tn: 1414.0000 - fn: 503.0000 - accuracy: 0.7334 - precision: 0.7292 - recall: 0.7423 - auc: 0.8129 - val_loss: 0.6853 - val_tp: 151.0000 - val_fp: 59.0000 - val_tn: 133.0000 - val_fn: 148.0000 - val_accuracy: 0.5784 - val_precision: 0.7190 - val_recall: 0.5050 - val_auc: 0.6442\n",
      "Epoch 99/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.5219 - tp: 1424.0000 - fp: 503.0000 - tn: 1449.0000 - fn: 528.0000 - accuracy: 0.7359 - precision: 0.7390 - recall: 0.7295 - auc: 0.8211 - val_loss: 0.7010 - val_tp: 148.0000 - val_fp: 57.0000 - val_tn: 135.0000 - val_fn: 151.0000 - val_accuracy: 0.5764 - val_precision: 0.7220 - val_recall: 0.4950 - val_auc: 0.6475\n",
      "Epoch 100/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.5215 - tp: 1450.0000 - fp: 526.0000 - tn: 1426.0000 - fn: 502.0000 - accuracy: 0.7367 - precision: 0.7338 - recall: 0.7428 - auc: 0.8218 - val_loss: 0.6915 - val_tp: 149.0000 - val_fp: 53.0000 - val_tn: 139.0000 - val_fn: 150.0000 - val_accuracy: 0.5866 - val_precision: 0.7376 - val_recall: 0.4983 - val_auc: 0.6471\n",
      "Epoch 101/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.5281 - tp: 1426.0000 - fp: 507.0000 - tn: 1445.0000 - fn: 526.0000 - accuracy: 0.7354 - precision: 0.7377 - recall: 0.7305 - auc: 0.8161 - val_loss: 0.6821 - val_tp: 165.0000 - val_fp: 70.0000 - val_tn: 122.0000 - val_fn: 134.0000 - val_accuracy: 0.5845 - val_precision: 0.7021 - val_recall: 0.5518 - val_auc: 0.6569\n",
      "Epoch 102/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.5246 - tp: 1436.0000 - fp: 528.0000 - tn: 1424.0000 - fn: 516.0000 - accuracy: 0.7326 - precision: 0.7312 - recall: 0.7357 - auc: 0.8176 - val_loss: 0.6877 - val_tp: 157.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 142.0000 - val_accuracy: 0.5988 - val_precision: 0.7406 - val_recall: 0.5251 - val_auc: 0.6574\n",
      "Epoch 103/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.5314 - tp: 1424.0000 - fp: 496.0000 - tn: 1456.0000 - fn: 528.0000 - accuracy: 0.7377 - precision: 0.7417 - recall: 0.7295 - auc: 0.8121 - val_loss: 0.6870 - val_tp: 154.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 145.0000 - val_accuracy: 0.5764 - val_precision: 0.7097 - val_recall: 0.5151 - val_auc: 0.6540\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904/3904 [==============================] - 0s 102us/step - loss: 0.5173 - tp: 1451.0000 - fp: 507.0000 - tn: 1445.0000 - fn: 501.0000 - accuracy: 0.7418 - precision: 0.7411 - recall: 0.7433 - auc: 0.8263 - val_loss: 0.6870 - val_tp: 158.0000 - val_fp: 61.0000 - val_tn: 131.0000 - val_fn: 141.0000 - val_accuracy: 0.5886 - val_precision: 0.7215 - val_recall: 0.5284 - val_auc: 0.6401\n",
      "Epoch 105/200\n",
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.5186 - tp: 1445.0000 - fp: 514.0000 - tn: 1438.0000 - fn: 507.0000 - accuracy: 0.7385 - precision: 0.7376 - recall: 0.7403 - auc: 0.8242 - val_loss: 0.6777 - val_tp: 161.0000 - val_fp: 66.0000 - val_tn: 126.0000 - val_fn: 138.0000 - val_accuracy: 0.5845 - val_precision: 0.7093 - val_recall: 0.5385 - val_auc: 0.6533\n",
      "Epoch 106/200\n",
      "3904/3904 [==============================] - 0s 102us/step - loss: 0.5077 - tp: 1458.0000 - fp: 479.0000 - tn: 1473.0000 - fn: 494.0000 - accuracy: 0.7508 - precision: 0.7527 - recall: 0.7469 - auc: 0.8339 - val_loss: 0.7142 - val_tp: 132.0000 - val_fp: 45.0000 - val_tn: 147.0000 - val_fn: 167.0000 - val_accuracy: 0.5682 - val_precision: 0.7458 - val_recall: 0.4415 - val_auc: 0.6454\n",
      "Epoch 107/200\n",
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.5141 - tp: 1454.0000 - fp: 494.0000 - tn: 1458.0000 - fn: 498.0000 - accuracy: 0.7459 - precision: 0.7464 - recall: 0.7449 - auc: 0.8283 - val_loss: 0.6991 - val_tp: 145.0000 - val_fp: 46.0000 - val_tn: 146.0000 - val_fn: 154.0000 - val_accuracy: 0.5927 - val_precision: 0.7592 - val_recall: 0.4849 - val_auc: 0.6575\n",
      "Epoch 108/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.5165 - tp: 1446.0000 - fp: 519.0000 - tn: 1433.0000 - fn: 506.0000 - accuracy: 0.7374 - precision: 0.7359 - recall: 0.7408 - auc: 0.8252 - val_loss: 0.6647 - val_tp: 179.0000 - val_fp: 67.0000 - val_tn: 125.0000 - val_fn: 120.0000 - val_accuracy: 0.6191 - val_precision: 0.7276 - val_recall: 0.5987 - val_auc: 0.6619\n",
      "Epoch 109/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.5102 - tp: 1449.0000 - fp: 469.0000 - tn: 1483.0000 - fn: 503.0000 - accuracy: 0.7510 - precision: 0.7555 - recall: 0.7423 - auc: 0.8329 - val_loss: 0.6730 - val_tp: 160.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 139.0000 - val_accuracy: 0.6069 - val_precision: 0.7477 - val_recall: 0.5351 - val_auc: 0.6679\n",
      "Epoch 110/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.5055 - tp: 1486.0000 - fp: 473.0000 - tn: 1479.0000 - fn: 466.0000 - accuracy: 0.7595 - precision: 0.7586 - recall: 0.7613 - auc: 0.8375 - val_loss: 0.6790 - val_tp: 165.0000 - val_fp: 57.0000 - val_tn: 135.0000 - val_fn: 134.0000 - val_accuracy: 0.6110 - val_precision: 0.7432 - val_recall: 0.5518 - val_auc: 0.6662\n",
      "Epoch 111/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.5115 - tp: 1494.0000 - fp: 492.0000 - tn: 1460.0000 - fn: 458.0000 - accuracy: 0.7567 - precision: 0.7523 - recall: 0.7654 - auc: 0.8319 - val_loss: 0.6816 - val_tp: 167.0000 - val_fp: 62.0000 - val_tn: 130.0000 - val_fn: 132.0000 - val_accuracy: 0.6049 - val_precision: 0.7293 - val_recall: 0.5585 - val_auc: 0.6518\n",
      "Epoch 112/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.5063 - tp: 1479.0000 - fp: 486.0000 - tn: 1466.0000 - fn: 473.0000 - accuracy: 0.7544 - precision: 0.7527 - recall: 0.7577 - auc: 0.8351 - val_loss: 0.6751 - val_tp: 164.0000 - val_fp: 59.0000 - val_tn: 133.0000 - val_fn: 135.0000 - val_accuracy: 0.6049 - val_precision: 0.7354 - val_recall: 0.5485 - val_auc: 0.6726\n",
      "Epoch 113/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5017 - tp: 1489.0000 - fp: 478.0000 - tn: 1474.0000 - fn: 463.0000 - accuracy: 0.7590 - precision: 0.7570 - recall: 0.7628 - auc: 0.8394 - val_loss: 0.6695 - val_tp: 165.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 134.0000 - val_accuracy: 0.6151 - val_precision: 0.7500 - val_recall: 0.5518 - val_auc: 0.6730\n",
      "Epoch 114/200\n",
      "3904/3904 [==============================] - 0s 109us/step - loss: 0.5031 - tp: 1487.0000 - fp: 477.0000 - tn: 1475.0000 - fn: 465.0000 - accuracy: 0.7587 - precision: 0.7571 - recall: 0.7618 - auc: 0.8394 - val_loss: 0.6793 - val_tp: 171.0000 - val_fp: 58.0000 - val_tn: 134.0000 - val_fn: 128.0000 - val_accuracy: 0.6212 - val_precision: 0.7467 - val_recall: 0.5719 - val_auc: 0.6686\n",
      "Epoch 115/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.4973 - tp: 1480.0000 - fp: 459.0000 - tn: 1493.0000 - fn: 472.0000 - accuracy: 0.7615 - precision: 0.7633 - recall: 0.7582 - auc: 0.8450 - val_loss: 0.6673 - val_tp: 159.0000 - val_fp: 56.0000 - val_tn: 136.0000 - val_fn: 140.0000 - val_accuracy: 0.6008 - val_precision: 0.7395 - val_recall: 0.5318 - val_auc: 0.6783\n",
      "Epoch 116/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.5011 - tp: 1482.0000 - fp: 453.0000 - tn: 1499.0000 - fn: 470.0000 - accuracy: 0.7636 - precision: 0.7659 - recall: 0.7592 - auc: 0.8386 - val_loss: 0.6988 - val_tp: 144.0000 - val_fp: 53.0000 - val_tn: 139.0000 - val_fn: 155.0000 - val_accuracy: 0.5764 - val_precision: 0.7310 - val_recall: 0.4816 - val_auc: 0.6654\n",
      "Epoch 117/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.4989 - tp: 1473.0000 - fp: 479.0000 - tn: 1473.0000 - fn: 479.0000 - accuracy: 0.7546 - precision: 0.7546 - recall: 0.7546 - auc: 0.8394 - val_loss: 0.6757 - val_tp: 169.0000 - val_fp: 66.0000 - val_tn: 126.0000 - val_fn: 130.0000 - val_accuracy: 0.6008 - val_precision: 0.7191 - val_recall: 0.5652 - val_auc: 0.6638\n",
      "Epoch 118/200\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 0.5025 - tp: 1462.0000 - fp: 489.0000 - tn: 1463.0000 - fn: 490.0000 - accuracy: 0.7492 - precision: 0.7494 - recall: 0.7490 - auc: 0.8337 - val_loss: 0.6773 - val_tp: 167.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 132.0000 - val_accuracy: 0.6029 - val_precision: 0.7261 - val_recall: 0.5585 - val_auc: 0.6667\n",
      "Epoch 119/200\n",
      "3904/3904 [==============================] - 0s 109us/step - loss: 0.4913 - tp: 1513.0000 - fp: 490.0000 - tn: 1462.0000 - fn: 439.0000 - accuracy: 0.7620 - precision: 0.7554 - recall: 0.7751 - auc: 0.8480 - val_loss: 0.6735 - val_tp: 163.0000 - val_fp: 57.0000 - val_tn: 135.0000 - val_fn: 136.0000 - val_accuracy: 0.6069 - val_precision: 0.7409 - val_recall: 0.5452 - val_auc: 0.6846\n",
      "Epoch 120/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.4917 - tp: 1482.0000 - fp: 469.0000 - tn: 1483.0000 - fn: 470.0000 - accuracy: 0.7595 - precision: 0.7596 - recall: 0.7592 - auc: 0.8455 - val_loss: 0.6814 - val_tp: 154.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 145.0000 - val_accuracy: 0.5947 - val_precision: 0.7404 - val_recall: 0.5151 - val_auc: 0.6770\n",
      "Epoch 121/200\n",
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.4906 - tp: 1476.0000 - fp: 467.0000 - tn: 1485.0000 - fn: 476.0000 - accuracy: 0.7585 - precision: 0.7597 - recall: 0.7561 - auc: 0.8452 - val_loss: 0.6876 - val_tp: 166.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 133.0000 - val_accuracy: 0.5906 - val_precision: 0.7094 - val_recall: 0.5552 - val_auc: 0.6641\n",
      "Epoch 122/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.4836 - tp: 1503.0000 - fp: 453.0000 - tn: 1499.0000 - fn: 449.0000 - accuracy: 0.7690 - precision: 0.7684 - recall: 0.7700 - auc: 0.8536 - val_loss: 0.6871 - val_tp: 160.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 139.0000 - val_accuracy: 0.6069 - val_precision: 0.7477 - val_recall: 0.5351 - val_auc: 0.6708\n",
      "Epoch 123/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.4962 - tp: 1496.0000 - fp: 505.0000 - tn: 1447.0000 - fn: 456.0000 - accuracy: 0.7538 - precision: 0.7476 - recall: 0.7664 - auc: 0.8417 - val_loss: 0.6880 - val_tp: 154.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 145.0000 - val_accuracy: 0.5927 - val_precision: 0.7368 - val_recall: 0.5151 - val_auc: 0.6707\n",
      "Epoch 124/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.4829 - tp: 1495.0000 - fp: 441.0000 - tn: 1511.0000 - fn: 457.0000 - accuracy: 0.7700 - precision: 0.7722 - recall: 0.7659 - auc: 0.8546 - val_loss: 0.6812 - val_tp: 162.0000 - val_fp: 64.0000 - val_tn: 128.0000 - val_fn: 137.0000 - val_accuracy: 0.5906 - val_precision: 0.7168 - val_recall: 0.5418 - val_auc: 0.6674\n",
      "Epoch 125/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.4817 - tp: 1507.0000 - fp: 439.0000 - tn: 1513.0000 - fn: 445.0000 - accuracy: 0.7736 - precision: 0.7744 - recall: 0.7720 - auc: 0.8553 - val_loss: 0.6761 - val_tp: 168.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 131.0000 - val_accuracy: 0.6110 - val_precision: 0.7368 - val_recall: 0.5619 - val_auc: 0.6712\n",
      "Epoch 126/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.4771 - tp: 1518.0000 - fp: 418.0000 - tn: 1534.0000 - fn: 434.0000 - accuracy: 0.7818 - precision: 0.7841 - recall: 0.7777 - auc: 0.8589 - val_loss: 0.6798 - val_tp: 169.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 130.0000 - val_accuracy: 0.6130 - val_precision: 0.7380 - val_recall: 0.5652 - val_auc: 0.6585\n",
      "Epoch 127/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.4756 - tp: 1501.0000 - fp: 440.0000 - tn: 1512.0000 - fn: 451.0000 - accuracy: 0.7718 - precision: 0.7733 - recall: 0.7690 - auc: 0.8586 - val_loss: 0.6662 - val_tp: 183.0000 - val_fp: 68.0000 - val_tn: 124.0000 - val_fn: 116.0000 - val_accuracy: 0.6253 - val_precision: 0.7291 - val_recall: 0.6120 - val_auc: 0.6777\n",
      "Epoch 128/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4779 - tp: 1525.0000 - fp: 441.0000 - tn: 1511.0000 - fn: 427.0000 - accuracy: 0.7777 - precision: 0.7757 - recall: 0.7812 - auc: 0.8589 - val_loss: 0.7049 - val_tp: 158.0000 - val_fp: 51.0000 - val_tn: 141.0000 - val_fn: 141.0000 - val_accuracy: 0.6090 - val_precision: 0.7560 - val_recall: 0.5284 - val_auc: 0.6661\n",
      "Epoch 129/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.4816 - tp: 1462.0000 - fp: 445.0000 - tn: 1507.0000 - fn: 490.0000 - accuracy: 0.7605 - precision: 0.7666 - recall: 0.7490 - auc: 0.8506 - val_loss: 0.6911 - val_tp: 171.0000 - val_fp: 64.0000 - val_tn: 128.0000 - val_fn: 128.0000 - val_accuracy: 0.6090 - val_precision: 0.7277 - val_recall: 0.5719 - val_auc: 0.6627\n",
      "Epoch 130/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.4698 - tp: 1504.0000 - fp: 442.0000 - tn: 1510.0000 - fn: 448.0000 - accuracy: 0.7720 - precision: 0.7729 - recall: 0.7705 - auc: 0.8623 - val_loss: 0.6858 - val_tp: 164.0000 - val_fp: 53.0000 - val_tn: 139.0000 - val_fn: 135.0000 - val_accuracy: 0.6171 - val_precision: 0.7558 - val_recall: 0.5485 - val_auc: 0.6728\n",
      "Epoch 131/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.4754 - tp: 1525.0000 - fp: 441.0000 - tn: 1511.0000 - fn: 427.0000 - accuracy: 0.7777 - precision: 0.7757 - recall: 0.7812 - auc: 0.8580 - val_loss: 0.6992 - val_tp: 175.0000 - val_fp: 75.0000 - val_tn: 117.0000 - val_fn: 124.0000 - val_accuracy: 0.5947 - val_precision: 0.7000 - val_recall: 0.5853 - val_auc: 0.6522\n",
      "Epoch 132/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.4747 - tp: 1527.0000 - fp: 470.0000 - tn: 1482.0000 - fn: 425.0000 - accuracy: 0.7707 - precision: 0.7646 - recall: 0.7823 - auc: 0.8574 - val_loss: 0.6845 - val_tp: 174.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 125.0000 - val_accuracy: 0.6049 - val_precision: 0.7160 - val_recall: 0.5819 - val_auc: 0.6702\n",
      "Epoch 133/200\n",
      "3904/3904 [==============================] - 0s 103us/step - loss: 0.4655 - tp: 1530.0000 - fp: 433.0000 - tn: 1519.0000 - fn: 422.0000 - accuracy: 0.7810 - precision: 0.7794 - recall: 0.7838 - auc: 0.8672 - val_loss: 0.6820 - val_tp: 170.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 129.0000 - val_accuracy: 0.6151 - val_precision: 0.7391 - val_recall: 0.5686 - val_auc: 0.6762\n",
      "Epoch 134/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.4641 - tp: 1496.0000 - fp: 419.0000 - tn: 1533.0000 - fn: 456.0000 - accuracy: 0.7759 - precision: 0.7812 - recall: 0.7664 - auc: 0.8664 - val_loss: 0.6810 - val_tp: 158.0000 - val_fp: 56.0000 - val_tn: 136.0000 - val_fn: 141.0000 - val_accuracy: 0.5988 - val_precision: 0.7383 - val_recall: 0.5284 - val_auc: 0.6844\n",
      "Epoch 135/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.4690 - tp: 1502.0000 - fp: 437.0000 - tn: 1515.0000 - fn: 450.0000 - accuracy: 0.7728 - precision: 0.7746 - recall: 0.7695 - auc: 0.8616 - val_loss: 0.6887 - val_tp: 178.0000 - val_fp: 72.0000 - val_tn: 120.0000 - val_fn: 121.0000 - val_accuracy: 0.6069 - val_precision: 0.7120 - val_recall: 0.5953 - val_auc: 0.6656\n",
      "Epoch 136/200\n",
      "3904/3904 [==============================] - 0s 102us/step - loss: 0.4744 - tp: 1519.0000 - fp: 457.0000 - tn: 1495.0000 - fn: 433.0000 - accuracy: 0.7720 - precision: 0.7687 - recall: 0.7782 - auc: 0.8564 - val_loss: 0.6783 - val_tp: 171.0000 - val_fp: 59.0000 - val_tn: 133.0000 - val_fn: 128.0000 - val_accuracy: 0.6191 - val_precision: 0.7435 - val_recall: 0.5719 - val_auc: 0.6793\n",
      "Epoch 137/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.4782 - tp: 1492.0000 - fp: 441.0000 - tn: 1511.0000 - fn: 460.0000 - accuracy: 0.7692 - precision: 0.7719 - recall: 0.7643 - auc: 0.8547 - val_loss: 0.6682 - val_tp: 173.0000 - val_fp: 58.0000 - val_tn: 134.0000 - val_fn: 126.0000 - val_accuracy: 0.6253 - val_precision: 0.7489 - val_recall: 0.5786 - val_auc: 0.6806\n",
      "Epoch 138/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.4594 - tp: 1527.0000 - fp: 422.0000 - tn: 1530.0000 - fn: 425.0000 - accuracy: 0.7830 - precision: 0.7835 - recall: 0.7823 - auc: 0.8702 - val_loss: 0.6881 - val_tp: 176.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 123.0000 - val_accuracy: 0.6273 - val_precision: 0.7458 - val_recall: 0.5886 - val_auc: 0.6700\n",
      "Epoch 139/200\n",
      "3904/3904 [==============================] - 0s 105us/step - loss: 0.4516 - tp: 1523.0000 - fp: 402.0000 - tn: 1550.0000 - fn: 429.0000 - accuracy: 0.7871 - precision: 0.7912 - recall: 0.7802 - auc: 0.8755 - val_loss: 0.7133 - val_tp: 175.0000 - val_fp: 71.0000 - val_tn: 121.0000 - val_fn: 124.0000 - val_accuracy: 0.6029 - val_precision: 0.7114 - val_recall: 0.5853 - val_auc: 0.6507\n",
      "Epoch 140/200\n",
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.4568 - tp: 1531.0000 - fp: 424.0000 - tn: 1528.0000 - fn: 421.0000 - accuracy: 0.7836 - precision: 0.7831 - recall: 0.7843 - auc: 0.8700 - val_loss: 0.6915 - val_tp: 172.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 127.0000 - val_accuracy: 0.6130 - val_precision: 0.7319 - val_recall: 0.5753 - val_auc: 0.6739\n",
      "Epoch 141/200\n",
      "3904/3904 [==============================] - 0s 106us/step - loss: 0.4606 - tp: 1533.0000 - fp: 433.0000 - tn: 1519.0000 - fn: 419.0000 - accuracy: 0.7818 - precision: 0.7798 - recall: 0.7853 - auc: 0.8670 - val_loss: 0.6837 - val_tp: 179.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 120.0000 - val_accuracy: 0.6273 - val_precision: 0.7397 - val_recall: 0.5987 - val_auc: 0.6782\n",
      "Epoch 142/200\n",
      "3904/3904 [==============================] - 0s 104us/step - loss: 0.4381 - tp: 1548.0000 - fp: 366.0000 - tn: 1586.0000 - fn: 404.0000 - accuracy: 0.8028 - precision: 0.8088 - recall: 0.7930 - auc: 0.8861 - val_loss: 0.6809 - val_tp: 178.0000 - val_fp: 56.0000 - val_tn: 136.0000 - val_fn: 121.0000 - val_accuracy: 0.6395 - val_precision: 0.7607 - val_recall: 0.5953 - val_auc: 0.6832\n",
      "Epoch 143/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4644 - tp: 1530.0000 - fp: 480.0000 - tn: 1472.0000 - fn: 422.0000 - accuracy: 0.7690 - precision: 0.7612 - recall: 0.7838 - auc: 0.8629 - val_loss: 0.7069 - val_tp: 152.0000 - val_fp: 52.0000 - val_tn: 140.0000 - val_fn: 147.0000 - val_accuracy: 0.5947 - val_precision: 0.7451 - val_recall: 0.5084 - val_auc: 0.6715\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4604 - tp: 1520.0000 - fp: 430.0000 - tn: 1522.0000 - fn: 432.0000 - accuracy: 0.7792 - precision: 0.7795 - recall: 0.7787 - auc: 0.8670 - val_loss: 0.6678 - val_tp: 180.0000 - val_fp: 61.0000 - val_tn: 131.0000 - val_fn: 119.0000 - val_accuracy: 0.6334 - val_precision: 0.7469 - val_recall: 0.6020 - val_auc: 0.6857\n",
      "Epoch 145/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.4506 - tp: 1555.0000 - fp: 419.0000 - tn: 1533.0000 - fn: 397.0000 - accuracy: 0.7910 - precision: 0.7877 - recall: 0.7966 - auc: 0.8753 - val_loss: 0.6828 - val_tp: 167.0000 - val_fp: 59.0000 - val_tn: 133.0000 - val_fn: 132.0000 - val_accuracy: 0.6110 - val_precision: 0.7389 - val_recall: 0.5585 - val_auc: 0.6760\n",
      "Epoch 146/200\n",
      "3904/3904 [==============================] - 0s 98us/step - loss: 0.4501 - tp: 1542.0000 - fp: 426.0000 - tn: 1526.0000 - fn: 410.0000 - accuracy: 0.7859 - precision: 0.7835 - recall: 0.7900 - auc: 0.8747 - val_loss: 0.6787 - val_tp: 178.0000 - val_fp: 57.0000 - val_tn: 135.0000 - val_fn: 121.0000 - val_accuracy: 0.6375 - val_precision: 0.7574 - val_recall: 0.5953 - val_auc: 0.6971\n",
      "Epoch 147/200\n",
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.4497 - tp: 1536.0000 - fp: 430.0000 - tn: 1522.0000 - fn: 416.0000 - accuracy: 0.7833 - precision: 0.7813 - recall: 0.7869 - auc: 0.8741 - val_loss: 0.6961 - val_tp: 157.0000 - val_fp: 47.0000 - val_tn: 145.0000 - val_fn: 142.0000 - val_accuracy: 0.6151 - val_precision: 0.7696 - val_recall: 0.5251 - val_auc: 0.6862\n",
      "Epoch 148/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.4415 - tp: 1538.0000 - fp: 393.0000 - tn: 1559.0000 - fn: 414.0000 - accuracy: 0.7933 - precision: 0.7965 - recall: 0.7879 - auc: 0.8799 - val_loss: 0.6889 - val_tp: 157.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 142.0000 - val_accuracy: 0.5988 - val_precision: 0.7406 - val_recall: 0.5251 - val_auc: 0.6735\n",
      "Epoch 149/200\n",
      "3904/3904 [==============================] - 0s 102us/step - loss: 0.4367 - tp: 1534.0000 - fp: 379.0000 - tn: 1573.0000 - fn: 418.0000 - accuracy: 0.7959 - precision: 0.8019 - recall: 0.7859 - auc: 0.8839 - val_loss: 0.6629 - val_tp: 180.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 119.0000 - val_accuracy: 0.6354 - val_precision: 0.7500 - val_recall: 0.6020 - val_auc: 0.6954\n",
      "Epoch 150/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4546 - tp: 1539.0000 - fp: 421.0000 - tn: 1531.0000 - fn: 413.0000 - accuracy: 0.7864 - precision: 0.7852 - recall: 0.7884 - auc: 0.8709 - val_loss: 0.6956 - val_tp: 178.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 121.0000 - val_accuracy: 0.6415 - val_precision: 0.7639 - val_recall: 0.5953 - val_auc: 0.6856\n",
      "Epoch 151/200\n",
      "3904/3904 [==============================] - 0s 98us/step - loss: 0.4476 - tp: 1546.0000 - fp: 410.0000 - tn: 1542.0000 - fn: 406.0000 - accuracy: 0.7910 - precision: 0.7904 - recall: 0.7920 - auc: 0.8755 - val_loss: 0.7059 - val_tp: 166.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 133.0000 - val_accuracy: 0.6191 - val_precision: 0.7545 - val_recall: 0.5552 - val_auc: 0.6774\n",
      "Epoch 152/200\n",
      "3904/3904 [==============================] - 0s 96us/step - loss: 0.4374 - tp: 1550.0000 - fp: 393.0000 - tn: 1559.0000 - fn: 402.0000 - accuracy: 0.7964 - precision: 0.7977 - recall: 0.7941 - auc: 0.8838 - val_loss: 0.6870 - val_tp: 173.0000 - val_fp: 58.0000 - val_tn: 134.0000 - val_fn: 126.0000 - val_accuracy: 0.6253 - val_precision: 0.7489 - val_recall: 0.5786 - val_auc: 0.6910\n",
      "Epoch 153/200\n",
      "3904/3904 [==============================] - 0s 97us/step - loss: 0.4417 - tp: 1563.0000 - fp: 417.0000 - tn: 1535.0000 - fn: 389.0000 - accuracy: 0.7935 - precision: 0.7894 - recall: 0.8007 - auc: 0.8804 - val_loss: 0.6691 - val_tp: 165.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 134.0000 - val_accuracy: 0.6171 - val_precision: 0.7534 - val_recall: 0.5518 - val_auc: 0.6981\n",
      "Epoch 154/200\n",
      "3904/3904 [==============================] - 0s 98us/step - loss: 0.4457 - tp: 1525.0000 - fp: 381.0000 - tn: 1571.0000 - fn: 427.0000 - accuracy: 0.7930 - precision: 0.8001 - recall: 0.7812 - auc: 0.8783 - val_loss: 0.6886 - val_tp: 163.0000 - val_fp: 50.0000 - val_tn: 142.0000 - val_fn: 136.0000 - val_accuracy: 0.6212 - val_precision: 0.7653 - val_recall: 0.5452 - val_auc: 0.6994\n",
      "Epoch 155/200\n",
      "3904/3904 [==============================] - 0s 97us/step - loss: 0.4410 - tp: 1556.0000 - fp: 422.0000 - tn: 1530.0000 - fn: 396.0000 - accuracy: 0.7905 - precision: 0.7867 - recall: 0.7971 - auc: 0.8803 - val_loss: 0.6906 - val_tp: 161.0000 - val_fp: 49.0000 - val_tn: 143.0000 - val_fn: 138.0000 - val_accuracy: 0.6191 - val_precision: 0.7667 - val_recall: 0.5385 - val_auc: 0.6914\n",
      "Epoch 156/200\n",
      "3904/3904 [==============================] - 0s 96us/step - loss: 0.4392 - tp: 1548.0000 - fp: 399.0000 - tn: 1553.0000 - fn: 404.0000 - accuracy: 0.7943 - precision: 0.7951 - recall: 0.7930 - auc: 0.8816 - val_loss: 0.6914 - val_tp: 164.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 135.0000 - val_accuracy: 0.6151 - val_precision: 0.7523 - val_recall: 0.5485 - val_auc: 0.6890\n",
      "Epoch 157/200\n",
      "3904/3904 [==============================] - 0s 97us/step - loss: 0.4317 - tp: 1573.0000 - fp: 385.0000 - tn: 1567.0000 - fn: 379.0000 - accuracy: 0.8043 - precision: 0.8034 - recall: 0.8058 - auc: 0.8871 - val_loss: 0.6884 - val_tp: 173.0000 - val_fp: 56.0000 - val_tn: 136.0000 - val_fn: 126.0000 - val_accuracy: 0.6293 - val_precision: 0.7555 - val_recall: 0.5786 - val_auc: 0.6865\n",
      "Epoch 158/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.4299 - tp: 1570.0000 - fp: 364.0000 - tn: 1588.0000 - fn: 382.0000 - accuracy: 0.8089 - precision: 0.8118 - recall: 0.8043 - auc: 0.8880 - val_loss: 0.6725 - val_tp: 178.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 121.0000 - val_accuracy: 0.6436 - val_precision: 0.7672 - val_recall: 0.5953 - val_auc: 0.7049\n",
      "Epoch 159/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4288 - tp: 1582.0000 - fp: 398.0000 - tn: 1554.0000 - fn: 370.0000 - accuracy: 0.8033 - precision: 0.7990 - recall: 0.8105 - auc: 0.8902 - val_loss: 0.6783 - val_tp: 167.0000 - val_fp: 49.0000 - val_tn: 143.0000 - val_fn: 132.0000 - val_accuracy: 0.6314 - val_precision: 0.7731 - val_recall: 0.5585 - val_auc: 0.7053\n",
      "Epoch 160/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4372 - tp: 1528.0000 - fp: 362.0000 - tn: 1590.0000 - fn: 424.0000 - accuracy: 0.7987 - precision: 0.8085 - recall: 0.7828 - auc: 0.8829 - val_loss: 0.6724 - val_tp: 175.0000 - val_fp: 52.0000 - val_tn: 140.0000 - val_fn: 124.0000 - val_accuracy: 0.6415 - val_precision: 0.7709 - val_recall: 0.5853 - val_auc: 0.6987\n",
      "Epoch 161/200\n",
      "3904/3904 [==============================] - 0s 98us/step - loss: 0.4354 - tp: 1578.0000 - fp: 436.0000 - tn: 1516.0000 - fn: 374.0000 - accuracy: 0.7925 - precision: 0.7835 - recall: 0.8084 - auc: 0.8814 - val_loss: 0.6866 - val_tp: 170.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 129.0000 - val_accuracy: 0.6273 - val_precision: 0.7589 - val_recall: 0.5686 - val_auc: 0.6910\n",
      "Epoch 162/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.4388 - tp: 1554.0000 - fp: 388.0000 - tn: 1564.0000 - fn: 398.0000 - accuracy: 0.7987 - precision: 0.8002 - recall: 0.7961 - auc: 0.8806 - val_loss: 0.6575 - val_tp: 182.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 117.0000 - val_accuracy: 0.6334 - val_precision: 0.7429 - val_recall: 0.6087 - val_auc: 0.7021\n",
      "Epoch 163/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4291 - tp: 1580.0000 - fp: 388.0000 - tn: 1564.0000 - fn: 372.0000 - accuracy: 0.8053 - precision: 0.8028 - recall: 0.8094 - auc: 0.8863 - val_loss: 0.6947 - val_tp: 181.0000 - val_fp: 61.0000 - val_tn: 131.0000 - val_fn: 118.0000 - val_accuracy: 0.6354 - val_precision: 0.7479 - val_recall: 0.6054 - val_auc: 0.6872\n",
      "Epoch 164/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.4276 - tp: 1546.0000 - fp: 373.0000 - tn: 1579.0000 - fn: 406.0000 - accuracy: 0.8005 - precision: 0.8056 - recall: 0.7920 - auc: 0.8882 - val_loss: 0.6919 - val_tp: 176.0000 - val_fp: 62.0000 - val_tn: 130.0000 - val_fn: 123.0000 - val_accuracy: 0.6232 - val_precision: 0.7395 - val_recall: 0.5886 - val_auc: 0.6845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.4116 - tp: 1607.0000 - fp: 363.0000 - tn: 1589.0000 - fn: 345.0000 - accuracy: 0.8186 - precision: 0.8157 - recall: 0.8233 - auc: 0.8999 - val_loss: 0.6875 - val_tp: 179.0000 - val_fp: 62.0000 - val_tn: 130.0000 - val_fn: 120.0000 - val_accuracy: 0.6293 - val_precision: 0.7427 - val_recall: 0.5987 - val_auc: 0.6867\n",
      "Epoch 166/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4269 - tp: 1565.0000 - fp: 397.0000 - tn: 1555.0000 - fn: 387.0000 - accuracy: 0.7992 - precision: 0.7977 - recall: 0.8017 - auc: 0.8891 - val_loss: 0.6690 - val_tp: 187.0000 - val_fp: 56.0000 - val_tn: 136.0000 - val_fn: 112.0000 - val_accuracy: 0.6578 - val_precision: 0.7695 - val_recall: 0.6254 - val_auc: 0.7013\n",
      "Epoch 167/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4114 - tp: 1599.0000 - fp: 366.0000 - tn: 1586.0000 - fn: 353.0000 - accuracy: 0.8158 - precision: 0.8137 - recall: 0.8192 - auc: 0.9003 - val_loss: 0.7055 - val_tp: 163.0000 - val_fp: 49.0000 - val_tn: 143.0000 - val_fn: 136.0000 - val_accuracy: 0.6232 - val_precision: 0.7689 - val_recall: 0.5452 - val_auc: 0.6933\n",
      "Epoch 168/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4116 - tp: 1558.0000 - fp: 364.0000 - tn: 1588.0000 - fn: 394.0000 - accuracy: 0.8058 - precision: 0.8106 - recall: 0.7982 - auc: 0.8980 - val_loss: 0.6777 - val_tp: 180.0000 - val_fp: 63.0000 - val_tn: 129.0000 - val_fn: 119.0000 - val_accuracy: 0.6293 - val_precision: 0.7407 - val_recall: 0.6020 - val_auc: 0.6964\n",
      "Epoch 169/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4126 - tp: 1567.0000 - fp: 362.0000 - tn: 1590.0000 - fn: 385.0000 - accuracy: 0.8087 - precision: 0.8123 - recall: 0.8028 - auc: 0.8983 - val_loss: 0.6991 - val_tp: 167.0000 - val_fp: 51.0000 - val_tn: 141.0000 - val_fn: 132.0000 - val_accuracy: 0.6273 - val_precision: 0.7661 - val_recall: 0.5585 - val_auc: 0.6906\n",
      "Epoch 170/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4191 - tp: 1598.0000 - fp: 407.0000 - tn: 1545.0000 - fn: 354.0000 - accuracy: 0.8051 - precision: 0.7970 - recall: 0.8186 - auc: 0.8924 - val_loss: 0.6850 - val_tp: 170.0000 - val_fp: 52.0000 - val_tn: 140.0000 - val_fn: 129.0000 - val_accuracy: 0.6314 - val_precision: 0.7658 - val_recall: 0.5686 - val_auc: 0.6967\n",
      "Epoch 171/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4196 - tp: 1589.0000 - fp: 383.0000 - tn: 1569.0000 - fn: 363.0000 - accuracy: 0.8089 - precision: 0.8058 - recall: 0.8140 - auc: 0.8925 - val_loss: 0.6836 - val_tp: 180.0000 - val_fp: 57.0000 - val_tn: 135.0000 - val_fn: 119.0000 - val_accuracy: 0.6415 - val_precision: 0.7595 - val_recall: 0.6020 - val_auc: 0.6935\n",
      "Epoch 172/200\n",
      "3904/3904 [==============================] - 0s 99us/step - loss: 0.4088 - tp: 1589.0000 - fp: 367.0000 - tn: 1585.0000 - fn: 363.0000 - accuracy: 0.8130 - precision: 0.8124 - recall: 0.8140 - auc: 0.8999 - val_loss: 0.6936 - val_tp: 175.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 124.0000 - val_accuracy: 0.6354 - val_precision: 0.7609 - val_recall: 0.5853 - val_auc: 0.6946\n",
      "Epoch 173/200\n",
      "3904/3904 [==============================] - 0s 100us/step - loss: 0.4220 - tp: 1566.0000 - fp: 363.0000 - tn: 1589.0000 - fn: 386.0000 - accuracy: 0.8081 - precision: 0.8118 - recall: 0.8023 - auc: 0.8916 - val_loss: 0.6675 - val_tp: 187.0000 - val_fp: 54.0000 - val_tn: 138.0000 - val_fn: 112.0000 - val_accuracy: 0.6619 - val_precision: 0.7759 - val_recall: 0.6254 - val_auc: 0.7096\n",
      "Epoch 174/200\n",
      "3904/3904 [==============================] - 0s 101us/step - loss: 0.4210 - tp: 1570.0000 - fp: 397.0000 - tn: 1555.0000 - fn: 382.0000 - accuracy: 0.8005 - precision: 0.7982 - recall: 0.8043 - auc: 0.8908 - val_loss: 0.7213 - val_tp: 159.0000 - val_fp: 49.0000 - val_tn: 143.0000 - val_fn: 140.0000 - val_accuracy: 0.6151 - val_precision: 0.7644 - val_recall: 0.5318 - val_auc: 0.6876\n",
      "Epoch 175/200\n",
      "3904/3904 [==============================] - 0s 107us/step - loss: 0.4087 - tp: 1584.0000 - fp: 354.0000 - tn: 1598.0000 - fn: 368.0000 - accuracy: 0.8151 - precision: 0.8173 - recall: 0.8115 - auc: 0.9001 - val_loss: 0.6784 - val_tp: 190.0000 - val_fp: 69.0000 - val_tn: 123.0000 - val_fn: 109.0000 - val_accuracy: 0.6375 - val_precision: 0.7336 - val_recall: 0.6355 - val_auc: 0.6917\n",
      "Epoch 176/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.4088 - tp: 1598.0000 - fp: 354.0000 - tn: 1598.0000 - fn: 354.0000 - accuracy: 0.8186 - precision: 0.8186 - recall: 0.8186 - auc: 0.8989 - val_loss: 0.7047 - val_tp: 168.0000 - val_fp: 59.0000 - val_tn: 133.0000 - val_fn: 131.0000 - val_accuracy: 0.6130 - val_precision: 0.7401 - val_recall: 0.5619 - val_auc: 0.6846\n",
      "Epoch 177/200\n",
      "3904/3904 [==============================] - 0s 108us/step - loss: 0.4150 - tp: 1580.0000 - fp: 386.0000 - tn: 1566.0000 - fn: 372.0000 - accuracy: 0.8058 - precision: 0.8037 - recall: 0.8094 - auc: 0.8946 - val_loss: 0.6997 - val_tp: 174.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 125.0000 - val_accuracy: 0.6232 - val_precision: 0.7436 - val_recall: 0.5819 - val_auc: 0.6925\n",
      "Epoch 178/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.4091 - tp: 1605.0000 - fp: 364.0000 - tn: 1588.0000 - fn: 347.0000 - accuracy: 0.8179 - precision: 0.8151 - recall: 0.8222 - auc: 0.9001 - val_loss: 0.6991 - val_tp: 160.0000 - val_fp: 48.0000 - val_tn: 144.0000 - val_fn: 139.0000 - val_accuracy: 0.6191 - val_precision: 0.7692 - val_recall: 0.5351 - val_auc: 0.6952\n",
      "Epoch 179/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.4081 - tp: 1590.0000 - fp: 401.0000 - tn: 1551.0000 - fn: 362.0000 - accuracy: 0.8046 - precision: 0.7986 - recall: 0.8145 - auc: 0.8973 - val_loss: 0.6813 - val_tp: 179.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 120.0000 - val_accuracy: 0.6334 - val_precision: 0.7490 - val_recall: 0.5987 - val_auc: 0.7010\n",
      "Epoch 180/200\n",
      "3904/3904 [==============================] - 0s 110us/step - loss: 0.4075 - tp: 1583.0000 - fp: 350.0000 - tn: 1602.0000 - fn: 369.0000 - accuracy: 0.8158 - precision: 0.8189 - recall: 0.8110 - auc: 0.8995 - val_loss: 0.6818 - val_tp: 177.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 122.0000 - val_accuracy: 0.6395 - val_precision: 0.7629 - val_recall: 0.5920 - val_auc: 0.7072\n",
      "Epoch 181/200\n",
      "3904/3904 [==============================] - 0s 113us/step - loss: 0.4039 - tp: 1610.0000 - fp: 338.0000 - tn: 1614.0000 - fn: 342.0000 - accuracy: 0.8258 - precision: 0.8265 - recall: 0.8248 - auc: 0.9037 - val_loss: 0.6905 - val_tp: 181.0000 - val_fp: 62.0000 - val_tn: 130.0000 - val_fn: 118.0000 - val_accuracy: 0.6334 - val_precision: 0.7449 - val_recall: 0.6054 - val_auc: 0.6992\n",
      "Epoch 182/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.4093 - tp: 1579.0000 - fp: 366.0000 - tn: 1586.0000 - fn: 373.0000 - accuracy: 0.8107 - precision: 0.8118 - recall: 0.8089 - auc: 0.8980 - val_loss: 0.6872 - val_tp: 191.0000 - val_fp: 60.0000 - val_tn: 132.0000 - val_fn: 108.0000 - val_accuracy: 0.6578 - val_precision: 0.7610 - val_recall: 0.6388 - val_auc: 0.6977\n",
      "Epoch 183/200\n",
      "3904/3904 [==============================] - 0s 111us/step - loss: 0.4004 - tp: 1592.0000 - fp: 337.0000 - tn: 1615.0000 - fn: 360.0000 - accuracy: 0.8215 - precision: 0.8253 - recall: 0.8156 - auc: 0.9045 - val_loss: 0.6800 - val_tp: 178.0000 - val_fp: 55.0000 - val_tn: 137.0000 - val_fn: 121.0000 - val_accuracy: 0.6415 - val_precision: 0.7639 - val_recall: 0.5953 - val_auc: 0.7044\n",
      "Epoch 184/200\n",
      "2304/3904 [================>.............] - ETA: 0s - loss: 0.4091 - tp: 948.0000 - fp: 233.0000 - tn: 919.0000 - fn: 204.0000 - accuracy: 0.8103 - precision: 0.8027 - recall: 0.8229 - auc: 0.8972"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model-binary.hdf5\", save_best_only=True)\n",
    "\n",
    "history = model.fit(x=X_train_balanced, \n",
    "                    y=Y_train_balanced, \n",
    "                    batch_size=128, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    shuffle=True,\n",
    "                    callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "plotted_metrics = ['precision', 'recall', 'accuracy']\n",
    "\n",
    "fig = plt.figure(figsize=(18, 4 * len(plotted_metrics)))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "\n",
    "for idx, metric in enumerate(plotted_metrics):\n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+1)\n",
    "    plt.title(metric)\n",
    "    plt.plot(history_dict[metric])\n",
    "    \n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+2)\n",
    "    plt.title('val_{}'.format(metric))\n",
    "    plt.plot(history_dict['val_{}'.format(metric)])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
