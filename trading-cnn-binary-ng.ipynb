{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sheet I will apply Convolutional Neural Networks for stock prediction of S&P500 index. I will divide data from last 20 years into chunks. Every day contains four values: opening, low, high and closing ones - therefore input data for 30-day chunk has size 30x4. Predicted output has two classes: whether stock will go up or down or stay constant X days after last input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TIME = 120\n",
    "PREDICTION_AFTER_DAYS = 5\n",
    "EPOCHS = 20000\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_SPLIT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>1346.089966</td>\n",
       "      <td>1358.109985</td>\n",
       "      <td>1331.880005</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>980000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1370.109985</td>\n",
       "      <td>1342.439941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>993700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1364.800049</td>\n",
       "      <td>1329.880005</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1215000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1362.140015</td>\n",
       "      <td>1329.150024</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1065200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1360.819946</td>\n",
       "      <td>1325.069946</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1026500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2000-02-22  1346.089966  1358.109985  1331.880005  1352.170044   \n",
       "1  2000-02-23  1352.170044  1370.109985  1342.439941  1360.689941   \n",
       "2  2000-02-24  1360.689941  1364.800049  1329.880005  1353.430054   \n",
       "3  2000-02-25  1353.430054  1362.140015  1329.150024  1333.359985   \n",
       "4  2000-02-28  1333.359985  1360.819946  1325.069946  1348.050049   \n",
       "\n",
       "     Adj Close      Volume  \n",
       "0  1352.170044   980000000  \n",
       "1  1360.689941   993700000  \n",
       "2  1353.430054  1215000000  \n",
       "3  1333.359985  1065200000  \n",
       "4  1348.050049  1026500000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/sp500_20Y_without_covid.csv')\n",
    "df.reindex(index=df.index[::-1])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_to_label(percentage_value):\n",
    "    return 0 if percentage_value < 0 else 1\n",
    "\n",
    "def investor_observes_stocks_for(x_days=60, then_buy_stocks=True, and_sells_them=True, after_y_days=10, dataframe=df):\n",
    "    assert then_buy_stocks\n",
    "    assert and_sells_them\n",
    "    observe_buy_sell_process_length = x_days + after_y_days\n",
    "    \n",
    "    observed_chunks = []\n",
    "    observation_results = []\n",
    "    \n",
    "    for first_day_of_observation in range(len(dataframe) - observe_buy_sell_process_length):\n",
    "        buyout_day = first_day_of_observation + x_days\n",
    "        sell_day = buyout_day + after_y_days\n",
    "        \n",
    "        observed_chunk = dataframe[first_day_of_observation:buyout_day].reset_index()\n",
    "        observation_result = dataframe.iloc[sell_day]\n",
    "        \n",
    "        closing_price_on_buyout_day = dataframe.iloc[buyout_day]['Close']\n",
    "        opening_price_on_sell_day = dataframe.iloc[sell_day]['Open']\n",
    "        \n",
    "        relative_price_change_as_percentage = (opening_price_on_sell_day - closing_price_on_buyout_day) / closing_price_on_buyout_day * 100\n",
    "        \n",
    "        observed_chunks += [observed_chunk]\n",
    "        observation_results += [percentage_to_label(relative_price_change_as_percentage)]\n",
    "    \n",
    "    return observed_chunks, observation_results\n",
    "\n",
    "observed_chunks, observation_results = investor_observes_stocks_for(x_days=OBSERVATION_TIME, then_buy_stocks=True, and_sells_them=True, after_y_days=PREDICTION_AFTER_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    2761\n",
       "0    2144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(observation_results)\n",
    "plt.show()\n",
    "\n",
    "pd.Series(observation_results).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** Selecting test/valiations chunks randomly from the whole set of observation chunks seems to be a big mistake. It causes situation where network actually seen chunks from the future, therefore the more overfitted network, the better results will be. Network should be trained incrementally - with data from time x to x+1 and validated with data from x+1 to x+2. Then learned with data from x to x+2 and validated with data from x+3 to x+3 and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 120, 16)           336       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 120, 16)           64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 120, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 120, 8)            520       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 120, 8)            32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 120, 8)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 120, 8)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                61504     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 62,777\n",
      "Trainable params: 62,601\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (OBSERVATION_TIME, 5), filters=16, kernel_size=4, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution1D(filters=8, kernel_size=4, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4414, 120, 5), (4414,), (491, 120, 5), (491,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(list(map(lambda df: df[['Open', 'High', 'Low', 'Close', 'Adj Close']].to_numpy(), observed_chunks)))\n",
    "Y = np.array(observation_results).astype('float32')\n",
    "\n",
    "def chronological_split(X_data, Y_data, test_size=0.25):\n",
    "    training_test_split_index = int((1 - test_size) * len(X_data))\n",
    "    X_train = X_data[:training_test_split_index]\n",
    "    Y_train = Y_data[:training_test_split_index]\n",
    "    X_test = X_data[training_test_split_index:]\n",
    "    Y_test = Y_data[training_test_split_index:]\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "def random_split(X_data, Y_data, test_size=0.25):\n",
    "    return train_test_split(X_data, Y_data, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = random_split(X, Y, TEST_SIZE) if RANDOM_SPLIT else chronological_split(X, Y, TEST_SIZE)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def undersample_and_shuffle(X, Y):\n",
    "    count_class_1, count_class_0 = pd.Series(Y).value_counts()\n",
    "\n",
    "    lower_count = count_class_0 if count_class_0 < count_class_1 else count_class_1\n",
    "    lower_class = 0 if count_class_0 < count_class_1 else 1\n",
    "    \n",
    "    diff = abs(count_class_0 - count_class_1)\n",
    "    \n",
    "    vec0 = random.sample([(x, y) for x, y in zip(X, Y) if y == 0], lower_count)\n",
    "    vec1 = random.sample([(x, y) for x, y in zip(X, Y) if y == 1], lower_count)\n",
    "\n",
    "    \n",
    "    vec = vec0 + vec1\n",
    "    random.shuffle(vec)\n",
    "    return np.array(list(map(lambda x: x[0], vec))), np.array(list(map(lambda x: x[1], vec)))\n",
    "    \n",
    "X_train_balanced, Y_train_balanced = undersample_and_shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    313\n",
       "0.0    178\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(Y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_1_input to have shape (60, 5) but got array with shape (120, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c0c555585a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     callbacks = [checkpointer])\n\u001b[0m",
      "\u001b[0;32m~/university/trading-xai/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/university/trading-xai/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/university/trading-xai/venv/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_1_input to have shape (60, 5) but got array with shape (120, 5)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model-binary.hdf5\", save_best_only=True)\n",
    "\n",
    "history = model.fit(x=X_train_balanced, \n",
    "                    y=Y_train_balanced, \n",
    "                    batch_size=128, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    shuffle=True,\n",
    "                    callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "plotted_metrics = ['precision', 'recall', 'accuracy']\n",
    "\n",
    "fig = plt.figure(figsize=(18, 4 * len(plotted_metrics)))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "\n",
    "for idx, metric in enumerate(plotted_metrics):\n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+1)\n",
    "    plt.title(metric)\n",
    "    plt.plot(history_dict[metric])\n",
    "    \n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+2)\n",
    "    plt.title('val_{}'.format(metric))\n",
    "    plt.plot(history_dict['val_{}'.format(metric)])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
