{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sheet I will apply Convolutional Neural Networks for stock prediction of S&P500 index. I will divide data from last 20 years into chunks. Every day contains four values: opening, low, high and closing ones - therefore input data for 30-day chunk has size 30x4. Predicted output has three classes: whether stock will go up, down or stay constant X days after last input.\n",
    "\n",
    "Story: Investor observes stock for 60 days. Then decides whether buy some stocks and sell it after 10 days. Therefore input consist of stock prices from day 1st to 60th, output is:\n",
    "* 0 if price will go down by more than \\$\\{PERCENTAGE_THRESHOLD\\}%, \n",
    "* 1 if price will not go up or down by more than \\$\\{PERCENTAGE_THRESHOLD\\}% \n",
    "* 2 if price will go up by more than \\$\\{PERCENTAGE_THRESHOLD\\}%\n",
    "\n",
    "Relative change output is calculated using the following formula: (opening_price_on_buyout_day -closing_price_on_last_day_of_observation) / opening_price_on_buyout_day * 100%\n",
    "\n",
    "Based on percentage value from formula above, label will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_TIME = 60\n",
    "PREDICTION_AFTER_DAYS = 10\n",
    "PERCENTAGE_THRESHOLD = 1\n",
    "EPOCHS = 500\n",
    "TEST_SIZE = 0.10\n",
    "RANDOM_SPLIT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>1346.089966</td>\n",
       "      <td>1358.109985</td>\n",
       "      <td>1331.880005</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>980000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>1352.170044</td>\n",
       "      <td>1370.109985</td>\n",
       "      <td>1342.439941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>993700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>1360.689941</td>\n",
       "      <td>1364.800049</td>\n",
       "      <td>1329.880005</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1215000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>1353.430054</td>\n",
       "      <td>1362.140015</td>\n",
       "      <td>1329.150024</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1065200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>1333.359985</td>\n",
       "      <td>1360.819946</td>\n",
       "      <td>1325.069946</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1348.050049</td>\n",
       "      <td>1026500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2000-02-22  1346.089966  1358.109985  1331.880005  1352.170044   \n",
       "1  2000-02-23  1352.170044  1370.109985  1342.439941  1360.689941   \n",
       "2  2000-02-24  1360.689941  1364.800049  1329.880005  1353.430054   \n",
       "3  2000-02-25  1353.430054  1362.140015  1329.150024  1333.359985   \n",
       "4  2000-02-28  1333.359985  1360.819946  1325.069946  1348.050049   \n",
       "\n",
       "     Adj Close      Volume  \n",
       "0  1352.170044   980000000  \n",
       "1  1360.689941   993700000  \n",
       "2  1353.430054  1215000000  \n",
       "3  1333.359985  1065200000  \n",
       "4  1348.050049  1026500000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/sp500_20Y_without_covid.csv')\n",
    "df.reindex(index=df.index[::-1])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_to_label(percentage_value):\n",
    "    return 0 if percentage_value < -PERCENTAGE_THRESHOLD else 1 if percentage_value <= PERCENTAGE_THRESHOLD else 2\n",
    "\n",
    "def investor_observes_stocks_for(x_days=60, then_buy_stocks=True, and_sells_them=True, after_y_days=10, dataframe=df):\n",
    "    assert then_buy_stocks\n",
    "    assert and_sells_them\n",
    "    observe_buy_sell_process_length = x_days + after_y_days\n",
    "    \n",
    "    observed_chunks = []\n",
    "    observation_results = []\n",
    "    \n",
    "    for first_day_of_observation in range(len(dataframe) - observe_buy_sell_process_length):\n",
    "        buyout_day = first_day_of_observation + x_days\n",
    "        sell_day = buyout_day + after_y_days\n",
    "        \n",
    "        observed_chunk = dataframe[first_day_of_observation:buyout_day].reset_index()\n",
    "        observation_result = dataframe.iloc[sell_day]\n",
    "        \n",
    "        closing_price_on_buyout_day = dataframe.iloc[buyout_day]['Close']\n",
    "        opening_price_on_sell_day = dataframe.iloc[sell_day]['Open']\n",
    "        \n",
    "        relative_price_change_as_percentage = (opening_price_on_sell_day - closing_price_on_buyout_day) / closing_price_on_buyout_day * 100\n",
    "        \n",
    "        observed_chunks += [observed_chunk]\n",
    "        observation_results += [percentage_to_label(relative_price_change_as_percentage)]\n",
    "    \n",
    "    return observed_chunks, observation_results\n",
    "\n",
    "observed_chunks, observation_results = investor_observes_stocks_for(x_days=OBSERVATION_TIME, then_buy_stocks=True, and_sells_them=True, after_y_days=PREDICTION_AFTER_DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2    2005\n",
       "1    1598\n",
       "0    1357\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(observation_results)\n",
    "plt.show()\n",
    "\n",
    "pd.Series(observation_results).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** Selecting test/valiations chunks randomly from the whole set of observation chunks seems to be a big mistake. It causes situation where network actually seen chunks from the future, therefore the more overfitted network, the better results will be. Network should be trained incrementally - with data from time x to x+1 and validated with data from x+1 to x+2. Then learned with data from x to x+2 and validated with data from x+3 to x+3 and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def categorical_labels(label_array):\n",
    "    return to_categorical(label_array)\n",
    "\n",
    "categorical_labels(observation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 60, 16)            336       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 60, 16)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 60, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 60, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 60, 8)             520       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 60, 8)             32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 60, 8)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 60, 8)             0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                30784     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 32,187\n",
      "Trainable params: 32,011\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (60, 5), filters=16, kernel_size=4, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution1D(filters=8, kernel_size=4, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(optimizer=Nadam(lr=0.002),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4464, 60, 5), (4464, 3), (496, 60, 5), (496, 3))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(list(map(lambda df: df[['Open', 'High', 'Low', 'Close', 'Adj Close']].to_numpy(), observed_chunks)))\n",
    "Y = np.array(categorical_labels(observation_results))\n",
    "\n",
    "def chronological_split(X_data, Y_data, test_size=0.25):\n",
    "    training_test_split_index = int((1 - test_size) * len(X_data))\n",
    "    X_train = X_data[:training_test_split_index]\n",
    "    Y_train = Y_data[:training_test_split_index]\n",
    "    X_test = X_data[training_test_split_index:]\n",
    "    Y_test = Y_data[training_test_split_index:]\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "def random_split(X_data, Y_data, test_size=0.25):\n",
    "    return train_test_split(X_data, Y_data, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = random_split(X, Y, TEST_SIZE) if RANDOM_SPLIT else chronological_split(X, Y, TEST_SIZE)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4464 samples, validate on 496 samples\n",
      "Epoch 1/500\n",
      "4464/4464 [==============================] - 1s 261us/step - loss: 0.9900 - tp: 1438.0000 - fp: 997.0000 - tn: 7931.0000 - fn: 3026.0000 - accuracy: 0.5262 - precision: 0.5906 - recall: 0.3221 - auc: 0.7071 - val_loss: 1.7286 - val_tp: 141.0000 - val_fp: 258.0000 - val_tn: 734.0000 - val_fn: 355.0000 - val_accuracy: 0.3367 - val_precision: 0.3534 - val_recall: 0.2843 - val_auc: 0.4827\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.72863, saving model to model.hdf5\n",
      "Epoch 2/500\n",
      "4464/4464 [==============================] - 0s 77us/step - loss: 0.9833 - tp: 1452.0000 - fp: 954.0000 - tn: 7974.0000 - fn: 3012.0000 - accuracy: 0.5224 - precision: 0.6035 - recall: 0.3253 - auc: 0.7098 - val_loss: 1.3920 - val_tp: 130.0000 - val_fp: 213.0000 - val_tn: 779.0000 - val_fn: 366.0000 - val_accuracy: 0.3589 - val_precision: 0.3790 - val_recall: 0.2621 - val_auc: 0.5581\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.72863 to 1.39199, saving model to model.hdf5\n",
      "Epoch 3/500\n",
      "4464/4464 [==============================] - 0s 83us/step - loss: 0.9804 - tp: 1447.0000 - fp: 981.0000 - tn: 7947.0000 - fn: 3017.0000 - accuracy: 0.5237 - precision: 0.5960 - recall: 0.3241 - auc: 0.7110 - val_loss: 1.3096 - val_tp: 143.0000 - val_fp: 215.0000 - val_tn: 777.0000 - val_fn: 353.0000 - val_accuracy: 0.3730 - val_precision: 0.3994 - val_recall: 0.2883 - val_auc: 0.5621\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.39199 to 1.30963, saving model to model.hdf5\n",
      "Epoch 4/500\n",
      "4464/4464 [==============================] - 0s 80us/step - loss: 0.9721 - tp: 1448.0000 - fp: 951.0000 - tn: 7977.0000 - fn: 3016.0000 - accuracy: 0.5280 - precision: 0.6036 - recall: 0.3244 - auc: 0.7127 - val_loss: 1.3702 - val_tp: 162.0000 - val_fp: 213.0000 - val_tn: 779.0000 - val_fn: 334.0000 - val_accuracy: 0.4234 - val_precision: 0.4320 - val_recall: 0.3266 - val_auc: 0.5976\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.30963\n",
      "Epoch 5/500\n",
      "4464/4464 [==============================] - 0s 81us/step - loss: 0.9678 - tp: 1499.0000 - fp: 891.0000 - tn: 8037.0000 - fn: 2965.0000 - accuracy: 0.5381 - precision: 0.6272 - recall: 0.3358 - auc: 0.7197 - val_loss: 1.2623 - val_tp: 92.0000 - val_fp: 136.0000 - val_tn: 856.0000 - val_fn: 404.0000 - val_accuracy: 0.3488 - val_precision: 0.4035 - val_recall: 0.1855 - val_auc: 0.5174\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.30963 to 1.26231, saving model to model.hdf5\n",
      "Epoch 6/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9791 - tp: 1430.0000 - fp: 937.0000 - tn: 7991.0000 - fn: 3034.0000 - accuracy: 0.5226 - precision: 0.6041 - recall: 0.3203 - auc: 0.7094 - val_loss: 1.6811 - val_tp: 166.0000 - val_fp: 304.0000 - val_tn: 688.0000 - val_fn: 330.0000 - val_accuracy: 0.3468 - val_precision: 0.3532 - val_recall: 0.3347 - val_auc: 0.5358\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.26231\n",
      "Epoch 7/500\n",
      "4464/4464 [==============================] - 0s 76us/step - loss: 0.9601 - tp: 1431.0000 - fp: 863.0000 - tn: 8065.0000 - fn: 3033.0000 - accuracy: 0.5298 - precision: 0.6238 - recall: 0.3206 - auc: 0.7214 - val_loss: 1.5859 - val_tp: 167.0000 - val_fp: 279.0000 - val_tn: 713.0000 - val_fn: 329.0000 - val_accuracy: 0.3831 - val_precision: 0.3744 - val_recall: 0.3367 - val_auc: 0.5292\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.26231\n",
      "Epoch 8/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9635 - tp: 1399.0000 - fp: 872.0000 - tn: 8056.0000 - fn: 3065.0000 - accuracy: 0.5432 - precision: 0.6160 - recall: 0.3134 - auc: 0.7206 - val_loss: 1.7135 - val_tp: 151.0000 - val_fp: 288.0000 - val_tn: 704.0000 - val_fn: 345.0000 - val_accuracy: 0.3407 - val_precision: 0.3440 - val_recall: 0.3044 - val_auc: 0.4812\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.26231\n",
      "Epoch 9/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9633 - tp: 1427.0000 - fp: 823.0000 - tn: 8105.0000 - fn: 3037.0000 - accuracy: 0.5441 - precision: 0.6342 - recall: 0.3197 - auc: 0.7243 - val_loss: 1.5468 - val_tp: 159.0000 - val_fp: 244.0000 - val_tn: 748.0000 - val_fn: 337.0000 - val_accuracy: 0.3669 - val_precision: 0.3945 - val_recall: 0.3206 - val_auc: 0.5333\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.26231\n",
      "Epoch 10/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9669 - tp: 1455.0000 - fp: 893.0000 - tn: 8035.0000 - fn: 3009.0000 - accuracy: 0.5385 - precision: 0.6197 - recall: 0.3259 - auc: 0.7204 - val_loss: 2.6661 - val_tp: 156.0000 - val_fp: 331.0000 - val_tn: 661.0000 - val_fn: 340.0000 - val_accuracy: 0.3206 - val_precision: 0.3203 - val_recall: 0.3145 - val_auc: 0.5143\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.26231\n",
      "Epoch 11/500\n",
      "4464/4464 [==============================] - 0s 74us/step - loss: 0.9641 - tp: 1446.0000 - fp: 863.0000 - tn: 8065.0000 - fn: 3018.0000 - accuracy: 0.5332 - precision: 0.6262 - recall: 0.3239 - auc: 0.7212 - val_loss: 1.6216 - val_tp: 120.0000 - val_fp: 257.0000 - val_tn: 735.0000 - val_fn: 376.0000 - val_accuracy: 0.3085 - val_precision: 0.3183 - val_recall: 0.2419 - val_auc: 0.4614\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.26231\n",
      "Epoch 12/500\n",
      "4464/4464 [==============================] - 0s 74us/step - loss: 0.9534 - tp: 1464.0000 - fp: 862.0000 - tn: 8066.0000 - fn: 3000.0000 - accuracy: 0.5392 - precision: 0.6294 - recall: 0.3280 - auc: 0.7263 - val_loss: 1.6289 - val_tp: 150.0000 - val_fp: 283.0000 - val_tn: 709.0000 - val_fn: 346.0000 - val_accuracy: 0.3327 - val_precision: 0.3464 - val_recall: 0.3024 - val_auc: 0.4897\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.26231\n",
      "Epoch 13/500\n",
      "4464/4464 [==============================] - 0s 76us/step - loss: 0.9631 - tp: 1433.0000 - fp: 859.0000 - tn: 8069.0000 - fn: 3031.0000 - accuracy: 0.5388 - precision: 0.6252 - recall: 0.3210 - auc: 0.7231 - val_loss: 1.3772 - val_tp: 117.0000 - val_fp: 207.0000 - val_tn: 785.0000 - val_fn: 379.0000 - val_accuracy: 0.3306 - val_precision: 0.3611 - val_recall: 0.2359 - val_auc: 0.4840\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.26231\n",
      "Epoch 14/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9517 - tp: 1436.0000 - fp: 809.0000 - tn: 8119.0000 - fn: 3028.0000 - accuracy: 0.5482 - precision: 0.6396 - recall: 0.3217 - auc: 0.7314 - val_loss: 2.7068 - val_tp: 201.0000 - val_fp: 278.0000 - val_tn: 714.0000 - val_fn: 295.0000 - val_accuracy: 0.4234 - val_precision: 0.4196 - val_recall: 0.4052 - val_auc: 0.5909\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.26231\n",
      "Epoch 15/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9583 - tp: 1451.0000 - fp: 866.0000 - tn: 8062.0000 - fn: 3013.0000 - accuracy: 0.5475 - precision: 0.6262 - recall: 0.3250 - auc: 0.7262 - val_loss: 1.5314 - val_tp: 137.0000 - val_fp: 224.0000 - val_tn: 768.0000 - val_fn: 359.0000 - val_accuracy: 0.3468 - val_precision: 0.3795 - val_recall: 0.2762 - val_auc: 0.5327\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.26231\n",
      "Epoch 16/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9595 - tp: 1450.0000 - fp: 859.0000 - tn: 8069.0000 - fn: 3014.0000 - accuracy: 0.5426 - precision: 0.6280 - recall: 0.3248 - auc: 0.7230 - val_loss: 1.5913 - val_tp: 143.0000 - val_fp: 295.0000 - val_tn: 697.0000 - val_fn: 353.0000 - val_accuracy: 0.3125 - val_precision: 0.3265 - val_recall: 0.2883 - val_auc: 0.4630\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.26231\n",
      "Epoch 17/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9586 - tp: 1466.0000 - fp: 836.0000 - tn: 8092.0000 - fn: 2998.0000 - accuracy: 0.5455 - precision: 0.6368 - recall: 0.3284 - auc: 0.7277 - val_loss: 1.4409 - val_tp: 103.0000 - val_fp: 191.0000 - val_tn: 801.0000 - val_fn: 393.0000 - val_accuracy: 0.3125 - val_precision: 0.3503 - val_recall: 0.2077 - val_auc: 0.4884\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.26231\n",
      "Epoch 18/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9551 - tp: 1477.0000 - fp: 869.0000 - tn: 8059.0000 - fn: 2987.0000 - accuracy: 0.5356 - precision: 0.6296 - recall: 0.3309 - auc: 0.7254 - val_loss: 2.1978 - val_tp: 182.0000 - val_fp: 304.0000 - val_tn: 688.0000 - val_fn: 314.0000 - val_accuracy: 0.3750 - val_precision: 0.3745 - val_recall: 0.3669 - val_auc: 0.5732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_loss did not improve from 1.26231\n",
      "Epoch 19/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9524 - tp: 1461.0000 - fp: 836.0000 - tn: 8092.0000 - fn: 3003.0000 - accuracy: 0.5470 - precision: 0.6360 - recall: 0.3273 - auc: 0.7306 - val_loss: 2.1223 - val_tp: 167.0000 - val_fp: 320.0000 - val_tn: 672.0000 - val_fn: 329.0000 - val_accuracy: 0.3407 - val_precision: 0.3429 - val_recall: 0.3367 - val_auc: 0.5530\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.26231\n",
      "Epoch 20/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9648 - tp: 1386.0000 - fp: 842.0000 - tn: 8086.0000 - fn: 3078.0000 - accuracy: 0.5374 - precision: 0.6221 - recall: 0.3105 - auc: 0.7217 - val_loss: 2.2826 - val_tp: 205.0000 - val_fp: 289.0000 - val_tn: 703.0000 - val_fn: 291.0000 - val_accuracy: 0.4153 - val_precision: 0.4150 - val_recall: 0.4133 - val_auc: 0.6067\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.26231\n",
      "Epoch 21/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9570 - tp: 1459.0000 - fp: 841.0000 - tn: 8087.0000 - fn: 3005.0000 - accuracy: 0.5455 - precision: 0.6343 - recall: 0.3268 - auc: 0.7271 - val_loss: 1.8664 - val_tp: 206.0000 - val_fp: 245.0000 - val_tn: 747.0000 - val_fn: 290.0000 - val_accuracy: 0.4556 - val_precision: 0.4568 - val_recall: 0.4153 - val_auc: 0.5900\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.26231\n",
      "Epoch 22/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9509 - tp: 1484.0000 - fp: 842.0000 - tn: 8086.0000 - fn: 2980.0000 - accuracy: 0.5488 - precision: 0.6380 - recall: 0.3324 - auc: 0.7309 - val_loss: 1.9791 - val_tp: 160.0000 - val_fp: 323.0000 - val_tn: 669.0000 - val_fn: 336.0000 - val_accuracy: 0.3286 - val_precision: 0.3313 - val_recall: 0.3226 - val_auc: 0.5243\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.26231\n",
      "Epoch 23/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9570 - tp: 1485.0000 - fp: 861.0000 - tn: 8067.0000 - fn: 2979.0000 - accuracy: 0.5381 - precision: 0.6330 - recall: 0.3327 - auc: 0.7241 - val_loss: 1.5683 - val_tp: 158.0000 - val_fp: 275.0000 - val_tn: 717.0000 - val_fn: 338.0000 - val_accuracy: 0.3508 - val_precision: 0.3649 - val_recall: 0.3185 - val_auc: 0.5337\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.26231\n",
      "Epoch 24/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9564 - tp: 1432.0000 - fp: 836.0000 - tn: 8092.0000 - fn: 3032.0000 - accuracy: 0.5441 - precision: 0.6314 - recall: 0.3208 - auc: 0.7257 - val_loss: 1.4043 - val_tp: 125.0000 - val_fp: 215.0000 - val_tn: 777.0000 - val_fn: 371.0000 - val_accuracy: 0.3609 - val_precision: 0.3676 - val_recall: 0.2520 - val_auc: 0.4942\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.26231\n",
      "Epoch 25/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9649 - tp: 1319.0000 - fp: 809.0000 - tn: 8119.0000 - fn: 3145.0000 - accuracy: 0.5374 - precision: 0.6198 - recall: 0.2955 - auc: 0.7198 - val_loss: 1.4338 - val_tp: 167.0000 - val_fp: 266.0000 - val_tn: 726.0000 - val_fn: 329.0000 - val_accuracy: 0.3730 - val_precision: 0.3857 - val_recall: 0.3367 - val_auc: 0.5369\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.26231\n",
      "Epoch 26/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9603 - tp: 1400.0000 - fp: 838.0000 - tn: 8090.0000 - fn: 3064.0000 - accuracy: 0.5477 - precision: 0.6256 - recall: 0.3136 - auc: 0.7248 - val_loss: 1.4031 - val_tp: 169.0000 - val_fp: 242.0000 - val_tn: 750.0000 - val_fn: 327.0000 - val_accuracy: 0.4093 - val_precision: 0.4112 - val_recall: 0.3407 - val_auc: 0.5828\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.26231\n",
      "Epoch 27/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9588 - tp: 1450.0000 - fp: 849.0000 - tn: 8079.0000 - fn: 3014.0000 - accuracy: 0.5522 - precision: 0.6307 - recall: 0.3248 - auc: 0.7286 - val_loss: 1.2069 - val_tp: 106.0000 - val_fp: 124.0000 - val_tn: 868.0000 - val_fn: 390.0000 - val_accuracy: 0.4315 - val_precision: 0.4609 - val_recall: 0.2137 - val_auc: 0.5816\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.26231 to 1.20694, saving model to model.hdf5\n",
      "Epoch 28/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9546 - tp: 1436.0000 - fp: 856.0000 - tn: 8072.0000 - fn: 3028.0000 - accuracy: 0.5444 - precision: 0.6265 - recall: 0.3217 - auc: 0.7279 - val_loss: 1.4094 - val_tp: 144.0000 - val_fp: 219.0000 - val_tn: 773.0000 - val_fn: 352.0000 - val_accuracy: 0.4012 - val_precision: 0.3967 - val_recall: 0.2903 - val_auc: 0.5657\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.20694\n",
      "Epoch 29/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9492 - tp: 1471.0000 - fp: 821.0000 - tn: 8107.0000 - fn: 2993.0000 - accuracy: 0.5477 - precision: 0.6418 - recall: 0.3295 - auc: 0.7332 - val_loss: 2.4949 - val_tp: 193.0000 - val_fp: 299.0000 - val_tn: 693.0000 - val_fn: 303.0000 - val_accuracy: 0.3911 - val_precision: 0.3923 - val_recall: 0.3891 - val_auc: 0.5913\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.20694\n",
      "Epoch 30/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9577 - tp: 1415.0000 - fp: 786.0000 - tn: 8142.0000 - fn: 3049.0000 - accuracy: 0.5455 - precision: 0.6429 - recall: 0.3170 - auc: 0.7269 - val_loss: 1.5489 - val_tp: 202.0000 - val_fp: 244.0000 - val_tn: 748.0000 - val_fn: 294.0000 - val_accuracy: 0.4456 - val_precision: 0.4529 - val_recall: 0.4073 - val_auc: 0.5949\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.20694\n",
      "Epoch 31/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9571 - tp: 1403.0000 - fp: 776.0000 - tn: 8152.0000 - fn: 3061.0000 - accuracy: 0.5468 - precision: 0.6439 - recall: 0.3143 - auc: 0.7254 - val_loss: 1.3517 - val_tp: 109.0000 - val_fp: 169.0000 - val_tn: 823.0000 - val_fn: 387.0000 - val_accuracy: 0.3730 - val_precision: 0.3921 - val_recall: 0.2198 - val_auc: 0.5222\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.20694\n",
      "Epoch 32/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9504 - tp: 1458.0000 - fp: 871.0000 - tn: 8057.0000 - fn: 3006.0000 - accuracy: 0.5511 - precision: 0.6260 - recall: 0.3266 - auc: 0.7309 - val_loss: 1.2563 - val_tp: 131.0000 - val_fp: 197.0000 - val_tn: 795.0000 - val_fn: 365.0000 - val_accuracy: 0.3750 - val_precision: 0.3994 - val_recall: 0.2641 - val_auc: 0.5371\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.20694\n",
      "Epoch 33/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9500 - tp: 1457.0000 - fp: 825.0000 - tn: 8103.0000 - fn: 3007.0000 - accuracy: 0.5506 - precision: 0.6385 - recall: 0.3264 - auc: 0.7307 - val_loss: 1.9178 - val_tp: 200.0000 - val_fp: 268.0000 - val_tn: 724.0000 - val_fn: 296.0000 - val_accuracy: 0.4315 - val_precision: 0.4274 - val_recall: 0.4032 - val_auc: 0.5946\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.20694\n",
      "Epoch 34/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9644 - tp: 1423.0000 - fp: 884.0000 - tn: 8044.0000 - fn: 3041.0000 - accuracy: 0.5388 - precision: 0.6168 - recall: 0.3188 - auc: 0.7199 - val_loss: 1.3511 - val_tp: 192.0000 - val_fp: 233.0000 - val_tn: 759.0000 - val_fn: 304.0000 - val_accuracy: 0.4597 - val_precision: 0.4518 - val_recall: 0.3871 - val_auc: 0.6210\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.20694\n",
      "Epoch 35/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9485 - tp: 1493.0000 - fp: 858.0000 - tn: 8070.0000 - fn: 2971.0000 - accuracy: 0.5470 - precision: 0.6350 - recall: 0.3345 - auc: 0.7319 - val_loss: 1.7787 - val_tp: 183.0000 - val_fp: 295.0000 - val_tn: 697.0000 - val_fn: 313.0000 - val_accuracy: 0.3750 - val_precision: 0.3828 - val_recall: 0.3690 - val_auc: 0.5630\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.20694\n",
      "Epoch 36/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9506 - tp: 1483.0000 - fp: 848.0000 - tn: 8080.0000 - fn: 2981.0000 - accuracy: 0.5444 - precision: 0.6362 - recall: 0.3322 - auc: 0.7316 - val_loss: 1.5621 - val_tp: 162.0000 - val_fp: 287.0000 - val_tn: 705.0000 - val_fn: 334.0000 - val_accuracy: 0.3488 - val_precision: 0.3608 - val_recall: 0.3266 - val_auc: 0.5124\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.20694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9504 - tp: 1463.0000 - fp: 813.0000 - tn: 8115.0000 - fn: 3001.0000 - accuracy: 0.5497 - precision: 0.6428 - recall: 0.3277 - auc: 0.7299 - val_loss: 1.2523 - val_tp: 137.0000 - val_fp: 162.0000 - val_tn: 830.0000 - val_fn: 359.0000 - val_accuracy: 0.3992 - val_precision: 0.4582 - val_recall: 0.2762 - val_auc: 0.5723\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.20694\n",
      "Epoch 38/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9561 - tp: 1468.0000 - fp: 839.0000 - tn: 8089.0000 - fn: 2996.0000 - accuracy: 0.5497 - precision: 0.6363 - recall: 0.3289 - auc: 0.7298 - val_loss: 1.3967 - val_tp: 139.0000 - val_fp: 194.0000 - val_tn: 798.0000 - val_fn: 357.0000 - val_accuracy: 0.3931 - val_precision: 0.4174 - val_recall: 0.2802 - val_auc: 0.5611\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.20694\n",
      "Epoch 39/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9641 - tp: 1319.0000 - fp: 798.0000 - tn: 8130.0000 - fn: 3145.0000 - accuracy: 0.5336 - precision: 0.6231 - recall: 0.2955 - auc: 0.7219 - val_loss: 1.3024 - val_tp: 114.0000 - val_fp: 144.0000 - val_tn: 848.0000 - val_fn: 382.0000 - val_accuracy: 0.3911 - val_precision: 0.4419 - val_recall: 0.2298 - val_auc: 0.5694\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.20694\n",
      "Epoch 40/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9522 - tp: 1444.0000 - fp: 838.0000 - tn: 8090.0000 - fn: 3020.0000 - accuracy: 0.5466 - precision: 0.6328 - recall: 0.3235 - auc: 0.7291 - val_loss: 1.3314 - val_tp: 123.0000 - val_fp: 165.0000 - val_tn: 827.0000 - val_fn: 373.0000 - val_accuracy: 0.3810 - val_precision: 0.4271 - val_recall: 0.2480 - val_auc: 0.5637\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.20694\n",
      "Epoch 41/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9664 - tp: 1352.0000 - fp: 821.0000 - tn: 8107.0000 - fn: 3112.0000 - accuracy: 0.5311 - precision: 0.6222 - recall: 0.3029 - auc: 0.7181 - val_loss: 1.4533 - val_tp: 142.0000 - val_fp: 313.0000 - val_tn: 679.0000 - val_fn: 354.0000 - val_accuracy: 0.3306 - val_precision: 0.3121 - val_recall: 0.2863 - val_auc: 0.5133\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.20694\n",
      "Epoch 42/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9436 - tp: 1472.0000 - fp: 847.0000 - tn: 8081.0000 - fn: 2992.0000 - accuracy: 0.5531 - precision: 0.6348 - recall: 0.3297 - auc: 0.7338 - val_loss: 1.3914 - val_tp: 152.0000 - val_fp: 267.0000 - val_tn: 725.0000 - val_fn: 344.0000 - val_accuracy: 0.3649 - val_precision: 0.3628 - val_recall: 0.3065 - val_auc: 0.5333\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.20694\n",
      "Epoch 43/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9477 - tp: 1518.0000 - fp: 842.0000 - tn: 8086.0000 - fn: 2946.0000 - accuracy: 0.5464 - precision: 0.6432 - recall: 0.3401 - auc: 0.7332 - val_loss: 1.5565 - val_tp: 174.0000 - val_fp: 264.0000 - val_tn: 728.0000 - val_fn: 322.0000 - val_accuracy: 0.3851 - val_precision: 0.3973 - val_recall: 0.3508 - val_auc: 0.5588\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.20694\n",
      "Epoch 44/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9547 - tp: 1477.0000 - fp: 872.0000 - tn: 8056.0000 - fn: 2987.0000 - accuracy: 0.5479 - precision: 0.6288 - recall: 0.3309 - auc: 0.7287 - val_loss: 1.3385 - val_tp: 132.0000 - val_fp: 186.0000 - val_tn: 806.0000 - val_fn: 364.0000 - val_accuracy: 0.3931 - val_precision: 0.4151 - val_recall: 0.2661 - val_auc: 0.5664\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.20694\n",
      "Epoch 45/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9456 - tp: 1448.0000 - fp: 848.0000 - tn: 8080.0000 - fn: 3016.0000 - accuracy: 0.5484 - precision: 0.6307 - recall: 0.3244 - auc: 0.7325 - val_loss: 1.2179 - val_tp: 132.0000 - val_fp: 173.0000 - val_tn: 819.0000 - val_fn: 364.0000 - val_accuracy: 0.4254 - val_precision: 0.4328 - val_recall: 0.2661 - val_auc: 0.5737\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.20694\n",
      "Epoch 46/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9442 - tp: 1504.0000 - fp: 821.0000 - tn: 8107.0000 - fn: 2960.0000 - accuracy: 0.5540 - precision: 0.6469 - recall: 0.3369 - auc: 0.7357 - val_loss: 2.1229 - val_tp: 189.0000 - val_fp: 294.0000 - val_tn: 698.0000 - val_fn: 307.0000 - val_accuracy: 0.3972 - val_precision: 0.3913 - val_recall: 0.3810 - val_auc: 0.5661\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.20694\n",
      "Epoch 47/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9461 - tp: 1512.0000 - fp: 838.0000 - tn: 8090.0000 - fn: 2952.0000 - accuracy: 0.5587 - precision: 0.6434 - recall: 0.3387 - auc: 0.7371 - val_loss: 1.4838 - val_tp: 150.0000 - val_fp: 235.0000 - val_tn: 757.0000 - val_fn: 346.0000 - val_accuracy: 0.3669 - val_precision: 0.3896 - val_recall: 0.3024 - val_auc: 0.5475\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.20694\n",
      "Epoch 48/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9500 - tp: 1447.0000 - fp: 812.0000 - tn: 8116.0000 - fn: 3017.0000 - accuracy: 0.5506 - precision: 0.6405 - recall: 0.3241 - auc: 0.7324 - val_loss: 1.3175 - val_tp: 167.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 329.0000 - val_accuracy: 0.3871 - val_precision: 0.3986 - val_recall: 0.3367 - val_auc: 0.5687\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.20694\n",
      "Epoch 49/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9473 - tp: 1497.0000 - fp: 848.0000 - tn: 8080.0000 - fn: 2967.0000 - accuracy: 0.5500 - precision: 0.6384 - recall: 0.3353 - auc: 0.7333 - val_loss: 1.6686 - val_tp: 188.0000 - val_fp: 276.0000 - val_tn: 716.0000 - val_fn: 308.0000 - val_accuracy: 0.3972 - val_precision: 0.4052 - val_recall: 0.3790 - val_auc: 0.5677\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.20694\n",
      "Epoch 50/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9426 - tp: 1485.0000 - fp: 800.0000 - tn: 8128.0000 - fn: 2979.0000 - accuracy: 0.5526 - precision: 0.6499 - recall: 0.3327 - auc: 0.7337 - val_loss: 1.7409 - val_tp: 174.0000 - val_fp: 293.0000 - val_tn: 699.0000 - val_fn: 322.0000 - val_accuracy: 0.3629 - val_precision: 0.3726 - val_recall: 0.3508 - val_auc: 0.5574\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.20694\n",
      "Epoch 51/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9511 - tp: 1499.0000 - fp: 822.0000 - tn: 8106.0000 - fn: 2965.0000 - accuracy: 0.5502 - precision: 0.6458 - recall: 0.3358 - auc: 0.7332 - val_loss: 1.3562 - val_tp: 166.0000 - val_fp: 232.0000 - val_tn: 760.0000 - val_fn: 330.0000 - val_accuracy: 0.4194 - val_precision: 0.4171 - val_recall: 0.3347 - val_auc: 0.5784\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.20694\n",
      "Epoch 52/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9508 - tp: 1463.0000 - fp: 863.0000 - tn: 8065.0000 - fn: 3001.0000 - accuracy: 0.5464 - precision: 0.6290 - recall: 0.3277 - auc: 0.7303 - val_loss: 1.3897 - val_tp: 170.0000 - val_fp: 211.0000 - val_tn: 781.0000 - val_fn: 326.0000 - val_accuracy: 0.4173 - val_precision: 0.4462 - val_recall: 0.3427 - val_auc: 0.5919\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.20694\n",
      "Epoch 53/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9575 - tp: 1455.0000 - fp: 815.0000 - tn: 8113.0000 - fn: 3009.0000 - accuracy: 0.5426 - precision: 0.6410 - recall: 0.3259 - auc: 0.7269 - val_loss: 2.0705 - val_tp: 185.0000 - val_fp: 302.0000 - val_tn: 690.0000 - val_fn: 311.0000 - val_accuracy: 0.3871 - val_precision: 0.3799 - val_recall: 0.3730 - val_auc: 0.5770\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.20694\n",
      "Epoch 54/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9427 - tp: 1507.0000 - fp: 830.0000 - tn: 8098.0000 - fn: 2957.0000 - accuracy: 0.5488 - precision: 0.6448 - recall: 0.3376 - auc: 0.7365 - val_loss: 2.0289 - val_tp: 190.0000 - val_fp: 301.0000 - val_tn: 691.0000 - val_fn: 306.0000 - val_accuracy: 0.3851 - val_precision: 0.3870 - val_recall: 0.3831 - val_auc: 0.5679\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.20694\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9392 - tp: 1502.0000 - fp: 858.0000 - tn: 8070.0000 - fn: 2962.0000 - accuracy: 0.5553 - precision: 0.6364 - recall: 0.3365 - auc: 0.7380 - val_loss: 1.3833 - val_tp: 171.0000 - val_fp: 236.0000 - val_tn: 756.0000 - val_fn: 325.0000 - val_accuracy: 0.3992 - val_precision: 0.4201 - val_recall: 0.3448 - val_auc: 0.5696\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.20694\n",
      "Epoch 56/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9374 - tp: 1538.0000 - fp: 815.0000 - tn: 8113.0000 - fn: 2926.0000 - accuracy: 0.5526 - precision: 0.6536 - recall: 0.3445 - auc: 0.7412 - val_loss: 1.2069 - val_tp: 125.0000 - val_fp: 144.0000 - val_tn: 848.0000 - val_fn: 371.0000 - val_accuracy: 0.4133 - val_precision: 0.4647 - val_recall: 0.2520 - val_auc: 0.5891\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.20694 to 1.20685, saving model to model.hdf5\n",
      "Epoch 57/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9416 - tp: 1517.0000 - fp: 852.0000 - tn: 8076.0000 - fn: 2947.0000 - accuracy: 0.5605 - precision: 0.6404 - recall: 0.3398 - auc: 0.7381 - val_loss: 1.3658 - val_tp: 136.0000 - val_fp: 197.0000 - val_tn: 795.0000 - val_fn: 360.0000 - val_accuracy: 0.3952 - val_precision: 0.4084 - val_recall: 0.2742 - val_auc: 0.5680\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.20685\n",
      "Epoch 58/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9433 - tp: 1519.0000 - fp: 836.0000 - tn: 8092.0000 - fn: 2945.0000 - accuracy: 0.5529 - precision: 0.6450 - recall: 0.3403 - auc: 0.7353 - val_loss: 1.5942 - val_tp: 184.0000 - val_fp: 232.0000 - val_tn: 760.0000 - val_fn: 312.0000 - val_accuracy: 0.4415 - val_precision: 0.4423 - val_recall: 0.3710 - val_auc: 0.5962\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.20685\n",
      "Epoch 59/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9464 - tp: 1469.0000 - fp: 852.0000 - tn: 8076.0000 - fn: 2995.0000 - accuracy: 0.5444 - precision: 0.6329 - recall: 0.3291 - auc: 0.7312 - val_loss: 1.3658 - val_tp: 187.0000 - val_fp: 237.0000 - val_tn: 755.0000 - val_fn: 309.0000 - val_accuracy: 0.4294 - val_precision: 0.4410 - val_recall: 0.3770 - val_auc: 0.5944\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.20685\n",
      "Epoch 60/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9430 - tp: 1460.0000 - fp: 815.0000 - tn: 8113.0000 - fn: 3004.0000 - accuracy: 0.5560 - precision: 0.6418 - recall: 0.3271 - auc: 0.7372 - val_loss: 1.3003 - val_tp: 115.0000 - val_fp: 152.0000 - val_tn: 840.0000 - val_fn: 381.0000 - val_accuracy: 0.4173 - val_precision: 0.4307 - val_recall: 0.2319 - val_auc: 0.5734\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.20685\n",
      "Epoch 61/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9409 - tp: 1541.0000 - fp: 834.0000 - tn: 8094.0000 - fn: 2923.0000 - accuracy: 0.5531 - precision: 0.6488 - recall: 0.3452 - auc: 0.7376 - val_loss: 1.3055 - val_tp: 158.0000 - val_fp: 237.0000 - val_tn: 755.0000 - val_fn: 338.0000 - val_accuracy: 0.3952 - val_precision: 0.4000 - val_recall: 0.3185 - val_auc: 0.5674\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.20685\n",
      "Epoch 62/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9313 - tp: 1521.0000 - fp: 808.0000 - tn: 8120.0000 - fn: 2943.0000 - accuracy: 0.5578 - precision: 0.6531 - recall: 0.3407 - auc: 0.7430 - val_loss: 1.2798 - val_tp: 103.0000 - val_fp: 146.0000 - val_tn: 846.0000 - val_fn: 393.0000 - val_accuracy: 0.3730 - val_precision: 0.4137 - val_recall: 0.2077 - val_auc: 0.5589\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.20685\n",
      "Epoch 63/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9376 - tp: 1561.0000 - fp: 840.0000 - tn: 8088.0000 - fn: 2903.0000 - accuracy: 0.5556 - precision: 0.6501 - recall: 0.3497 - auc: 0.7395 - val_loss: 1.2507 - val_tp: 121.0000 - val_fp: 155.0000 - val_tn: 837.0000 - val_fn: 375.0000 - val_accuracy: 0.3992 - val_precision: 0.4384 - val_recall: 0.2440 - val_auc: 0.5714\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.20685\n",
      "Epoch 64/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9393 - tp: 1538.0000 - fp: 873.0000 - tn: 8055.0000 - fn: 2926.0000 - accuracy: 0.5573 - precision: 0.6379 - recall: 0.3445 - auc: 0.7398 - val_loss: 1.3559 - val_tp: 150.0000 - val_fp: 213.0000 - val_tn: 779.0000 - val_fn: 346.0000 - val_accuracy: 0.3871 - val_precision: 0.4132 - val_recall: 0.3024 - val_auc: 0.5511\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.20685\n",
      "Epoch 65/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9433 - tp: 1537.0000 - fp: 860.0000 - tn: 8068.0000 - fn: 2927.0000 - accuracy: 0.5493 - precision: 0.6412 - recall: 0.3443 - auc: 0.7370 - val_loss: 1.4631 - val_tp: 164.0000 - val_fp: 215.0000 - val_tn: 777.0000 - val_fn: 332.0000 - val_accuracy: 0.4133 - val_precision: 0.4327 - val_recall: 0.3306 - val_auc: 0.5820\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.20685\n",
      "Epoch 66/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9368 - tp: 1586.0000 - fp: 838.0000 - tn: 8090.0000 - fn: 2878.0000 - accuracy: 0.5663 - precision: 0.6543 - recall: 0.3553 - auc: 0.7435 - val_loss: 1.8850 - val_tp: 189.0000 - val_fp: 269.0000 - val_tn: 723.0000 - val_fn: 307.0000 - val_accuracy: 0.4173 - val_precision: 0.4127 - val_recall: 0.3810 - val_auc: 0.5827\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.20685\n",
      "Epoch 67/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9454 - tp: 1477.0000 - fp: 889.0000 - tn: 8039.0000 - fn: 2987.0000 - accuracy: 0.5482 - precision: 0.6243 - recall: 0.3309 - auc: 0.7334 - val_loss: 2.9489 - val_tp: 154.0000 - val_fp: 338.0000 - val_tn: 654.0000 - val_fn: 342.0000 - val_accuracy: 0.3125 - val_precision: 0.3130 - val_recall: 0.3105 - val_auc: 0.5030\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.20685\n",
      "Epoch 68/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9415 - tp: 1511.0000 - fp: 804.0000 - tn: 8124.0000 - fn: 2953.0000 - accuracy: 0.5526 - precision: 0.6527 - recall: 0.3385 - auc: 0.7369 - val_loss: 1.7834 - val_tp: 179.0000 - val_fp: 293.0000 - val_tn: 699.0000 - val_fn: 317.0000 - val_accuracy: 0.3690 - val_precision: 0.3792 - val_recall: 0.3609 - val_auc: 0.5577\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.20685\n",
      "Epoch 69/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9337 - tp: 1580.0000 - fp: 865.0000 - tn: 8063.0000 - fn: 2884.0000 - accuracy: 0.5580 - precision: 0.6462 - recall: 0.3539 - auc: 0.7418 - val_loss: 1.5507 - val_tp: 156.0000 - val_fp: 239.0000 - val_tn: 753.0000 - val_fn: 340.0000 - val_accuracy: 0.3750 - val_precision: 0.3949 - val_recall: 0.3145 - val_auc: 0.5440\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.20685\n",
      "Epoch 70/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9503 - tp: 1473.0000 - fp: 843.0000 - tn: 8085.0000 - fn: 2991.0000 - accuracy: 0.5515 - precision: 0.6360 - recall: 0.3300 - auc: 0.7327 - val_loss: 2.7563 - val_tp: 210.0000 - val_fp: 280.0000 - val_tn: 712.0000 - val_fn: 286.0000 - val_accuracy: 0.4315 - val_precision: 0.4286 - val_recall: 0.4234 - val_auc: 0.6046\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.20685\n",
      "Epoch 71/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9444 - tp: 1537.0000 - fp: 815.0000 - tn: 8113.0000 - fn: 2927.0000 - accuracy: 0.5493 - precision: 0.6535 - recall: 0.3443 - auc: 0.7362 - val_loss: 1.7116 - val_tp: 163.0000 - val_fp: 313.0000 - val_tn: 679.0000 - val_fn: 333.0000 - val_accuracy: 0.3427 - val_precision: 0.3424 - val_recall: 0.3286 - val_auc: 0.5209\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.20685\n",
      "Epoch 72/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9379 - tp: 1559.0000 - fp: 837.0000 - tn: 8091.0000 - fn: 2905.0000 - accuracy: 0.5582 - precision: 0.6507 - recall: 0.3492 - auc: 0.7401 - val_loss: 1.8482 - val_tp: 216.0000 - val_fp: 269.0000 - val_tn: 723.0000 - val_fn: 280.0000 - val_accuracy: 0.4456 - val_precision: 0.4454 - val_recall: 0.4355 - val_auc: 0.5973\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.20685\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9452 - tp: 1506.0000 - fp: 845.0000 - tn: 8083.0000 - fn: 2958.0000 - accuracy: 0.5437 - precision: 0.6406 - recall: 0.3374 - auc: 0.7340 - val_loss: 1.9692 - val_tp: 160.0000 - val_fp: 322.0000 - val_tn: 670.0000 - val_fn: 336.0000 - val_accuracy: 0.3306 - val_precision: 0.3320 - val_recall: 0.3226 - val_auc: 0.5319\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.20685\n",
      "Epoch 74/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9349 - tp: 1538.0000 - fp: 766.0000 - tn: 8162.0000 - fn: 2926.0000 - accuracy: 0.5616 - precision: 0.6675 - recall: 0.3445 - auc: 0.7441 - val_loss: 1.3243 - val_tp: 110.0000 - val_fp: 159.0000 - val_tn: 833.0000 - val_fn: 386.0000 - val_accuracy: 0.3730 - val_precision: 0.4089 - val_recall: 0.2218 - val_auc: 0.5390\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.20685\n",
      "Epoch 75/500\n",
      "4464/4464 [==============================] - 0s 74us/step - loss: 0.9401 - tp: 1561.0000 - fp: 847.0000 - tn: 8081.0000 - fn: 2903.0000 - accuracy: 0.5600 - precision: 0.6483 - recall: 0.3497 - auc: 0.7391 - val_loss: 1.6504 - val_tp: 186.0000 - val_fp: 280.0000 - val_tn: 712.0000 - val_fn: 310.0000 - val_accuracy: 0.3992 - val_precision: 0.3991 - val_recall: 0.3750 - val_auc: 0.5625\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.20685\n",
      "Epoch 76/500\n",
      "4464/4464 [==============================] - 0s 79us/step - loss: 0.9485 - tp: 1451.0000 - fp: 858.0000 - tn: 8070.0000 - fn: 3013.0000 - accuracy: 0.5484 - precision: 0.6284 - recall: 0.3250 - auc: 0.7318 - val_loss: 1.8522 - val_tp: 233.0000 - val_fp: 253.0000 - val_tn: 739.0000 - val_fn: 263.0000 - val_accuracy: 0.4758 - val_precision: 0.4794 - val_recall: 0.4698 - val_auc: 0.6101\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.20685\n",
      "Epoch 77/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9377 - tp: 1542.0000 - fp: 792.0000 - tn: 8136.0000 - fn: 2922.0000 - accuracy: 0.5585 - precision: 0.6607 - recall: 0.3454 - auc: 0.7417 - val_loss: 1.5664 - val_tp: 162.0000 - val_fp: 302.0000 - val_tn: 690.0000 - val_fn: 334.0000 - val_accuracy: 0.3387 - val_precision: 0.3491 - val_recall: 0.3266 - val_auc: 0.5345\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.20685\n",
      "Epoch 78/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9404 - tp: 1547.0000 - fp: 872.0000 - tn: 8056.0000 - fn: 2917.0000 - accuracy: 0.5466 - precision: 0.6395 - recall: 0.3466 - auc: 0.7371 - val_loss: 1.5435 - val_tp: 178.0000 - val_fp: 238.0000 - val_tn: 754.0000 - val_fn: 318.0000 - val_accuracy: 0.4133 - val_precision: 0.4279 - val_recall: 0.3589 - val_auc: 0.5695\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.20685\n",
      "Epoch 79/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9382 - tp: 1559.0000 - fp: 855.0000 - tn: 8073.0000 - fn: 2905.0000 - accuracy: 0.5549 - precision: 0.6458 - recall: 0.3492 - auc: 0.7383 - val_loss: 1.4239 - val_tp: 147.0000 - val_fp: 209.0000 - val_tn: 783.0000 - val_fn: 349.0000 - val_accuracy: 0.3931 - val_precision: 0.4129 - val_recall: 0.2964 - val_auc: 0.5647\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.20685\n",
      "Epoch 80/500\n",
      "4464/4464 [==============================] - 0s 78us/step - loss: 0.9415 - tp: 1486.0000 - fp: 861.0000 - tn: 8067.0000 - fn: 2978.0000 - accuracy: 0.5488 - precision: 0.6331 - recall: 0.3329 - auc: 0.7361 - val_loss: 1.4399 - val_tp: 168.0000 - val_fp: 286.0000 - val_tn: 706.0000 - val_fn: 328.0000 - val_accuracy: 0.3649 - val_precision: 0.3700 - val_recall: 0.3387 - val_auc: 0.5372\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.20685\n",
      "Epoch 81/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9365 - tp: 1603.0000 - fp: 858.0000 - tn: 8070.0000 - fn: 2861.0000 - accuracy: 0.5634 - precision: 0.6514 - recall: 0.3591 - auc: 0.7420 - val_loss: 1.6410 - val_tp: 190.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 306.0000 - val_accuracy: 0.4194 - val_precision: 0.4299 - val_recall: 0.3831 - val_auc: 0.5683\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.20685\n",
      "Epoch 82/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9359 - tp: 1571.0000 - fp: 817.0000 - tn: 8111.0000 - fn: 2893.0000 - accuracy: 0.5576 - precision: 0.6579 - recall: 0.3519 - auc: 0.7438 - val_loss: 1.4441 - val_tp: 138.0000 - val_fp: 206.0000 - val_tn: 786.0000 - val_fn: 358.0000 - val_accuracy: 0.3649 - val_precision: 0.4012 - val_recall: 0.2782 - val_auc: 0.5506\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.20685\n",
      "Epoch 83/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9418 - tp: 1521.0000 - fp: 854.0000 - tn: 8074.0000 - fn: 2943.0000 - accuracy: 0.5549 - precision: 0.6404 - recall: 0.3407 - auc: 0.7382 - val_loss: 1.4730 - val_tp: 163.0000 - val_fp: 283.0000 - val_tn: 709.0000 - val_fn: 333.0000 - val_accuracy: 0.3710 - val_precision: 0.3655 - val_recall: 0.3286 - val_auc: 0.5625\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.20685\n",
      "Epoch 84/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9275 - tp: 1618.0000 - fp: 902.0000 - tn: 8026.0000 - fn: 2846.0000 - accuracy: 0.5703 - precision: 0.6421 - recall: 0.3625 - auc: 0.7469 - val_loss: 1.3240 - val_tp: 157.0000 - val_fp: 201.0000 - val_tn: 791.0000 - val_fn: 339.0000 - val_accuracy: 0.4214 - val_precision: 0.4385 - val_recall: 0.3165 - val_auc: 0.5891\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.20685\n",
      "Epoch 85/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9372 - tp: 1588.0000 - fp: 872.0000 - tn: 8056.0000 - fn: 2876.0000 - accuracy: 0.5636 - precision: 0.6455 - recall: 0.3557 - auc: 0.7426 - val_loss: 1.4678 - val_tp: 111.0000 - val_fp: 210.0000 - val_tn: 782.0000 - val_fn: 385.0000 - val_accuracy: 0.3387 - val_precision: 0.3458 - val_recall: 0.2238 - val_auc: 0.4993\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.20685\n",
      "Epoch 86/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9369 - tp: 1562.0000 - fp: 850.0000 - tn: 8078.0000 - fn: 2902.0000 - accuracy: 0.5571 - precision: 0.6476 - recall: 0.3499 - auc: 0.7410 - val_loss: 1.6442 - val_tp: 156.0000 - val_fp: 310.0000 - val_tn: 682.0000 - val_fn: 340.0000 - val_accuracy: 0.3327 - val_precision: 0.3348 - val_recall: 0.3145 - val_auc: 0.5291\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.20685\n",
      "Epoch 87/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9352 - tp: 1543.0000 - fp: 826.0000 - tn: 8102.0000 - fn: 2921.0000 - accuracy: 0.5567 - precision: 0.6513 - recall: 0.3457 - auc: 0.7413 - val_loss: 1.4310 - val_tp: 128.0000 - val_fp: 192.0000 - val_tn: 800.0000 - val_fn: 368.0000 - val_accuracy: 0.3770 - val_precision: 0.4000 - val_recall: 0.2581 - val_auc: 0.5579\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.20685\n",
      "Epoch 88/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9241 - tp: 1607.0000 - fp: 809.0000 - tn: 8119.0000 - fn: 2857.0000 - accuracy: 0.5710 - precision: 0.6651 - recall: 0.3600 - auc: 0.7495 - val_loss: 1.5702 - val_tp: 177.0000 - val_fp: 220.0000 - val_tn: 772.0000 - val_fn: 319.0000 - val_accuracy: 0.4294 - val_precision: 0.4458 - val_recall: 0.3569 - val_auc: 0.5749\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.20685\n",
      "Epoch 89/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9262 - tp: 1621.0000 - fp: 854.0000 - tn: 8074.0000 - fn: 2843.0000 - accuracy: 0.5647 - precision: 0.6549 - recall: 0.3631 - auc: 0.7487 - val_loss: 1.8035 - val_tp: 183.0000 - val_fp: 295.0000 - val_tn: 697.0000 - val_fn: 313.0000 - val_accuracy: 0.3790 - val_precision: 0.3828 - val_recall: 0.3690 - val_auc: 0.5669\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.20685\n",
      "Epoch 90/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9285 - tp: 1587.0000 - fp: 851.0000 - tn: 8077.0000 - fn: 2877.0000 - accuracy: 0.5621 - precision: 0.6509 - recall: 0.3555 - auc: 0.7483 - val_loss: 2.2523 - val_tp: 155.0000 - val_fp: 334.0000 - val_tn: 658.0000 - val_fn: 341.0000 - val_accuracy: 0.3165 - val_precision: 0.3170 - val_recall: 0.3125 - val_auc: 0.5390\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.20685\n",
      "Epoch 91/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9335 - tp: 1576.0000 - fp: 819.0000 - tn: 8109.0000 - fn: 2888.0000 - accuracy: 0.5627 - precision: 0.6580 - recall: 0.3530 - auc: 0.7444 - val_loss: 1.3098 - val_tp: 145.0000 - val_fp: 171.0000 - val_tn: 821.0000 - val_fn: 351.0000 - val_accuracy: 0.4214 - val_precision: 0.4589 - val_recall: 0.2923 - val_auc: 0.5864\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.20685\n",
      "Epoch 92/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9262 - tp: 1639.0000 - fp: 879.0000 - tn: 8049.0000 - fn: 2825.0000 - accuracy: 0.5656 - precision: 0.6509 - recall: 0.3672 - auc: 0.7477 - val_loss: 1.2885 - val_tp: 185.0000 - val_fp: 218.0000 - val_tn: 774.0000 - val_fn: 311.0000 - val_accuracy: 0.4435 - val_precision: 0.4591 - val_recall: 0.3730 - val_auc: 0.6081\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.20685\n",
      "Epoch 93/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9349 - tp: 1535.0000 - fp: 849.0000 - tn: 8079.0000 - fn: 2929.0000 - accuracy: 0.5585 - precision: 0.6439 - recall: 0.3439 - auc: 0.7426 - val_loss: 1.5771 - val_tp: 166.0000 - val_fp: 239.0000 - val_tn: 753.0000 - val_fn: 330.0000 - val_accuracy: 0.4113 - val_precision: 0.4099 - val_recall: 0.3347 - val_auc: 0.5519\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.20685\n",
      "Epoch 94/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9293 - tp: 1668.0000 - fp: 894.0000 - tn: 8034.0000 - fn: 2796.0000 - accuracy: 0.5677 - precision: 0.6511 - recall: 0.3737 - auc: 0.7473 - val_loss: 2.0557 - val_tp: 236.0000 - val_fp: 253.0000 - val_tn: 739.0000 - val_fn: 260.0000 - val_accuracy: 0.4859 - val_precision: 0.4826 - val_recall: 0.4758 - val_auc: 0.6181\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.20685\n",
      "Epoch 95/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9381 - tp: 1583.0000 - fp: 875.0000 - tn: 8053.0000 - fn: 2881.0000 - accuracy: 0.5542 - precision: 0.6440 - recall: 0.3546 - auc: 0.7388 - val_loss: 1.3966 - val_tp: 182.0000 - val_fp: 210.0000 - val_tn: 782.0000 - val_fn: 314.0000 - val_accuracy: 0.4315 - val_precision: 0.4643 - val_recall: 0.3669 - val_auc: 0.6030\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.20685\n",
      "Epoch 96/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9301 - tp: 1621.0000 - fp: 856.0000 - tn: 8072.0000 - fn: 2843.0000 - accuracy: 0.5600 - precision: 0.6544 - recall: 0.3631 - auc: 0.7464 - val_loss: 1.5849 - val_tp: 151.0000 - val_fp: 238.0000 - val_tn: 754.0000 - val_fn: 345.0000 - val_accuracy: 0.3952 - val_precision: 0.3882 - val_recall: 0.3044 - val_auc: 0.5484\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.20685\n",
      "Epoch 97/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9262 - tp: 1630.0000 - fp: 861.0000 - tn: 8067.0000 - fn: 2834.0000 - accuracy: 0.5607 - precision: 0.6544 - recall: 0.3651 - auc: 0.7480 - val_loss: 1.9074 - val_tp: 213.0000 - val_fp: 250.0000 - val_tn: 742.0000 - val_fn: 283.0000 - val_accuracy: 0.4516 - val_precision: 0.4600 - val_recall: 0.4294 - val_auc: 0.6105\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.20685\n",
      "Epoch 98/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9212 - tp: 1657.0000 - fp: 832.0000 - tn: 8096.0000 - fn: 2807.0000 - accuracy: 0.5768 - precision: 0.6657 - recall: 0.3712 - auc: 0.7538 - val_loss: 2.0878 - val_tp: 160.0000 - val_fp: 325.0000 - val_tn: 667.0000 - val_fn: 336.0000 - val_accuracy: 0.3266 - val_precision: 0.3299 - val_recall: 0.3226 - val_auc: 0.5217\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.20685\n",
      "Epoch 99/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9285 - tp: 1601.0000 - fp: 819.0000 - tn: 8109.0000 - fn: 2863.0000 - accuracy: 0.5719 - precision: 0.6616 - recall: 0.3586 - auc: 0.7498 - val_loss: 1.4413 - val_tp: 175.0000 - val_fp: 238.0000 - val_tn: 754.0000 - val_fn: 321.0000 - val_accuracy: 0.4133 - val_precision: 0.4237 - val_recall: 0.3528 - val_auc: 0.5633\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.20685\n",
      "Epoch 100/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9333 - tp: 1568.0000 - fp: 824.0000 - tn: 8104.0000 - fn: 2896.0000 - accuracy: 0.5591 - precision: 0.6555 - recall: 0.3513 - auc: 0.7433 - val_loss: 1.3716 - val_tp: 133.0000 - val_fp: 176.0000 - val_tn: 816.0000 - val_fn: 363.0000 - val_accuracy: 0.4415 - val_precision: 0.4304 - val_recall: 0.2681 - val_auc: 0.5784\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.20685\n",
      "Epoch 101/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9404 - tp: 1554.0000 - fp: 866.0000 - tn: 8062.0000 - fn: 2910.0000 - accuracy: 0.5526 - precision: 0.6421 - recall: 0.3481 - auc: 0.7373 - val_loss: 1.5806 - val_tp: 179.0000 - val_fp: 241.0000 - val_tn: 751.0000 - val_fn: 317.0000 - val_accuracy: 0.4052 - val_precision: 0.4262 - val_recall: 0.3609 - val_auc: 0.5486\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.20685\n",
      "Epoch 102/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9284 - tp: 1582.0000 - fp: 861.0000 - tn: 8067.0000 - fn: 2882.0000 - accuracy: 0.5634 - precision: 0.6476 - recall: 0.3544 - auc: 0.7467 - val_loss: 1.8738 - val_tp: 184.0000 - val_fp: 290.0000 - val_tn: 702.0000 - val_fn: 312.0000 - val_accuracy: 0.3891 - val_precision: 0.3882 - val_recall: 0.3710 - val_auc: 0.5673\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.20685\n",
      "Epoch 103/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9309 - tp: 1602.0000 - fp: 864.0000 - tn: 8064.0000 - fn: 2862.0000 - accuracy: 0.5672 - precision: 0.6496 - recall: 0.3589 - auc: 0.7446 - val_loss: 1.6400 - val_tp: 177.0000 - val_fp: 290.0000 - val_tn: 702.0000 - val_fn: 319.0000 - val_accuracy: 0.3770 - val_precision: 0.3790 - val_recall: 0.3569 - val_auc: 0.5735\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.20685\n",
      "Epoch 104/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9322 - tp: 1600.0000 - fp: 875.0000 - tn: 8053.0000 - fn: 2864.0000 - accuracy: 0.5540 - precision: 0.6465 - recall: 0.3584 - auc: 0.7437 - val_loss: 1.6336 - val_tp: 201.0000 - val_fp: 271.0000 - val_tn: 721.0000 - val_fn: 295.0000 - val_accuracy: 0.4153 - val_precision: 0.4258 - val_recall: 0.4052 - val_auc: 0.5879\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.20685\n",
      "Epoch 105/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9326 - tp: 1588.0000 - fp: 822.0000 - tn: 8106.0000 - fn: 2876.0000 - accuracy: 0.5587 - precision: 0.6589 - recall: 0.3557 - auc: 0.7417 - val_loss: 1.7147 - val_tp: 211.0000 - val_fp: 246.0000 - val_tn: 746.0000 - val_fn: 285.0000 - val_accuracy: 0.4395 - val_precision: 0.4617 - val_recall: 0.4254 - val_auc: 0.6117\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.20685\n",
      "Epoch 106/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9305 - tp: 1526.0000 - fp: 834.0000 - tn: 8094.0000 - fn: 2938.0000 - accuracy: 0.5497 - precision: 0.6466 - recall: 0.3418 - auc: 0.7437 - val_loss: 1.7570 - val_tp: 204.0000 - val_fp: 278.0000 - val_tn: 714.0000 - val_fn: 292.0000 - val_accuracy: 0.4254 - val_precision: 0.4232 - val_recall: 0.4113 - val_auc: 0.5948\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.20685\n",
      "Epoch 107/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.9491 - tp: 1505.0000 - fp: 900.0000 - tn: 8028.0000 - fn: 2959.0000 - accuracy: 0.5435 - precision: 0.6258 - recall: 0.3371 - auc: 0.7297 - val_loss: 1.3381 - val_tp: 116.0000 - val_fp: 189.0000 - val_tn: 803.0000 - val_fn: 380.0000 - val_accuracy: 0.3690 - val_precision: 0.3803 - val_recall: 0.2339 - val_auc: 0.5412\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.20685\n",
      "Epoch 108/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9349 - tp: 1528.0000 - fp: 822.0000 - tn: 8106.0000 - fn: 2936.0000 - accuracy: 0.5569 - precision: 0.6502 - recall: 0.3423 - auc: 0.7403 - val_loss: 1.2656 - val_tp: 135.0000 - val_fp: 151.0000 - val_tn: 841.0000 - val_fn: 361.0000 - val_accuracy: 0.4294 - val_precision: 0.4720 - val_recall: 0.2722 - val_auc: 0.5844\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.20685\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9189 - tp: 1663.0000 - fp: 923.0000 - tn: 8005.0000 - fn: 2801.0000 - accuracy: 0.5694 - precision: 0.6431 - recall: 0.3725 - auc: 0.7522 - val_loss: 1.5869 - val_tp: 173.0000 - val_fp: 284.0000 - val_tn: 708.0000 - val_fn: 323.0000 - val_accuracy: 0.3690 - val_precision: 0.3786 - val_recall: 0.3488 - val_auc: 0.5649\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.20685\n",
      "Epoch 110/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9307 - tp: 1626.0000 - fp: 888.0000 - tn: 8040.0000 - fn: 2838.0000 - accuracy: 0.5553 - precision: 0.6468 - recall: 0.3642 - auc: 0.7463 - val_loss: 1.2511 - val_tp: 100.0000 - val_fp: 129.0000 - val_tn: 863.0000 - val_fn: 396.0000 - val_accuracy: 0.3992 - val_precision: 0.4367 - val_recall: 0.2016 - val_auc: 0.5659\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.20685\n",
      "Epoch 111/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.9342 - tp: 1580.0000 - fp: 885.0000 - tn: 8043.0000 - fn: 2884.0000 - accuracy: 0.5542 - precision: 0.6410 - recall: 0.3539 - auc: 0.7421 - val_loss: 1.3901 - val_tp: 142.0000 - val_fp: 253.0000 - val_tn: 739.0000 - val_fn: 354.0000 - val_accuracy: 0.3629 - val_precision: 0.3595 - val_recall: 0.2863 - val_auc: 0.5311\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.20685\n",
      "Epoch 112/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9199 - tp: 1605.0000 - fp: 837.0000 - tn: 8091.0000 - fn: 2859.0000 - accuracy: 0.5647 - precision: 0.6572 - recall: 0.3595 - auc: 0.7524 - val_loss: 1.5594 - val_tp: 195.0000 - val_fp: 233.0000 - val_tn: 759.0000 - val_fn: 301.0000 - val_accuracy: 0.4415 - val_precision: 0.4556 - val_recall: 0.3931 - val_auc: 0.6077\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.20685\n",
      "Epoch 113/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9271 - tp: 1595.0000 - fp: 858.0000 - tn: 8070.0000 - fn: 2869.0000 - accuracy: 0.5641 - precision: 0.6502 - recall: 0.3573 - auc: 0.7471 - val_loss: 2.1685 - val_tp: 159.0000 - val_fp: 331.0000 - val_tn: 661.0000 - val_fn: 337.0000 - val_accuracy: 0.3206 - val_precision: 0.3245 - val_recall: 0.3206 - val_auc: 0.5374\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.20685\n",
      "Epoch 114/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9304 - tp: 1588.0000 - fp: 856.0000 - tn: 8072.0000 - fn: 2876.0000 - accuracy: 0.5591 - precision: 0.6498 - recall: 0.3557 - auc: 0.7439 - val_loss: 1.6567 - val_tp: 182.0000 - val_fp: 293.0000 - val_tn: 699.0000 - val_fn: 314.0000 - val_accuracy: 0.3810 - val_precision: 0.3832 - val_recall: 0.3669 - val_auc: 0.5779\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.20685\n",
      "Epoch 115/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9125 - tp: 1682.0000 - fp: 875.0000 - tn: 8053.0000 - fn: 2782.0000 - accuracy: 0.5706 - precision: 0.6578 - recall: 0.3768 - auc: 0.7563 - val_loss: 1.4912 - val_tp: 148.0000 - val_fp: 219.0000 - val_tn: 773.0000 - val_fn: 348.0000 - val_accuracy: 0.4032 - val_precision: 0.4033 - val_recall: 0.2984 - val_auc: 0.5580\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.20685\n",
      "Epoch 116/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9103 - tp: 1698.0000 - fp: 864.0000 - tn: 8064.0000 - fn: 2766.0000 - accuracy: 0.5677 - precision: 0.6628 - recall: 0.3804 - auc: 0.7579 - val_loss: 1.7654 - val_tp: 182.0000 - val_fp: 292.0000 - val_tn: 700.0000 - val_fn: 314.0000 - val_accuracy: 0.3770 - val_precision: 0.3840 - val_recall: 0.3669 - val_auc: 0.5568\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.20685\n",
      "Epoch 117/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9201 - tp: 1670.0000 - fp: 898.0000 - tn: 8030.0000 - fn: 2794.0000 - accuracy: 0.5663 - precision: 0.6503 - recall: 0.3741 - auc: 0.7527 - val_loss: 1.4140 - val_tp: 176.0000 - val_fp: 211.0000 - val_tn: 781.0000 - val_fn: 320.0000 - val_accuracy: 0.4234 - val_precision: 0.4548 - val_recall: 0.3548 - val_auc: 0.5715\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.20685\n",
      "Epoch 118/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9177 - tp: 1686.0000 - fp: 843.0000 - tn: 8085.0000 - fn: 2778.0000 - accuracy: 0.5694 - precision: 0.6667 - recall: 0.3777 - auc: 0.7534 - val_loss: 1.3339 - val_tp: 124.0000 - val_fp: 159.0000 - val_tn: 833.0000 - val_fn: 372.0000 - val_accuracy: 0.4274 - val_precision: 0.4382 - val_recall: 0.2500 - val_auc: 0.5717\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.20685\n",
      "Epoch 119/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9196 - tp: 1655.0000 - fp: 874.0000 - tn: 8054.0000 - fn: 2809.0000 - accuracy: 0.5677 - precision: 0.6544 - recall: 0.3707 - auc: 0.7528 - val_loss: 1.3250 - val_tp: 176.0000 - val_fp: 191.0000 - val_tn: 801.0000 - val_fn: 320.0000 - val_accuracy: 0.4657 - val_precision: 0.4796 - val_recall: 0.3548 - val_auc: 0.6091\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.20685\n",
      "Epoch 120/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9292 - tp: 1647.0000 - fp: 846.0000 - tn: 8082.0000 - fn: 2817.0000 - accuracy: 0.5712 - precision: 0.6606 - recall: 0.3690 - auc: 0.7504 - val_loss: 1.7140 - val_tp: 182.0000 - val_fp: 295.0000 - val_tn: 697.0000 - val_fn: 314.0000 - val_accuracy: 0.3810 - val_precision: 0.3816 - val_recall: 0.3669 - val_auc: 0.5759\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.20685\n",
      "Epoch 121/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9427 - tp: 1535.0000 - fp: 885.0000 - tn: 8043.0000 - fn: 2929.0000 - accuracy: 0.5506 - precision: 0.6343 - recall: 0.3439 - auc: 0.7354 - val_loss: 1.4802 - val_tp: 191.0000 - val_fp: 221.0000 - val_tn: 771.0000 - val_fn: 305.0000 - val_accuracy: 0.4456 - val_precision: 0.4636 - val_recall: 0.3851 - val_auc: 0.6239\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.20685\n",
      "Epoch 122/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9261 - tp: 1626.0000 - fp: 857.0000 - tn: 8071.0000 - fn: 2838.0000 - accuracy: 0.5647 - precision: 0.6549 - recall: 0.3642 - auc: 0.7484 - val_loss: 1.6412 - val_tp: 189.0000 - val_fp: 243.0000 - val_tn: 749.0000 - val_fn: 307.0000 - val_accuracy: 0.4214 - val_precision: 0.4375 - val_recall: 0.3810 - val_auc: 0.5810\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.20685\n",
      "Epoch 123/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9253 - tp: 1630.0000 - fp: 861.0000 - tn: 8067.0000 - fn: 2834.0000 - accuracy: 0.5582 - precision: 0.6544 - recall: 0.3651 - auc: 0.7481 - val_loss: 1.3751 - val_tp: 164.0000 - val_fp: 241.0000 - val_tn: 751.0000 - val_fn: 332.0000 - val_accuracy: 0.4052 - val_precision: 0.4049 - val_recall: 0.3306 - val_auc: 0.5570\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.20685\n",
      "Epoch 124/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9273 - tp: 1610.0000 - fp: 901.0000 - tn: 8027.0000 - fn: 2854.0000 - accuracy: 0.5636 - precision: 0.6412 - recall: 0.3607 - auc: 0.7459 - val_loss: 1.5351 - val_tp: 195.0000 - val_fp: 264.0000 - val_tn: 728.0000 - val_fn: 301.0000 - val_accuracy: 0.4113 - val_precision: 0.4248 - val_recall: 0.3931 - val_auc: 0.5875\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.20685\n",
      "Epoch 125/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9227 - tp: 1691.0000 - fp: 853.0000 - tn: 8075.0000 - fn: 2773.0000 - accuracy: 0.5708 - precision: 0.6647 - recall: 0.3788 - auc: 0.7524 - val_loss: 1.5316 - val_tp: 179.0000 - val_fp: 235.0000 - val_tn: 757.0000 - val_fn: 317.0000 - val_accuracy: 0.4294 - val_precision: 0.4324 - val_recall: 0.3609 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.20685\n",
      "Epoch 126/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9181 - tp: 1627.0000 - fp: 874.0000 - tn: 8054.0000 - fn: 2837.0000 - accuracy: 0.5674 - precision: 0.6505 - recall: 0.3645 - auc: 0.7535 - val_loss: 1.4901 - val_tp: 189.0000 - val_fp: 225.0000 - val_tn: 767.0000 - val_fn: 307.0000 - val_accuracy: 0.4274 - val_precision: 0.4565 - val_recall: 0.3810 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.20685\n",
      "Epoch 127/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9251 - tp: 1643.0000 - fp: 886.0000 - tn: 8042.0000 - fn: 2821.0000 - accuracy: 0.5540 - precision: 0.6497 - recall: 0.3681 - auc: 0.7470 - val_loss: 1.3454 - val_tp: 189.0000 - val_fp: 217.0000 - val_tn: 775.0000 - val_fn: 307.0000 - val_accuracy: 0.4738 - val_precision: 0.4655 - val_recall: 0.3810 - val_auc: 0.6185\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.20685\n",
      "Epoch 128/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9211 - tp: 1643.0000 - fp: 890.0000 - tn: 8038.0000 - fn: 2821.0000 - accuracy: 0.5672 - precision: 0.6486 - recall: 0.3681 - auc: 0.7508 - val_loss: 2.2723 - val_tp: 154.0000 - val_fp: 332.0000 - val_tn: 660.0000 - val_fn: 342.0000 - val_accuracy: 0.3165 - val_precision: 0.3169 - val_recall: 0.3105 - val_auc: 0.5314\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.20685\n",
      "Epoch 129/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9145 - tp: 1679.0000 - fp: 844.0000 - tn: 8084.0000 - fn: 2785.0000 - accuracy: 0.5647 - precision: 0.6655 - recall: 0.3761 - auc: 0.7549 - val_loss: 1.4706 - val_tp: 198.0000 - val_fp: 231.0000 - val_tn: 761.0000 - val_fn: 298.0000 - val_accuracy: 0.4637 - val_precision: 0.4615 - val_recall: 0.3992 - val_auc: 0.6096\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.20685\n",
      "Epoch 130/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9165 - tp: 1634.0000 - fp: 850.0000 - tn: 8078.0000 - fn: 2830.0000 - accuracy: 0.5715 - precision: 0.6578 - recall: 0.3660 - auc: 0.7544 - val_loss: 1.4884 - val_tp: 192.0000 - val_fp: 274.0000 - val_tn: 718.0000 - val_fn: 304.0000 - val_accuracy: 0.4032 - val_precision: 0.4120 - val_recall: 0.3871 - val_auc: 0.5854\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.20685\n",
      "Epoch 131/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9222 - tp: 1686.0000 - fp: 855.0000 - tn: 8073.0000 - fn: 2778.0000 - accuracy: 0.5730 - precision: 0.6635 - recall: 0.3777 - auc: 0.7520 - val_loss: 1.6804 - val_tp: 202.0000 - val_fp: 270.0000 - val_tn: 722.0000 - val_fn: 294.0000 - val_accuracy: 0.4274 - val_precision: 0.4280 - val_recall: 0.4073 - val_auc: 0.5937\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.20685\n",
      "Epoch 132/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9256 - tp: 1628.0000 - fp: 902.0000 - tn: 8026.0000 - fn: 2836.0000 - accuracy: 0.5571 - precision: 0.6435 - recall: 0.3647 - auc: 0.7478 - val_loss: 1.4723 - val_tp: 195.0000 - val_fp: 229.0000 - val_tn: 763.0000 - val_fn: 301.0000 - val_accuracy: 0.4556 - val_precision: 0.4599 - val_recall: 0.3931 - val_auc: 0.6103\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.20685\n",
      "Epoch 133/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9169 - tp: 1677.0000 - fp: 877.0000 - tn: 8051.0000 - fn: 2787.0000 - accuracy: 0.5652 - precision: 0.6566 - recall: 0.3757 - auc: 0.7539 - val_loss: 1.5453 - val_tp: 200.0000 - val_fp: 234.0000 - val_tn: 758.0000 - val_fn: 296.0000 - val_accuracy: 0.4577 - val_precision: 0.4608 - val_recall: 0.4032 - val_auc: 0.6030\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.20685\n",
      "Epoch 134/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9221 - tp: 1746.0000 - fp: 914.0000 - tn: 8014.0000 - fn: 2718.0000 - accuracy: 0.5753 - precision: 0.6564 - recall: 0.3911 - auc: 0.7520 - val_loss: 1.8080 - val_tp: 166.0000 - val_fp: 315.0000 - val_tn: 677.0000 - val_fn: 330.0000 - val_accuracy: 0.3488 - val_precision: 0.3451 - val_recall: 0.3347 - val_auc: 0.5545\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.20685\n",
      "Epoch 135/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9090 - tp: 1688.0000 - fp: 838.0000 - tn: 8090.0000 - fn: 2776.0000 - accuracy: 0.5797 - precision: 0.6683 - recall: 0.3781 - auc: 0.7600 - val_loss: 1.3947 - val_tp: 159.0000 - val_fp: 224.0000 - val_tn: 768.0000 - val_fn: 337.0000 - val_accuracy: 0.4012 - val_precision: 0.4151 - val_recall: 0.3206 - val_auc: 0.5567\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.20685\n",
      "Epoch 136/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9178 - tp: 1684.0000 - fp: 862.0000 - tn: 8066.0000 - fn: 2780.0000 - accuracy: 0.5728 - precision: 0.6614 - recall: 0.3772 - auc: 0.7554 - val_loss: 1.4534 - val_tp: 119.0000 - val_fp: 186.0000 - val_tn: 806.0000 - val_fn: 377.0000 - val_accuracy: 0.3851 - val_precision: 0.3902 - val_recall: 0.2399 - val_auc: 0.5376\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.20685\n",
      "Epoch 137/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9184 - tp: 1718.0000 - fp: 901.0000 - tn: 8027.0000 - fn: 2746.0000 - accuracy: 0.5724 - precision: 0.6560 - recall: 0.3849 - auc: 0.7536 - val_loss: 1.3769 - val_tp: 133.0000 - val_fp: 192.0000 - val_tn: 800.0000 - val_fn: 363.0000 - val_accuracy: 0.3911 - val_precision: 0.4092 - val_recall: 0.2681 - val_auc: 0.5592\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.20685\n",
      "Epoch 138/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.9089 - tp: 1737.0000 - fp: 830.0000 - tn: 8098.0000 - fn: 2727.0000 - accuracy: 0.5706 - precision: 0.6767 - recall: 0.3891 - auc: 0.7608 - val_loss: 1.4474 - val_tp: 191.0000 - val_fp: 244.0000 - val_tn: 748.0000 - val_fn: 305.0000 - val_accuracy: 0.4133 - val_precision: 0.4391 - val_recall: 0.3851 - val_auc: 0.5950\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.20685\n",
      "Epoch 139/500\n",
      "4464/4464 [==============================] - 0s 76us/step - loss: 0.9085 - tp: 1702.0000 - fp: 872.0000 - tn: 8056.0000 - fn: 2762.0000 - accuracy: 0.5728 - precision: 0.6612 - recall: 0.3813 - auc: 0.7603 - val_loss: 1.5406 - val_tp: 186.0000 - val_fp: 223.0000 - val_tn: 769.0000 - val_fn: 310.0000 - val_accuracy: 0.4476 - val_precision: 0.4548 - val_recall: 0.3750 - val_auc: 0.6053\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.20685\n",
      "Epoch 140/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9181 - tp: 1719.0000 - fp: 881.0000 - tn: 8047.0000 - fn: 2745.0000 - accuracy: 0.5685 - precision: 0.6612 - recall: 0.3851 - auc: 0.7526 - val_loss: 2.2236 - val_tp: 209.0000 - val_fp: 266.0000 - val_tn: 726.0000 - val_fn: 287.0000 - val_accuracy: 0.4456 - val_precision: 0.4400 - val_recall: 0.4214 - val_auc: 0.6031\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.20685\n",
      "Epoch 141/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9161 - tp: 1711.0000 - fp: 860.0000 - tn: 8068.0000 - fn: 2753.0000 - accuracy: 0.5762 - precision: 0.6655 - recall: 0.3833 - auc: 0.7546 - val_loss: 1.5218 - val_tp: 189.0000 - val_fp: 246.0000 - val_tn: 746.0000 - val_fn: 307.0000 - val_accuracy: 0.4234 - val_precision: 0.4345 - val_recall: 0.3810 - val_auc: 0.5697\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.20685\n",
      "Epoch 142/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9044 - tp: 1764.0000 - fp: 850.0000 - tn: 8078.0000 - fn: 2700.0000 - accuracy: 0.5833 - precision: 0.6748 - recall: 0.3952 - auc: 0.7628 - val_loss: 1.5984 - val_tp: 162.0000 - val_fp: 238.0000 - val_tn: 754.0000 - val_fn: 334.0000 - val_accuracy: 0.4052 - val_precision: 0.4050 - val_recall: 0.3266 - val_auc: 0.5754\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.20685\n",
      "Epoch 143/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9161 - tp: 1701.0000 - fp: 870.0000 - tn: 8058.0000 - fn: 2763.0000 - accuracy: 0.5730 - precision: 0.6616 - recall: 0.3810 - auc: 0.7551 - val_loss: 2.1396 - val_tp: 230.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 266.0000 - val_accuracy: 0.4778 - val_precision: 0.4772 - val_recall: 0.4637 - val_auc: 0.6146\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.20685\n",
      "Epoch 144/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.9182 - tp: 1719.0000 - fp: 832.0000 - tn: 8096.0000 - fn: 2745.0000 - accuracy: 0.5786 - precision: 0.6739 - recall: 0.3851 - auc: 0.7574 - val_loss: 1.6839 - val_tp: 190.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 306.0000 - val_accuracy: 0.4133 - val_precision: 0.4222 - val_recall: 0.3831 - val_auc: 0.5896\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.20685\n",
      "Epoch 145/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.9281 - tp: 1590.0000 - fp: 945.0000 - tn: 7983.0000 - fn: 2874.0000 - accuracy: 0.5576 - precision: 0.6272 - recall: 0.3562 - auc: 0.7444 - val_loss: 1.7350 - val_tp: 190.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 306.0000 - val_accuracy: 0.4214 - val_precision: 0.4299 - val_recall: 0.3831 - val_auc: 0.5923\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.20685\n",
      "Epoch 146/500\n",
      "4464/4464 [==============================] - 0s 74us/step - loss: 0.9039 - tp: 1709.0000 - fp: 868.0000 - tn: 8060.0000 - fn: 2755.0000 - accuracy: 0.5681 - precision: 0.6632 - recall: 0.3828 - auc: 0.7615 - val_loss: 1.9434 - val_tp: 174.0000 - val_fp: 297.0000 - val_tn: 695.0000 - val_fn: 322.0000 - val_accuracy: 0.3649 - val_precision: 0.3694 - val_recall: 0.3508 - val_auc: 0.5551\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.20685\n",
      "Epoch 147/500\n",
      "4464/4464 [==============================] - 0s 80us/step - loss: 0.9131 - tp: 1736.0000 - fp: 865.0000 - tn: 8063.0000 - fn: 2728.0000 - accuracy: 0.5737 - precision: 0.6674 - recall: 0.3889 - auc: 0.7572 - val_loss: 1.4899 - val_tp: 180.0000 - val_fp: 228.0000 - val_tn: 764.0000 - val_fn: 316.0000 - val_accuracy: 0.4375 - val_precision: 0.4412 - val_recall: 0.3629 - val_auc: 0.5769\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.20685\n",
      "Epoch 148/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9075 - tp: 1737.0000 - fp: 888.0000 - tn: 8040.0000 - fn: 2727.0000 - accuracy: 0.5804 - precision: 0.6617 - recall: 0.3891 - auc: 0.7619 - val_loss: 1.5918 - val_tp: 201.0000 - val_fp: 236.0000 - val_tn: 756.0000 - val_fn: 295.0000 - val_accuracy: 0.4637 - val_precision: 0.4600 - val_recall: 0.4052 - val_auc: 0.6060\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.20685\n",
      "Epoch 149/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9160 - tp: 1707.0000 - fp: 846.0000 - tn: 8082.0000 - fn: 2757.0000 - accuracy: 0.5701 - precision: 0.6686 - recall: 0.3824 - auc: 0.7549 - val_loss: 1.9922 - val_tp: 192.0000 - val_fp: 262.0000 - val_tn: 730.0000 - val_fn: 304.0000 - val_accuracy: 0.4173 - val_precision: 0.4229 - val_recall: 0.3871 - val_auc: 0.5893\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.20685\n",
      "Epoch 150/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9076 - tp: 1745.0000 - fp: 929.0000 - tn: 7999.0000 - fn: 2719.0000 - accuracy: 0.5746 - precision: 0.6526 - recall: 0.3909 - auc: 0.7592 - val_loss: 1.4440 - val_tp: 184.0000 - val_fp: 222.0000 - val_tn: 770.0000 - val_fn: 312.0000 - val_accuracy: 0.4234 - val_precision: 0.4532 - val_recall: 0.3710 - val_auc: 0.6014\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.20685\n",
      "Epoch 151/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9191 - tp: 1685.0000 - fp: 894.0000 - tn: 8034.0000 - fn: 2779.0000 - accuracy: 0.5683 - precision: 0.6534 - recall: 0.3775 - auc: 0.7531 - val_loss: 1.5377 - val_tp: 202.0000 - val_fp: 240.0000 - val_tn: 752.0000 - val_fn: 294.0000 - val_accuracy: 0.4435 - val_precision: 0.4570 - val_recall: 0.4073 - val_auc: 0.6036\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.20685\n",
      "Epoch 152/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9001 - tp: 1748.0000 - fp: 884.0000 - tn: 8044.0000 - fn: 2716.0000 - accuracy: 0.5726 - precision: 0.6641 - recall: 0.3916 - auc: 0.7645 - val_loss: 1.6125 - val_tp: 194.0000 - val_fp: 268.0000 - val_tn: 724.0000 - val_fn: 302.0000 - val_accuracy: 0.4113 - val_precision: 0.4199 - val_recall: 0.3911 - val_auc: 0.5869\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.20685\n",
      "Epoch 153/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9099 - tp: 1673.0000 - fp: 880.0000 - tn: 8048.0000 - fn: 2791.0000 - accuracy: 0.5759 - precision: 0.6553 - recall: 0.3748 - auc: 0.7584 - val_loss: 1.5454 - val_tp: 197.0000 - val_fp: 250.0000 - val_tn: 742.0000 - val_fn: 299.0000 - val_accuracy: 0.4234 - val_precision: 0.4407 - val_recall: 0.3972 - val_auc: 0.5847\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.20685\n",
      "Epoch 154/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9023 - tp: 1776.0000 - fp: 867.0000 - tn: 8061.0000 - fn: 2688.0000 - accuracy: 0.5833 - precision: 0.6720 - recall: 0.3978 - auc: 0.7658 - val_loss: 1.9545 - val_tp: 208.0000 - val_fp: 269.0000 - val_tn: 723.0000 - val_fn: 288.0000 - val_accuracy: 0.4294 - val_precision: 0.4361 - val_recall: 0.4194 - val_auc: 0.6008\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.20685\n",
      "Epoch 155/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9140 - tp: 1763.0000 - fp: 922.0000 - tn: 8006.0000 - fn: 2701.0000 - accuracy: 0.5712 - precision: 0.6566 - recall: 0.3949 - auc: 0.7560 - val_loss: 1.6209 - val_tp: 188.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 308.0000 - val_accuracy: 0.4234 - val_precision: 0.4273 - val_recall: 0.3790 - val_auc: 0.5663\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.20685\n",
      "Epoch 156/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.9064 - tp: 1783.0000 - fp: 898.0000 - tn: 8030.0000 - fn: 2681.0000 - accuracy: 0.5782 - precision: 0.6651 - recall: 0.3994 - auc: 0.7617 - val_loss: 1.4646 - val_tp: 204.0000 - val_fp: 243.0000 - val_tn: 749.0000 - val_fn: 292.0000 - val_accuracy: 0.4415 - val_precision: 0.4564 - val_recall: 0.4113 - val_auc: 0.6066\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.20685\n",
      "Epoch 157/500\n",
      "4464/4464 [==============================] - 0s 79us/step - loss: 0.9040 - tp: 1756.0000 - fp: 875.0000 - tn: 8053.0000 - fn: 2708.0000 - accuracy: 0.5811 - precision: 0.6674 - recall: 0.3934 - auc: 0.7626 - val_loss: 1.6367 - val_tp: 192.0000 - val_fp: 272.0000 - val_tn: 720.0000 - val_fn: 304.0000 - val_accuracy: 0.4052 - val_precision: 0.4138 - val_recall: 0.3871 - val_auc: 0.5857\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.20685\n",
      "Epoch 158/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.9157 - tp: 1668.0000 - fp: 878.0000 - tn: 8050.0000 - fn: 2796.0000 - accuracy: 0.5717 - precision: 0.6551 - recall: 0.3737 - auc: 0.7553 - val_loss: 1.6305 - val_tp: 191.0000 - val_fp: 268.0000 - val_tn: 724.0000 - val_fn: 305.0000 - val_accuracy: 0.4153 - val_precision: 0.4161 - val_recall: 0.3851 - val_auc: 0.5746\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.20685\n",
      "Epoch 159/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9111 - tp: 1745.0000 - fp: 934.0000 - tn: 7994.0000 - fn: 2719.0000 - accuracy: 0.5768 - precision: 0.6514 - recall: 0.3909 - auc: 0.7594 - val_loss: 1.5155 - val_tp: 175.0000 - val_fp: 243.0000 - val_tn: 749.0000 - val_fn: 321.0000 - val_accuracy: 0.4133 - val_precision: 0.4187 - val_recall: 0.3528 - val_auc: 0.5910\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.20685\n",
      "Epoch 160/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.9103 - tp: 1697.0000 - fp: 890.0000 - tn: 8038.0000 - fn: 2767.0000 - accuracy: 0.5802 - precision: 0.6560 - recall: 0.3802 - auc: 0.7597 - val_loss: 1.5264 - val_tp: 148.0000 - val_fp: 228.0000 - val_tn: 764.0000 - val_fn: 348.0000 - val_accuracy: 0.3770 - val_precision: 0.3936 - val_recall: 0.2984 - val_auc: 0.5509\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.20685\n",
      "Epoch 161/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9078 - tp: 1735.0000 - fp: 869.0000 - tn: 8059.0000 - fn: 2729.0000 - accuracy: 0.5800 - precision: 0.6663 - recall: 0.3887 - auc: 0.7617 - val_loss: 1.4278 - val_tp: 169.0000 - val_fp: 213.0000 - val_tn: 779.0000 - val_fn: 327.0000 - val_accuracy: 0.4254 - val_precision: 0.4424 - val_recall: 0.3407 - val_auc: 0.5915\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.20685\n",
      "Epoch 162/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8999 - tp: 1783.0000 - fp: 899.0000 - tn: 8029.0000 - fn: 2681.0000 - accuracy: 0.5836 - precision: 0.6648 - recall: 0.3994 - auc: 0.7647 - val_loss: 1.4332 - val_tp: 156.0000 - val_fp: 191.0000 - val_tn: 801.0000 - val_fn: 340.0000 - val_accuracy: 0.4375 - val_precision: 0.4496 - val_recall: 0.3145 - val_auc: 0.5843\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.20685\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9056 - tp: 1757.0000 - fp: 877.0000 - tn: 8051.0000 - fn: 2707.0000 - accuracy: 0.5759 - precision: 0.6670 - recall: 0.3936 - auc: 0.7607 - val_loss: 1.9092 - val_tp: 164.0000 - val_fp: 316.0000 - val_tn: 676.0000 - val_fn: 332.0000 - val_accuracy: 0.3407 - val_precision: 0.3417 - val_recall: 0.3306 - val_auc: 0.5331\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.20685\n",
      "Epoch 164/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9022 - tp: 1737.0000 - fp: 900.0000 - tn: 8028.0000 - fn: 2727.0000 - accuracy: 0.5764 - precision: 0.6587 - recall: 0.3891 - auc: 0.7643 - val_loss: 2.1441 - val_tp: 204.0000 - val_fp: 274.0000 - val_tn: 718.0000 - val_fn: 292.0000 - val_accuracy: 0.4274 - val_precision: 0.4268 - val_recall: 0.4113 - val_auc: 0.5906\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.20685\n",
      "Epoch 165/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8994 - tp: 1822.0000 - fp: 895.0000 - tn: 8033.0000 - fn: 2642.0000 - accuracy: 0.5773 - precision: 0.6706 - recall: 0.4082 - auc: 0.7668 - val_loss: 2.0262 - val_tp: 186.0000 - val_fp: 287.0000 - val_tn: 705.0000 - val_fn: 310.0000 - val_accuracy: 0.3871 - val_precision: 0.3932 - val_recall: 0.3750 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.20685\n",
      "Epoch 166/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9053 - tp: 1785.0000 - fp: 908.0000 - tn: 8020.0000 - fn: 2679.0000 - accuracy: 0.5768 - precision: 0.6628 - recall: 0.3999 - auc: 0.7629 - val_loss: 2.2578 - val_tp: 209.0000 - val_fp: 268.0000 - val_tn: 724.0000 - val_fn: 287.0000 - val_accuracy: 0.4335 - val_precision: 0.4382 - val_recall: 0.4214 - val_auc: 0.6099\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.20685\n",
      "Epoch 167/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9088 - tp: 1719.0000 - fp: 891.0000 - tn: 8037.0000 - fn: 2745.0000 - accuracy: 0.5793 - precision: 0.6586 - recall: 0.3851 - auc: 0.7619 - val_loss: 1.6178 - val_tp: 198.0000 - val_fp: 269.0000 - val_tn: 723.0000 - val_fn: 298.0000 - val_accuracy: 0.4234 - val_precision: 0.4240 - val_recall: 0.3992 - val_auc: 0.5801\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.20685\n",
      "Epoch 168/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9047 - tp: 1764.0000 - fp: 856.0000 - tn: 8072.0000 - fn: 2700.0000 - accuracy: 0.5688 - precision: 0.6733 - recall: 0.3952 - auc: 0.7614 - val_loss: 1.4133 - val_tp: 180.0000 - val_fp: 212.0000 - val_tn: 780.0000 - val_fn: 316.0000 - val_accuracy: 0.4375 - val_precision: 0.4592 - val_recall: 0.3629 - val_auc: 0.5921\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.20685\n",
      "Epoch 169/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9050 - tp: 1813.0000 - fp: 937.0000 - tn: 7991.0000 - fn: 2651.0000 - accuracy: 0.5811 - precision: 0.6593 - recall: 0.4061 - auc: 0.7631 - val_loss: 1.4355 - val_tp: 178.0000 - val_fp: 214.0000 - val_tn: 778.0000 - val_fn: 318.0000 - val_accuracy: 0.4315 - val_precision: 0.4541 - val_recall: 0.3589 - val_auc: 0.6032\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.20685\n",
      "Epoch 170/500\n",
      "4464/4464 [==============================] - 0s 65us/step - loss: 0.9001 - tp: 1779.0000 - fp: 880.0000 - tn: 8048.0000 - fn: 2685.0000 - accuracy: 0.5746 - precision: 0.6690 - recall: 0.3985 - auc: 0.7643 - val_loss: 1.7296 - val_tp: 203.0000 - val_fp: 269.0000 - val_tn: 723.0000 - val_fn: 293.0000 - val_accuracy: 0.4315 - val_precision: 0.4301 - val_recall: 0.4093 - val_auc: 0.5882\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.20685\n",
      "Epoch 171/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8911 - tp: 1840.0000 - fp: 887.0000 - tn: 8041.0000 - fn: 2624.0000 - accuracy: 0.5912 - precision: 0.6747 - recall: 0.4122 - auc: 0.7728 - val_loss: 1.7106 - val_tp: 195.0000 - val_fp: 253.0000 - val_tn: 739.0000 - val_fn: 301.0000 - val_accuracy: 0.4294 - val_precision: 0.4353 - val_recall: 0.3931 - val_auc: 0.5937\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.20685\n",
      "Epoch 172/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8960 - tp: 1797.0000 - fp: 858.0000 - tn: 8070.0000 - fn: 2667.0000 - accuracy: 0.5836 - precision: 0.6768 - recall: 0.4026 - auc: 0.7670 - val_loss: 1.5824 - val_tp: 191.0000 - val_fp: 227.0000 - val_tn: 765.0000 - val_fn: 305.0000 - val_accuracy: 0.4395 - val_precision: 0.4569 - val_recall: 0.3851 - val_auc: 0.6028\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.20685\n",
      "Epoch 173/500\n",
      "4464/4464 [==============================] - 0s 65us/step - loss: 0.8982 - tp: 1813.0000 - fp: 921.0000 - tn: 8007.0000 - fn: 2651.0000 - accuracy: 0.5764 - precision: 0.6631 - recall: 0.4061 - auc: 0.7654 - val_loss: 1.6407 - val_tp: 204.0000 - val_fp: 237.0000 - val_tn: 755.0000 - val_fn: 292.0000 - val_accuracy: 0.4516 - val_precision: 0.4626 - val_recall: 0.4113 - val_auc: 0.6121\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.20685\n",
      "Epoch 174/500\n",
      "4464/4464 [==============================] - 0s 65us/step - loss: 0.8993 - tp: 1819.0000 - fp: 901.0000 - tn: 8027.0000 - fn: 2645.0000 - accuracy: 0.5916 - precision: 0.6687 - recall: 0.4075 - auc: 0.7680 - val_loss: 2.0044 - val_tp: 217.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 279.0000 - val_accuracy: 0.4597 - val_precision: 0.4627 - val_recall: 0.4375 - val_auc: 0.6109\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.20685\n",
      "Epoch 175/500\n",
      "4464/4464 [==============================] - 0s 65us/step - loss: 0.9027 - tp: 1832.0000 - fp: 910.0000 - tn: 8018.0000 - fn: 2632.0000 - accuracy: 0.5759 - precision: 0.6681 - recall: 0.4104 - auc: 0.7637 - val_loss: 2.0640 - val_tp: 186.0000 - val_fp: 286.0000 - val_tn: 706.0000 - val_fn: 310.0000 - val_accuracy: 0.3911 - val_precision: 0.3941 - val_recall: 0.3750 - val_auc: 0.5660\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.20685\n",
      "Epoch 176/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.9013 - tp: 1784.0000 - fp: 895.0000 - tn: 8033.0000 - fn: 2680.0000 - accuracy: 0.5845 - precision: 0.6659 - recall: 0.3996 - auc: 0.7659 - val_loss: 2.0533 - val_tp: 217.0000 - val_fp: 261.0000 - val_tn: 731.0000 - val_fn: 279.0000 - val_accuracy: 0.4456 - val_precision: 0.4540 - val_recall: 0.4375 - val_auc: 0.6006\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.20685\n",
      "Epoch 177/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9093 - tp: 1778.0000 - fp: 882.0000 - tn: 8046.0000 - fn: 2686.0000 - accuracy: 0.5771 - precision: 0.6684 - recall: 0.3983 - auc: 0.7593 - val_loss: 1.8417 - val_tp: 202.0000 - val_fp: 261.0000 - val_tn: 731.0000 - val_fn: 294.0000 - val_accuracy: 0.4234 - val_precision: 0.4363 - val_recall: 0.4073 - val_auc: 0.6032\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.20685\n",
      "Epoch 178/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9118 - tp: 1763.0000 - fp: 935.0000 - tn: 7993.0000 - fn: 2701.0000 - accuracy: 0.5674 - precision: 0.6534 - recall: 0.3949 - auc: 0.7577 - val_loss: 2.0763 - val_tp: 221.0000 - val_fp: 269.0000 - val_tn: 723.0000 - val_fn: 275.0000 - val_accuracy: 0.4516 - val_precision: 0.4510 - val_recall: 0.4456 - val_auc: 0.6110\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.20685\n",
      "Epoch 179/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.9077 - tp: 1771.0000 - fp: 913.0000 - tn: 8015.0000 - fn: 2693.0000 - accuracy: 0.5822 - precision: 0.6598 - recall: 0.3967 - auc: 0.7624 - val_loss: 1.6004 - val_tp: 201.0000 - val_fp: 233.0000 - val_tn: 759.0000 - val_fn: 295.0000 - val_accuracy: 0.4718 - val_precision: 0.4631 - val_recall: 0.4052 - val_auc: 0.6196\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.20685\n",
      "Epoch 180/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9011 - tp: 1790.0000 - fp: 937.0000 - tn: 7991.0000 - fn: 2674.0000 - accuracy: 0.5753 - precision: 0.6564 - recall: 0.4010 - auc: 0.7639 - val_loss: 1.6183 - val_tp: 192.0000 - val_fp: 235.0000 - val_tn: 757.0000 - val_fn: 304.0000 - val_accuracy: 0.4254 - val_precision: 0.4496 - val_recall: 0.3871 - val_auc: 0.5938\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.20685\n",
      "Epoch 181/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.9017 - tp: 1830.0000 - fp: 877.0000 - tn: 8051.0000 - fn: 2634.0000 - accuracy: 0.5813 - precision: 0.6760 - recall: 0.4099 - auc: 0.7664 - val_loss: 2.4832 - val_tp: 202.0000 - val_fp: 280.0000 - val_tn: 712.0000 - val_fn: 294.0000 - val_accuracy: 0.4153 - val_precision: 0.4191 - val_recall: 0.4073 - val_auc: 0.5892\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.20685\n",
      "Epoch 182/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8895 - tp: 1854.0000 - fp: 884.0000 - tn: 8044.0000 - fn: 2610.0000 - accuracy: 0.5912 - precision: 0.6771 - recall: 0.4153 - auc: 0.7735 - val_loss: 1.5716 - val_tp: 192.0000 - val_fp: 242.0000 - val_tn: 750.0000 - val_fn: 304.0000 - val_accuracy: 0.4294 - val_precision: 0.4424 - val_recall: 0.3871 - val_auc: 0.5830\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.20685\n",
      "Epoch 183/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.9053 - tp: 1772.0000 - fp: 926.0000 - tn: 8002.0000 - fn: 2692.0000 - accuracy: 0.5762 - precision: 0.6568 - recall: 0.3970 - auc: 0.7622 - val_loss: 1.9894 - val_tp: 212.0000 - val_fp: 263.0000 - val_tn: 729.0000 - val_fn: 284.0000 - val_accuracy: 0.4476 - val_precision: 0.4463 - val_recall: 0.4274 - val_auc: 0.6109\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.20685\n",
      "Epoch 184/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8927 - tp: 1861.0000 - fp: 867.0000 - tn: 8061.0000 - fn: 2603.0000 - accuracy: 0.5849 - precision: 0.6822 - recall: 0.4169 - auc: 0.7716 - val_loss: 1.7107 - val_tp: 171.0000 - val_fp: 299.0000 - val_tn: 693.0000 - val_fn: 325.0000 - val_accuracy: 0.3528 - val_precision: 0.3638 - val_recall: 0.3448 - val_auc: 0.5499\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.20685\n",
      "Epoch 185/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8984 - tp: 1853.0000 - fp: 933.0000 - tn: 7995.0000 - fn: 2611.0000 - accuracy: 0.5822 - precision: 0.6651 - recall: 0.4151 - auc: 0.7647 - val_loss: 1.6621 - val_tp: 192.0000 - val_fp: 275.0000 - val_tn: 717.0000 - val_fn: 304.0000 - val_accuracy: 0.4073 - val_precision: 0.4111 - val_recall: 0.3871 - val_auc: 0.5699\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.20685\n",
      "Epoch 186/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.9010 - tp: 1770.0000 - fp: 892.0000 - tn: 8036.0000 - fn: 2694.0000 - accuracy: 0.5802 - precision: 0.6649 - recall: 0.3965 - auc: 0.7637 - val_loss: 1.5677 - val_tp: 197.0000 - val_fp: 233.0000 - val_tn: 759.0000 - val_fn: 299.0000 - val_accuracy: 0.4435 - val_precision: 0.4581 - val_recall: 0.3972 - val_auc: 0.5925\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.20685\n",
      "Epoch 187/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8922 - tp: 1813.0000 - fp: 898.0000 - tn: 8030.0000 - fn: 2651.0000 - accuracy: 0.5867 - precision: 0.6688 - recall: 0.4061 - auc: 0.7703 - val_loss: 1.5240 - val_tp: 175.0000 - val_fp: 278.0000 - val_tn: 714.0000 - val_fn: 321.0000 - val_accuracy: 0.3911 - val_precision: 0.3863 - val_recall: 0.3528 - val_auc: 0.5731\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.20685\n",
      "Epoch 188/500\n",
      "4464/4464 [==============================] - 0s 78us/step - loss: 0.8957 - tp: 1813.0000 - fp: 888.0000 - tn: 8040.0000 - fn: 2651.0000 - accuracy: 0.5856 - precision: 0.6712 - recall: 0.4061 - auc: 0.7684 - val_loss: 2.1912 - val_tp: 226.0000 - val_fp: 246.0000 - val_tn: 746.0000 - val_fn: 270.0000 - val_accuracy: 0.4698 - val_precision: 0.4788 - val_recall: 0.4556 - val_auc: 0.6260\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.20685\n",
      "Epoch 189/500\n",
      "4464/4464 [==============================] - 0s 78us/step - loss: 0.8981 - tp: 1815.0000 - fp: 870.0000 - tn: 8058.0000 - fn: 2649.0000 - accuracy: 0.5869 - precision: 0.6760 - recall: 0.4066 - auc: 0.7669 - val_loss: 1.8371 - val_tp: 214.0000 - val_fp: 243.0000 - val_tn: 749.0000 - val_fn: 282.0000 - val_accuracy: 0.4536 - val_precision: 0.4683 - val_recall: 0.4315 - val_auc: 0.6141\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.20685\n",
      "Epoch 190/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8927 - tp: 1880.0000 - fp: 935.0000 - tn: 7993.0000 - fn: 2584.0000 - accuracy: 0.5871 - precision: 0.6679 - recall: 0.4211 - auc: 0.7710 - val_loss: 1.6137 - val_tp: 203.0000 - val_fp: 238.0000 - val_tn: 754.0000 - val_fn: 293.0000 - val_accuracy: 0.4657 - val_precision: 0.4603 - val_recall: 0.4093 - val_auc: 0.6198\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.20685\n",
      "Epoch 191/500\n",
      "4464/4464 [==============================] - 0s 80us/step - loss: 0.9046 - tp: 1769.0000 - fp: 931.0000 - tn: 7997.0000 - fn: 2695.0000 - accuracy: 0.5677 - precision: 0.6552 - recall: 0.3963 - auc: 0.7614 - val_loss: 1.3918 - val_tp: 138.0000 - val_fp: 207.0000 - val_tn: 785.0000 - val_fn: 358.0000 - val_accuracy: 0.3548 - val_precision: 0.4000 - val_recall: 0.2782 - val_auc: 0.5415\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.20685\n",
      "Epoch 192/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9032 - tp: 1789.0000 - fp: 917.0000 - tn: 8011.0000 - fn: 2675.0000 - accuracy: 0.5815 - precision: 0.6611 - recall: 0.4008 - auc: 0.7641 - val_loss: 1.6039 - val_tp: 201.0000 - val_fp: 259.0000 - val_tn: 733.0000 - val_fn: 295.0000 - val_accuracy: 0.4214 - val_precision: 0.4370 - val_recall: 0.4052 - val_auc: 0.5987\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.20685\n",
      "Epoch 193/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8919 - tp: 1805.0000 - fp: 878.0000 - tn: 8050.0000 - fn: 2659.0000 - accuracy: 0.5907 - precision: 0.6728 - recall: 0.4043 - auc: 0.7723 - val_loss: 1.5587 - val_tp: 172.0000 - val_fp: 229.0000 - val_tn: 763.0000 - val_fn: 324.0000 - val_accuracy: 0.4153 - val_precision: 0.4289 - val_recall: 0.3468 - val_auc: 0.5891\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.20685\n",
      "Epoch 194/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8982 - tp: 1837.0000 - fp: 921.0000 - tn: 8007.0000 - fn: 2627.0000 - accuracy: 0.5883 - precision: 0.6661 - recall: 0.4115 - auc: 0.7677 - val_loss: 1.5319 - val_tp: 193.0000 - val_fp: 231.0000 - val_tn: 761.0000 - val_fn: 303.0000 - val_accuracy: 0.4597 - val_precision: 0.4552 - val_recall: 0.3891 - val_auc: 0.6146\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.20685\n",
      "Epoch 195/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8938 - tp: 1834.0000 - fp: 907.0000 - tn: 8021.0000 - fn: 2630.0000 - accuracy: 0.5822 - precision: 0.6691 - recall: 0.4108 - auc: 0.7679 - val_loss: 1.7284 - val_tp: 217.0000 - val_fp: 254.0000 - val_tn: 738.0000 - val_fn: 279.0000 - val_accuracy: 0.4556 - val_precision: 0.4607 - val_recall: 0.4375 - val_auc: 0.6100\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.20685\n",
      "Epoch 196/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8975 - tp: 1829.0000 - fp: 906.0000 - tn: 8022.0000 - fn: 2635.0000 - accuracy: 0.5858 - precision: 0.6687 - recall: 0.4097 - auc: 0.7688 - val_loss: 1.7017 - val_tp: 202.0000 - val_fp: 254.0000 - val_tn: 738.0000 - val_fn: 294.0000 - val_accuracy: 0.4274 - val_precision: 0.4430 - val_recall: 0.4073 - val_auc: 0.6016\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.20685\n",
      "Epoch 197/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.8975 - tp: 1867.0000 - fp: 890.0000 - tn: 8038.0000 - fn: 2597.0000 - accuracy: 0.5934 - precision: 0.6772 - recall: 0.4182 - auc: 0.7686 - val_loss: 1.5179 - val_tp: 167.0000 - val_fp: 220.0000 - val_tn: 772.0000 - val_fn: 329.0000 - val_accuracy: 0.4173 - val_precision: 0.4315 - val_recall: 0.3367 - val_auc: 0.5697\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.20685\n",
      "Epoch 198/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8875 - tp: 1838.0000 - fp: 905.0000 - tn: 8023.0000 - fn: 2626.0000 - accuracy: 0.5907 - precision: 0.6701 - recall: 0.4117 - auc: 0.7732 - val_loss: 1.4364 - val_tp: 187.0000 - val_fp: 201.0000 - val_tn: 791.0000 - val_fn: 309.0000 - val_accuracy: 0.4516 - val_precision: 0.4820 - val_recall: 0.3770 - val_auc: 0.6025\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.20685\n",
      "Epoch 199/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8917 - tp: 1844.0000 - fp: 926.0000 - tn: 8002.0000 - fn: 2620.0000 - accuracy: 0.5874 - precision: 0.6657 - recall: 0.4131 - auc: 0.7706 - val_loss: 2.7855 - val_tp: 236.0000 - val_fp: 253.0000 - val_tn: 739.0000 - val_fn: 260.0000 - val_accuracy: 0.4778 - val_precision: 0.4826 - val_recall: 0.4758 - val_auc: 0.6199\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.20685\n",
      "Epoch 200/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8941 - tp: 1834.0000 - fp: 913.0000 - tn: 8015.0000 - fn: 2630.0000 - accuracy: 0.5918 - precision: 0.6676 - recall: 0.4108 - auc: 0.7693 - val_loss: 2.1062 - val_tp: 173.0000 - val_fp: 309.0000 - val_tn: 683.0000 - val_fn: 323.0000 - val_accuracy: 0.3528 - val_precision: 0.3589 - val_recall: 0.3488 - val_auc: 0.5665\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.20685\n",
      "Epoch 201/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8957 - tp: 1865.0000 - fp: 962.0000 - tn: 7966.0000 - fn: 2599.0000 - accuracy: 0.5824 - precision: 0.6597 - recall: 0.4178 - auc: 0.7685 - val_loss: 1.8172 - val_tp: 211.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 285.0000 - val_accuracy: 0.4415 - val_precision: 0.4480 - val_recall: 0.4254 - val_auc: 0.6026\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.20685\n",
      "Epoch 202/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8871 - tp: 1891.0000 - fp: 909.0000 - tn: 8019.0000 - fn: 2573.0000 - accuracy: 0.5909 - precision: 0.6754 - recall: 0.4236 - auc: 0.7743 - val_loss: 1.6142 - val_tp: 200.0000 - val_fp: 239.0000 - val_tn: 753.0000 - val_fn: 296.0000 - val_accuracy: 0.4415 - val_precision: 0.4556 - val_recall: 0.4032 - val_auc: 0.6071\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.20685\n",
      "Epoch 203/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8944 - tp: 1832.0000 - fp: 908.0000 - tn: 8020.0000 - fn: 2632.0000 - accuracy: 0.5925 - precision: 0.6686 - recall: 0.4104 - auc: 0.7704 - val_loss: 1.9084 - val_tp: 161.0000 - val_fp: 319.0000 - val_tn: 673.0000 - val_fn: 335.0000 - val_accuracy: 0.3327 - val_precision: 0.3354 - val_recall: 0.3246 - val_auc: 0.5196\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.20685\n",
      "Epoch 204/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8878 - tp: 1845.0000 - fp: 885.0000 - tn: 8043.0000 - fn: 2619.0000 - accuracy: 0.5856 - precision: 0.6758 - recall: 0.4133 - auc: 0.7719 - val_loss: 1.6359 - val_tp: 214.0000 - val_fp: 232.0000 - val_tn: 760.0000 - val_fn: 282.0000 - val_accuracy: 0.4698 - val_precision: 0.4798 - val_recall: 0.4315 - val_auc: 0.6259\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.20685\n",
      "Epoch 205/500\n",
      "4464/4464 [==============================] - 0s 90us/step - loss: 0.8981 - tp: 1844.0000 - fp: 960.0000 - tn: 7968.0000 - fn: 2620.0000 - accuracy: 0.5804 - precision: 0.6576 - recall: 0.4131 - auc: 0.7661 - val_loss: 1.4073 - val_tp: 209.0000 - val_fp: 238.0000 - val_tn: 754.0000 - val_fn: 287.0000 - val_accuracy: 0.4738 - val_precision: 0.4676 - val_recall: 0.4214 - val_auc: 0.6247\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.20685\n",
      "Epoch 206/500\n",
      "4464/4464 [==============================] - 0s 90us/step - loss: 0.8916 - tp: 1833.0000 - fp: 944.0000 - tn: 7984.0000 - fn: 2631.0000 - accuracy: 0.5925 - precision: 0.6601 - recall: 0.4106 - auc: 0.7701 - val_loss: 1.6909 - val_tp: 207.0000 - val_fp: 267.0000 - val_tn: 725.0000 - val_fn: 289.0000 - val_accuracy: 0.4234 - val_precision: 0.4367 - val_recall: 0.4173 - val_auc: 0.5948\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.20685\n",
      "Epoch 207/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.8859 - tp: 1913.0000 - fp: 939.0000 - tn: 7989.0000 - fn: 2551.0000 - accuracy: 0.5909 - precision: 0.6708 - recall: 0.4285 - auc: 0.7750 - val_loss: 1.8506 - val_tp: 201.0000 - val_fp: 272.0000 - val_tn: 720.0000 - val_fn: 295.0000 - val_accuracy: 0.4173 - val_precision: 0.4249 - val_recall: 0.4052 - val_auc: 0.5823\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.20685\n",
      "Epoch 208/500\n",
      "4464/4464 [==============================] - 0s 74us/step - loss: 0.8794 - tp: 1917.0000 - fp: 889.0000 - tn: 8039.0000 - fn: 2547.0000 - accuracy: 0.5981 - precision: 0.6832 - recall: 0.4294 - auc: 0.7794 - val_loss: 1.9632 - val_tp: 164.0000 - val_fp: 302.0000 - val_tn: 690.0000 - val_fn: 332.0000 - val_accuracy: 0.3468 - val_precision: 0.3519 - val_recall: 0.3306 - val_auc: 0.5378\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.20685\n",
      "Epoch 209/500\n",
      "4464/4464 [==============================] - 0s 74us/step - loss: 0.8874 - tp: 1826.0000 - fp: 889.0000 - tn: 8039.0000 - fn: 2638.0000 - accuracy: 0.5842 - precision: 0.6726 - recall: 0.4091 - auc: 0.7721 - val_loss: 1.6357 - val_tp: 189.0000 - val_fp: 253.0000 - val_tn: 739.0000 - val_fn: 307.0000 - val_accuracy: 0.4153 - val_precision: 0.4276 - val_recall: 0.3810 - val_auc: 0.5795\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.20685\n",
      "Epoch 210/500\n",
      "4464/4464 [==============================] - 0s 77us/step - loss: 0.8923 - tp: 1893.0000 - fp: 951.0000 - tn: 7977.0000 - fn: 2571.0000 - accuracy: 0.5907 - precision: 0.6656 - recall: 0.4241 - auc: 0.7699 - val_loss: 1.7460 - val_tp: 193.0000 - val_fp: 259.0000 - val_tn: 733.0000 - val_fn: 303.0000 - val_accuracy: 0.4294 - val_precision: 0.4270 - val_recall: 0.3891 - val_auc: 0.5868\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.20685\n",
      "Epoch 211/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.8852 - tp: 1879.0000 - fp: 895.0000 - tn: 8033.0000 - fn: 2585.0000 - accuracy: 0.5979 - precision: 0.6774 - recall: 0.4209 - auc: 0.7761 - val_loss: 2.0781 - val_tp: 195.0000 - val_fp: 273.0000 - val_tn: 719.0000 - val_fn: 301.0000 - val_accuracy: 0.4254 - val_precision: 0.4167 - val_recall: 0.3931 - val_auc: 0.5828\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.20685\n",
      "Epoch 212/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8937 - tp: 1800.0000 - fp: 887.0000 - tn: 8041.0000 - fn: 2664.0000 - accuracy: 0.5811 - precision: 0.6699 - recall: 0.4032 - auc: 0.7706 - val_loss: 1.6490 - val_tp: 210.0000 - val_fp: 254.0000 - val_tn: 738.0000 - val_fn: 286.0000 - val_accuracy: 0.4395 - val_precision: 0.4526 - val_recall: 0.4234 - val_auc: 0.6097\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.20685\n",
      "Epoch 213/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8966 - tp: 1892.0000 - fp: 946.0000 - tn: 7982.0000 - fn: 2572.0000 - accuracy: 0.5793 - precision: 0.6667 - recall: 0.4238 - auc: 0.7688 - val_loss: 2.0332 - val_tp: 196.0000 - val_fp: 292.0000 - val_tn: 700.0000 - val_fn: 300.0000 - val_accuracy: 0.3992 - val_precision: 0.4016 - val_recall: 0.3952 - val_auc: 0.5702\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.20685\n",
      "Epoch 214/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8978 - tp: 1803.0000 - fp: 910.0000 - tn: 8018.0000 - fn: 2661.0000 - accuracy: 0.5811 - precision: 0.6646 - recall: 0.4039 - auc: 0.7663 - val_loss: 1.4435 - val_tp: 177.0000 - val_fp: 213.0000 - val_tn: 779.0000 - val_fn: 319.0000 - val_accuracy: 0.4496 - val_precision: 0.4538 - val_recall: 0.3569 - val_auc: 0.6025\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.20685\n",
      "Epoch 215/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8917 - tp: 1887.0000 - fp: 909.0000 - tn: 8019.0000 - fn: 2577.0000 - accuracy: 0.5887 - precision: 0.6749 - recall: 0.4227 - auc: 0.7720 - val_loss: 1.3418 - val_tp: 129.0000 - val_fp: 162.0000 - val_tn: 830.0000 - val_fn: 367.0000 - val_accuracy: 0.4173 - val_precision: 0.4433 - val_recall: 0.2601 - val_auc: 0.5714\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.20685\n",
      "Epoch 216/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8913 - tp: 1844.0000 - fp: 895.0000 - tn: 8033.0000 - fn: 2620.0000 - accuracy: 0.5806 - precision: 0.6732 - recall: 0.4131 - auc: 0.7713 - val_loss: 1.8071 - val_tp: 209.0000 - val_fp: 254.0000 - val_tn: 738.0000 - val_fn: 287.0000 - val_accuracy: 0.4476 - val_precision: 0.4514 - val_recall: 0.4214 - val_auc: 0.6078\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.20685\n",
      "Epoch 217/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8888 - tp: 1844.0000 - fp: 931.0000 - tn: 7997.0000 - fn: 2620.0000 - accuracy: 0.5889 - precision: 0.6645 - recall: 0.4131 - auc: 0.7728 - val_loss: 1.8593 - val_tp: 217.0000 - val_fp: 251.0000 - val_tn: 741.0000 - val_fn: 279.0000 - val_accuracy: 0.4577 - val_precision: 0.4637 - val_recall: 0.4375 - val_auc: 0.6184\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.20685\n",
      "Epoch 218/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8836 - tp: 1907.0000 - fp: 929.0000 - tn: 7999.0000 - fn: 2557.0000 - accuracy: 0.5901 - precision: 0.6724 - recall: 0.4272 - auc: 0.7753 - val_loss: 1.5820 - val_tp: 169.0000 - val_fp: 257.0000 - val_tn: 735.0000 - val_fn: 327.0000 - val_accuracy: 0.4032 - val_precision: 0.3967 - val_recall: 0.3407 - val_auc: 0.5610\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.20685\n",
      "Epoch 219/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8952 - tp: 1815.0000 - fp: 929.0000 - tn: 7999.0000 - fn: 2649.0000 - accuracy: 0.5860 - precision: 0.6614 - recall: 0.4066 - auc: 0.7690 - val_loss: 1.5786 - val_tp: 213.0000 - val_fp: 244.0000 - val_tn: 748.0000 - val_fn: 283.0000 - val_accuracy: 0.4657 - val_precision: 0.4661 - val_recall: 0.4294 - val_auc: 0.6082\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.20685\n",
      "Epoch 220/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8888 - tp: 1879.0000 - fp: 901.0000 - tn: 8027.0000 - fn: 2585.0000 - accuracy: 0.5916 - precision: 0.6759 - recall: 0.4209 - auc: 0.7738 - val_loss: 1.3582 - val_tp: 173.0000 - val_fp: 216.0000 - val_tn: 776.0000 - val_fn: 323.0000 - val_accuracy: 0.4214 - val_precision: 0.4447 - val_recall: 0.3488 - val_auc: 0.5935\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.20685\n",
      "Epoch 221/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8796 - tp: 1897.0000 - fp: 897.0000 - tn: 8031.0000 - fn: 2567.0000 - accuracy: 0.6001 - precision: 0.6790 - recall: 0.4250 - auc: 0.7789 - val_loss: 1.5369 - val_tp: 191.0000 - val_fp: 230.0000 - val_tn: 762.0000 - val_fn: 305.0000 - val_accuracy: 0.4496 - val_precision: 0.4537 - val_recall: 0.3851 - val_auc: 0.5988\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.20685\n",
      "Epoch 222/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8721 - tp: 1903.0000 - fp: 878.0000 - tn: 8050.0000 - fn: 2561.0000 - accuracy: 0.5909 - precision: 0.6843 - recall: 0.4263 - auc: 0.7811 - val_loss: 2.0272 - val_tp: 184.0000 - val_fp: 297.0000 - val_tn: 695.0000 - val_fn: 312.0000 - val_accuracy: 0.3790 - val_precision: 0.3825 - val_recall: 0.3710 - val_auc: 0.5644\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.20685\n",
      "Epoch 223/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8884 - tp: 1868.0000 - fp: 919.0000 - tn: 8009.0000 - fn: 2596.0000 - accuracy: 0.5869 - precision: 0.6703 - recall: 0.4185 - auc: 0.7710 - val_loss: 1.6698 - val_tp: 180.0000 - val_fp: 271.0000 - val_tn: 721.0000 - val_fn: 316.0000 - val_accuracy: 0.3911 - val_precision: 0.3991 - val_recall: 0.3629 - val_auc: 0.5672\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.20685\n",
      "Epoch 224/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8901 - tp: 1898.0000 - fp: 940.0000 - tn: 7988.0000 - fn: 2566.0000 - accuracy: 0.5851 - precision: 0.6688 - recall: 0.4252 - auc: 0.7712 - val_loss: 1.5957 - val_tp: 135.0000 - val_fp: 209.0000 - val_tn: 783.0000 - val_fn: 361.0000 - val_accuracy: 0.3690 - val_precision: 0.3924 - val_recall: 0.2722 - val_auc: 0.5138\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.20685\n",
      "Epoch 225/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8877 - tp: 1876.0000 - fp: 904.0000 - tn: 8024.0000 - fn: 2588.0000 - accuracy: 0.5836 - precision: 0.6748 - recall: 0.4203 - auc: 0.7728 - val_loss: 1.6256 - val_tp: 204.0000 - val_fp: 261.0000 - val_tn: 731.0000 - val_fn: 292.0000 - val_accuracy: 0.4254 - val_precision: 0.4387 - val_recall: 0.4113 - val_auc: 0.6056\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.20685\n",
      "Epoch 226/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8814 - tp: 1922.0000 - fp: 921.0000 - tn: 8007.0000 - fn: 2542.0000 - accuracy: 0.5941 - precision: 0.6760 - recall: 0.4306 - auc: 0.7774 - val_loss: 1.9864 - val_tp: 190.0000 - val_fp: 263.0000 - val_tn: 729.0000 - val_fn: 306.0000 - val_accuracy: 0.4214 - val_precision: 0.4194 - val_recall: 0.3831 - val_auc: 0.5713\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.20685\n",
      "Epoch 227/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8736 - tp: 1894.0000 - fp: 906.0000 - tn: 8022.0000 - fn: 2570.0000 - accuracy: 0.5990 - precision: 0.6764 - recall: 0.4243 - auc: 0.7816 - val_loss: 2.0060 - val_tp: 212.0000 - val_fp: 254.0000 - val_tn: 738.0000 - val_fn: 284.0000 - val_accuracy: 0.4456 - val_precision: 0.4549 - val_recall: 0.4274 - val_auc: 0.6100\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.20685\n",
      "Epoch 228/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8744 - tp: 1929.0000 - fp: 867.0000 - tn: 8061.0000 - fn: 2535.0000 - accuracy: 0.5988 - precision: 0.6899 - recall: 0.4321 - auc: 0.7823 - val_loss: 1.7593 - val_tp: 181.0000 - val_fp: 289.0000 - val_tn: 703.0000 - val_fn: 315.0000 - val_accuracy: 0.3931 - val_precision: 0.3851 - val_recall: 0.3649 - val_auc: 0.5530\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.20685\n",
      "Epoch 229/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8762 - tp: 1898.0000 - fp: 907.0000 - tn: 8021.0000 - fn: 2566.0000 - accuracy: 0.5889 - precision: 0.6766 - recall: 0.4252 - auc: 0.7793 - val_loss: 1.6681 - val_tp: 208.0000 - val_fp: 248.0000 - val_tn: 744.0000 - val_fn: 288.0000 - val_accuracy: 0.4536 - val_precision: 0.4561 - val_recall: 0.4194 - val_auc: 0.6152\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.20685\n",
      "Epoch 230/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8810 - tp: 1982.0000 - fp: 966.0000 - tn: 7962.0000 - fn: 2482.0000 - accuracy: 0.5934 - precision: 0.6723 - recall: 0.4440 - auc: 0.7781 - val_loss: 1.7193 - val_tp: 191.0000 - val_fp: 267.0000 - val_tn: 725.0000 - val_fn: 305.0000 - val_accuracy: 0.4173 - val_precision: 0.4170 - val_recall: 0.3851 - val_auc: 0.5669\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.20685\n",
      "Epoch 231/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8829 - tp: 1931.0000 - fp: 907.0000 - tn: 8021.0000 - fn: 2533.0000 - accuracy: 0.5966 - precision: 0.6804 - recall: 0.4326 - auc: 0.7755 - val_loss: 1.6279 - val_tp: 181.0000 - val_fp: 286.0000 - val_tn: 706.0000 - val_fn: 315.0000 - val_accuracy: 0.3831 - val_precision: 0.3876 - val_recall: 0.3649 - val_auc: 0.5756\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.20685\n",
      "Epoch 232/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.9028 - tp: 1816.0000 - fp: 964.0000 - tn: 7964.0000 - fn: 2648.0000 - accuracy: 0.5759 - precision: 0.6532 - recall: 0.4068 - auc: 0.7627 - val_loss: 2.1948 - val_tp: 228.0000 - val_fp: 251.0000 - val_tn: 741.0000 - val_fn: 268.0000 - val_accuracy: 0.4798 - val_precision: 0.4760 - val_recall: 0.4597 - val_auc: 0.6155\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.20685\n",
      "Epoch 233/500\n",
      "4464/4464 [==============================] - 0s 83us/step - loss: 0.8855 - tp: 1870.0000 - fp: 928.0000 - tn: 8000.0000 - fn: 2594.0000 - accuracy: 0.5943 - precision: 0.6683 - recall: 0.4189 - auc: 0.7753 - val_loss: 1.7587 - val_tp: 195.0000 - val_fp: 250.0000 - val_tn: 742.0000 - val_fn: 301.0000 - val_accuracy: 0.4254 - val_precision: 0.4382 - val_recall: 0.3931 - val_auc: 0.6045\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.20685\n",
      "Epoch 234/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.8830 - tp: 1917.0000 - fp: 916.0000 - tn: 8012.0000 - fn: 2547.0000 - accuracy: 0.5954 - precision: 0.6767 - recall: 0.4294 - auc: 0.7769 - val_loss: 2.1092 - val_tp: 201.0000 - val_fp: 281.0000 - val_tn: 711.0000 - val_fn: 295.0000 - val_accuracy: 0.4153 - val_precision: 0.4170 - val_recall: 0.4052 - val_auc: 0.5867\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.20685\n",
      "Epoch 235/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 77us/step - loss: 0.8860 - tp: 1878.0000 - fp: 909.0000 - tn: 8019.0000 - fn: 2586.0000 - accuracy: 0.5869 - precision: 0.6738 - recall: 0.4207 - auc: 0.7739 - val_loss: 1.7196 - val_tp: 196.0000 - val_fp: 268.0000 - val_tn: 724.0000 - val_fn: 300.0000 - val_accuracy: 0.4315 - val_precision: 0.4224 - val_recall: 0.3952 - val_auc: 0.5726\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.20685\n",
      "Epoch 236/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.8852 - tp: 1895.0000 - fp: 931.0000 - tn: 7997.0000 - fn: 2569.0000 - accuracy: 0.5878 - precision: 0.6706 - recall: 0.4245 - auc: 0.7752 - val_loss: 1.7430 - val_tp: 187.0000 - val_fp: 271.0000 - val_tn: 721.0000 - val_fn: 309.0000 - val_accuracy: 0.4052 - val_precision: 0.4083 - val_recall: 0.3770 - val_auc: 0.5840\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.20685\n",
      "Epoch 237/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8737 - tp: 1946.0000 - fp: 926.0000 - tn: 8002.0000 - fn: 2518.0000 - accuracy: 0.5880 - precision: 0.6776 - recall: 0.4359 - auc: 0.7793 - val_loss: 1.8180 - val_tp: 206.0000 - val_fp: 265.0000 - val_tn: 727.0000 - val_fn: 290.0000 - val_accuracy: 0.4355 - val_precision: 0.4374 - val_recall: 0.4153 - val_auc: 0.6016\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.20685\n",
      "Epoch 238/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8745 - tp: 1923.0000 - fp: 889.0000 - tn: 8039.0000 - fn: 2541.0000 - accuracy: 0.6028 - precision: 0.6839 - recall: 0.4308 - auc: 0.7824 - val_loss: 1.5915 - val_tp: 202.0000 - val_fp: 247.0000 - val_tn: 745.0000 - val_fn: 294.0000 - val_accuracy: 0.4435 - val_precision: 0.4499 - val_recall: 0.4073 - val_auc: 0.5910\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.20685\n",
      "Epoch 239/500\n",
      "4464/4464 [==============================] - 0s 74us/step - loss: 0.8704 - tp: 1950.0000 - fp: 928.0000 - tn: 8000.0000 - fn: 2514.0000 - accuracy: 0.5979 - precision: 0.6776 - recall: 0.4368 - auc: 0.7835 - val_loss: 1.6736 - val_tp: 191.0000 - val_fp: 248.0000 - val_tn: 744.0000 - val_fn: 305.0000 - val_accuracy: 0.4476 - val_precision: 0.4351 - val_recall: 0.3851 - val_auc: 0.5960\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.20685\n",
      "Epoch 240/500\n",
      "4464/4464 [==============================] - 0s 83us/step - loss: 0.8837 - tp: 1866.0000 - fp: 893.0000 - tn: 8035.0000 - fn: 2598.0000 - accuracy: 0.5889 - precision: 0.6763 - recall: 0.4180 - auc: 0.7753 - val_loss: 1.6470 - val_tp: 182.0000 - val_fp: 251.0000 - val_tn: 741.0000 - val_fn: 314.0000 - val_accuracy: 0.4113 - val_precision: 0.4203 - val_recall: 0.3669 - val_auc: 0.5709\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.20685\n",
      "Epoch 241/500\n",
      "4464/4464 [==============================] - 0s 75us/step - loss: 0.8852 - tp: 1872.0000 - fp: 969.0000 - tn: 7959.0000 - fn: 2592.0000 - accuracy: 0.5880 - precision: 0.6589 - recall: 0.4194 - auc: 0.7725 - val_loss: 1.7676 - val_tp: 189.0000 - val_fp: 236.0000 - val_tn: 756.0000 - val_fn: 307.0000 - val_accuracy: 0.4335 - val_precision: 0.4447 - val_recall: 0.3810 - val_auc: 0.6043\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.20685\n",
      "Epoch 242/500\n",
      "4464/4464 [==============================] - 0s 82us/step - loss: 0.8783 - tp: 1942.0000 - fp: 933.0000 - tn: 7995.0000 - fn: 2522.0000 - accuracy: 0.5860 - precision: 0.6755 - recall: 0.4350 - auc: 0.7775 - val_loss: 1.5207 - val_tp: 195.0000 - val_fp: 222.0000 - val_tn: 770.0000 - val_fn: 301.0000 - val_accuracy: 0.4577 - val_precision: 0.4676 - val_recall: 0.3931 - val_auc: 0.6154\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.20685\n",
      "Epoch 243/500\n",
      "4464/4464 [==============================] - 0s 76us/step - loss: 0.8783 - tp: 1927.0000 - fp: 925.0000 - tn: 8003.0000 - fn: 2537.0000 - accuracy: 0.5952 - precision: 0.6757 - recall: 0.4317 - auc: 0.7794 - val_loss: 1.8804 - val_tp: 198.0000 - val_fp: 285.0000 - val_tn: 707.0000 - val_fn: 298.0000 - val_accuracy: 0.4153 - val_precision: 0.4099 - val_recall: 0.3992 - val_auc: 0.5737\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.20685\n",
      "Epoch 244/500\n",
      "4464/4464 [==============================] - 0s 78us/step - loss: 0.8771 - tp: 1966.0000 - fp: 940.0000 - tn: 7988.0000 - fn: 2498.0000 - accuracy: 0.5988 - precision: 0.6765 - recall: 0.4404 - auc: 0.7800 - val_loss: 1.5516 - val_tp: 184.0000 - val_fp: 236.0000 - val_tn: 756.0000 - val_fn: 312.0000 - val_accuracy: 0.4315 - val_precision: 0.4381 - val_recall: 0.3710 - val_auc: 0.5802\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.20685\n",
      "Epoch 245/500\n",
      "4464/4464 [==============================] - 0s 76us/step - loss: 0.8805 - tp: 1904.0000 - fp: 933.0000 - tn: 7995.0000 - fn: 2560.0000 - accuracy: 0.5901 - precision: 0.6711 - recall: 0.4265 - auc: 0.7771 - val_loss: 1.4161 - val_tp: 175.0000 - val_fp: 194.0000 - val_tn: 798.0000 - val_fn: 321.0000 - val_accuracy: 0.4597 - val_precision: 0.4743 - val_recall: 0.3528 - val_auc: 0.6089\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.20685\n",
      "Epoch 246/500\n",
      "4464/4464 [==============================] - 0s 78us/step - loss: 0.8755 - tp: 1912.0000 - fp: 925.0000 - tn: 8003.0000 - fn: 2552.0000 - accuracy: 0.5894 - precision: 0.6740 - recall: 0.4283 - auc: 0.7791 - val_loss: 1.7898 - val_tp: 152.0000 - val_fp: 302.0000 - val_tn: 690.0000 - val_fn: 344.0000 - val_accuracy: 0.3246 - val_precision: 0.3348 - val_recall: 0.3065 - val_auc: 0.5142\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.20685\n",
      "Epoch 247/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8763 - tp: 1872.0000 - fp: 882.0000 - tn: 8046.0000 - fn: 2592.0000 - accuracy: 0.5923 - precision: 0.6797 - recall: 0.4194 - auc: 0.7817 - val_loss: 1.9310 - val_tp: 203.0000 - val_fp: 259.0000 - val_tn: 733.0000 - val_fn: 293.0000 - val_accuracy: 0.4375 - val_precision: 0.4394 - val_recall: 0.4093 - val_auc: 0.5984\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.20685\n",
      "Epoch 248/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8780 - tp: 1923.0000 - fp: 938.0000 - tn: 7990.0000 - fn: 2541.0000 - accuracy: 0.5950 - precision: 0.6721 - recall: 0.4308 - auc: 0.7806 - val_loss: 1.4602 - val_tp: 200.0000 - val_fp: 212.0000 - val_tn: 780.0000 - val_fn: 296.0000 - val_accuracy: 0.4556 - val_precision: 0.4854 - val_recall: 0.4032 - val_auc: 0.6181\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.20685\n",
      "Epoch 249/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8748 - tp: 1982.0000 - fp: 910.0000 - tn: 8018.0000 - fn: 2482.0000 - accuracy: 0.6019 - precision: 0.6853 - recall: 0.4440 - auc: 0.7828 - val_loss: 1.8604 - val_tp: 200.0000 - val_fp: 275.0000 - val_tn: 717.0000 - val_fn: 296.0000 - val_accuracy: 0.4153 - val_precision: 0.4211 - val_recall: 0.4032 - val_auc: 0.5956\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.20685\n",
      "Epoch 250/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8590 - tp: 2030.0000 - fp: 902.0000 - tn: 8026.0000 - fn: 2434.0000 - accuracy: 0.6080 - precision: 0.6924 - recall: 0.4547 - auc: 0.7906 - val_loss: 1.6938 - val_tp: 220.0000 - val_fp: 253.0000 - val_tn: 739.0000 - val_fn: 276.0000 - val_accuracy: 0.4577 - val_precision: 0.4651 - val_recall: 0.4435 - val_auc: 0.6197\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.20685\n",
      "Epoch 251/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8619 - tp: 2020.0000 - fp: 902.0000 - tn: 8026.0000 - fn: 2444.0000 - accuracy: 0.6060 - precision: 0.6913 - recall: 0.4525 - auc: 0.7902 - val_loss: 2.4888 - val_tp: 208.0000 - val_fp: 280.0000 - val_tn: 712.0000 - val_fn: 288.0000 - val_accuracy: 0.4294 - val_precision: 0.4262 - val_recall: 0.4194 - val_auc: 0.6034\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.20685\n",
      "Epoch 252/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.8706 - tp: 1966.0000 - fp: 964.0000 - tn: 7964.0000 - fn: 2498.0000 - accuracy: 0.6022 - precision: 0.6710 - recall: 0.4404 - auc: 0.7834 - val_loss: 2.1413 - val_tp: 220.0000 - val_fp: 257.0000 - val_tn: 735.0000 - val_fn: 276.0000 - val_accuracy: 0.4577 - val_precision: 0.4612 - val_recall: 0.4435 - val_auc: 0.6132\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.20685\n",
      "Epoch 253/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8836 - tp: 1879.0000 - fp: 917.0000 - tn: 8011.0000 - fn: 2585.0000 - accuracy: 0.5883 - precision: 0.6720 - recall: 0.4209 - auc: 0.7756 - val_loss: 1.8488 - val_tp: 204.0000 - val_fp: 276.0000 - val_tn: 716.0000 - val_fn: 292.0000 - val_accuracy: 0.4254 - val_precision: 0.4250 - val_recall: 0.4113 - val_auc: 0.5872\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.20685\n",
      "Epoch 254/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8723 - tp: 1935.0000 - fp: 873.0000 - tn: 8055.0000 - fn: 2529.0000 - accuracy: 0.5972 - precision: 0.6891 - recall: 0.4335 - auc: 0.7808 - val_loss: 1.5272 - val_tp: 171.0000 - val_fp: 230.0000 - val_tn: 762.0000 - val_fn: 325.0000 - val_accuracy: 0.4375 - val_precision: 0.4264 - val_recall: 0.3448 - val_auc: 0.5734\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.20685\n",
      "Epoch 255/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8826 - tp: 1944.0000 - fp: 984.0000 - tn: 7944.0000 - fn: 2520.0000 - accuracy: 0.5952 - precision: 0.6639 - recall: 0.4355 - auc: 0.7757 - val_loss: 1.8026 - val_tp: 201.0000 - val_fp: 279.0000 - val_tn: 713.0000 - val_fn: 295.0000 - val_accuracy: 0.4093 - val_precision: 0.4187 - val_recall: 0.4052 - val_auc: 0.5780\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.20685\n",
      "Epoch 256/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8766 - tp: 1916.0000 - fp: 924.0000 - tn: 8004.0000 - fn: 2548.0000 - accuracy: 0.6015 - precision: 0.6746 - recall: 0.4292 - auc: 0.7803 - val_loss: 1.7519 - val_tp: 230.0000 - val_fp: 251.0000 - val_tn: 741.0000 - val_fn: 266.0000 - val_accuracy: 0.4738 - val_precision: 0.4782 - val_recall: 0.4637 - val_auc: 0.6260\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.20685\n",
      "Epoch 257/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8721 - tp: 1969.0000 - fp: 949.0000 - tn: 7979.0000 - fn: 2495.0000 - accuracy: 0.5932 - precision: 0.6748 - recall: 0.4411 - auc: 0.7814 - val_loss: 1.6301 - val_tp: 189.0000 - val_fp: 279.0000 - val_tn: 713.0000 - val_fn: 307.0000 - val_accuracy: 0.4012 - val_precision: 0.4038 - val_recall: 0.3810 - val_auc: 0.5597\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.20685\n",
      "Epoch 258/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8701 - tp: 1972.0000 - fp: 916.0000 - tn: 8012.0000 - fn: 2492.0000 - accuracy: 0.6111 - precision: 0.6828 - recall: 0.4418 - auc: 0.7839 - val_loss: 1.9426 - val_tp: 228.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 268.0000 - val_accuracy: 0.4657 - val_precision: 0.4672 - val_recall: 0.4597 - val_auc: 0.6095\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.20685\n",
      "Epoch 259/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8914 - tp: 1902.0000 - fp: 1004.0000 - tn: 7924.0000 - fn: 2562.0000 - accuracy: 0.5791 - precision: 0.6545 - recall: 0.4261 - auc: 0.7704 - val_loss: 1.4344 - val_tp: 110.0000 - val_fp: 198.0000 - val_tn: 794.0000 - val_fn: 386.0000 - val_accuracy: 0.3710 - val_precision: 0.3571 - val_recall: 0.2218 - val_auc: 0.5339\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.20685\n",
      "Epoch 260/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8682 - tp: 1965.0000 - fp: 905.0000 - tn: 8023.0000 - fn: 2499.0000 - accuracy: 0.5986 - precision: 0.6847 - recall: 0.4402 - auc: 0.7853 - val_loss: 1.7229 - val_tp: 217.0000 - val_fp: 258.0000 - val_tn: 734.0000 - val_fn: 279.0000 - val_accuracy: 0.4556 - val_precision: 0.4568 - val_recall: 0.4375 - val_auc: 0.6186\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.20685\n",
      "Epoch 261/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8853 - tp: 1937.0000 - fp: 950.0000 - tn: 7978.0000 - fn: 2527.0000 - accuracy: 0.5966 - precision: 0.6709 - recall: 0.4339 - auc: 0.7757 - val_loss: 1.7900 - val_tp: 197.0000 - val_fp: 257.0000 - val_tn: 735.0000 - val_fn: 299.0000 - val_accuracy: 0.4274 - val_precision: 0.4339 - val_recall: 0.3972 - val_auc: 0.5983\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.20685\n",
      "Epoch 262/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8773 - tp: 1906.0000 - fp: 921.0000 - tn: 8007.0000 - fn: 2558.0000 - accuracy: 0.5927 - precision: 0.6742 - recall: 0.4270 - auc: 0.7795 - val_loss: 1.4746 - val_tp: 195.0000 - val_fp: 230.0000 - val_tn: 762.0000 - val_fn: 301.0000 - val_accuracy: 0.4435 - val_precision: 0.4588 - val_recall: 0.3931 - val_auc: 0.5864\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.20685\n",
      "Epoch 263/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8796 - tp: 1917.0000 - fp: 917.0000 - tn: 8011.0000 - fn: 2547.0000 - accuracy: 0.5867 - precision: 0.6764 - recall: 0.4294 - auc: 0.7782 - val_loss: 1.6289 - val_tp: 174.0000 - val_fp: 274.0000 - val_tn: 718.0000 - val_fn: 322.0000 - val_accuracy: 0.3851 - val_precision: 0.3884 - val_recall: 0.3508 - val_auc: 0.5684\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.20685\n",
      "Epoch 264/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8722 - tp: 1938.0000 - fp: 948.0000 - tn: 7980.0000 - fn: 2526.0000 - accuracy: 0.5981 - precision: 0.6715 - recall: 0.4341 - auc: 0.7820 - val_loss: 1.5147 - val_tp: 202.0000 - val_fp: 238.0000 - val_tn: 754.0000 - val_fn: 294.0000 - val_accuracy: 0.4496 - val_precision: 0.4591 - val_recall: 0.4073 - val_auc: 0.6160\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.20685\n",
      "Epoch 265/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8704 - tp: 1951.0000 - fp: 927.0000 - tn: 8001.0000 - fn: 2513.0000 - accuracy: 0.6004 - precision: 0.6779 - recall: 0.4371 - auc: 0.7830 - val_loss: 2.3320 - val_tp: 197.0000 - val_fp: 291.0000 - val_tn: 701.0000 - val_fn: 299.0000 - val_accuracy: 0.3992 - val_precision: 0.4037 - val_recall: 0.3972 - val_auc: 0.5888\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.20685\n",
      "Epoch 266/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8743 - tp: 1937.0000 - fp: 919.0000 - tn: 8009.0000 - fn: 2527.0000 - accuracy: 0.5988 - precision: 0.6782 - recall: 0.4339 - auc: 0.7806 - val_loss: 1.6162 - val_tp: 185.0000 - val_fp: 242.0000 - val_tn: 750.0000 - val_fn: 311.0000 - val_accuracy: 0.4113 - val_precision: 0.4333 - val_recall: 0.3730 - val_auc: 0.5837\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 1.20685\n",
      "Epoch 267/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8776 - tp: 1958.0000 - fp: 934.0000 - tn: 7994.0000 - fn: 2506.0000 - accuracy: 0.5970 - precision: 0.6770 - recall: 0.4386 - auc: 0.7798 - val_loss: 2.0939 - val_tp: 212.0000 - val_fp: 261.0000 - val_tn: 731.0000 - val_fn: 284.0000 - val_accuracy: 0.4516 - val_precision: 0.4482 - val_recall: 0.4274 - val_auc: 0.6099\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 1.20685\n",
      "Epoch 268/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8622 - tp: 2000.0000 - fp: 927.0000 - tn: 8001.0000 - fn: 2464.0000 - accuracy: 0.6055 - precision: 0.6833 - recall: 0.4480 - auc: 0.7893 - val_loss: 2.0804 - val_tp: 227.0000 - val_fp: 264.0000 - val_tn: 728.0000 - val_fn: 269.0000 - val_accuracy: 0.4637 - val_precision: 0.4623 - val_recall: 0.4577 - val_auc: 0.6151\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 1.20685\n",
      "Epoch 269/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8676 - tp: 1967.0000 - fp: 881.0000 - tn: 8047.0000 - fn: 2497.0000 - accuracy: 0.6086 - precision: 0.6907 - recall: 0.4406 - auc: 0.7861 - val_loss: 1.6042 - val_tp: 191.0000 - val_fp: 254.0000 - val_tn: 738.0000 - val_fn: 305.0000 - val_accuracy: 0.4153 - val_precision: 0.4292 - val_recall: 0.3851 - val_auc: 0.5766\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 1.20685\n",
      "Epoch 270/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8677 - tp: 2016.0000 - fp: 963.0000 - tn: 7965.0000 - fn: 2448.0000 - accuracy: 0.6082 - precision: 0.6767 - recall: 0.4516 - auc: 0.7855 - val_loss: 2.2136 - val_tp: 201.0000 - val_fp: 277.0000 - val_tn: 715.0000 - val_fn: 295.0000 - val_accuracy: 0.4133 - val_precision: 0.4205 - val_recall: 0.4052 - val_auc: 0.5862\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 1.20685\n",
      "Epoch 271/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8637 - tp: 1943.0000 - fp: 886.0000 - tn: 8042.0000 - fn: 2521.0000 - accuracy: 0.6055 - precision: 0.6868 - recall: 0.4353 - auc: 0.7894 - val_loss: 1.8884 - val_tp: 201.0000 - val_fp: 257.0000 - val_tn: 735.0000 - val_fn: 295.0000 - val_accuracy: 0.4355 - val_precision: 0.4389 - val_recall: 0.4052 - val_auc: 0.5938\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 1.20685\n",
      "Epoch 272/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8781 - tp: 2002.0000 - fp: 970.0000 - tn: 7958.0000 - fn: 2462.0000 - accuracy: 0.5918 - precision: 0.6736 - recall: 0.4485 - auc: 0.7787 - val_loss: 1.9458 - val_tp: 213.0000 - val_fp: 266.0000 - val_tn: 726.0000 - val_fn: 283.0000 - val_accuracy: 0.4435 - val_precision: 0.4447 - val_recall: 0.4294 - val_auc: 0.5927\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 1.20685\n",
      "Epoch 273/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8878 - tp: 1935.0000 - fp: 933.0000 - tn: 7995.0000 - fn: 2529.0000 - accuracy: 0.5952 - precision: 0.6747 - recall: 0.4335 - auc: 0.7734 - val_loss: 2.1704 - val_tp: 186.0000 - val_fp: 278.0000 - val_tn: 714.0000 - val_fn: 310.0000 - val_accuracy: 0.3992 - val_precision: 0.4009 - val_recall: 0.3750 - val_auc: 0.5874\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 1.20685\n",
      "Epoch 274/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8778 - tp: 1916.0000 - fp: 954.0000 - tn: 7974.0000 - fn: 2548.0000 - accuracy: 0.6006 - precision: 0.6676 - recall: 0.4292 - auc: 0.7799 - val_loss: 1.4435 - val_tp: 180.0000 - val_fp: 200.0000 - val_tn: 792.0000 - val_fn: 316.0000 - val_accuracy: 0.4516 - val_precision: 0.4737 - val_recall: 0.3629 - val_auc: 0.6056\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 1.20685\n",
      "Epoch 275/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8628 - tp: 1974.0000 - fp: 915.0000 - tn: 8013.0000 - fn: 2490.0000 - accuracy: 0.6098 - precision: 0.6833 - recall: 0.4422 - auc: 0.7881 - val_loss: 1.8697 - val_tp: 200.0000 - val_fp: 244.0000 - val_tn: 748.0000 - val_fn: 296.0000 - val_accuracy: 0.4435 - val_precision: 0.4505 - val_recall: 0.4032 - val_auc: 0.6084\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 1.20685\n",
      "Epoch 276/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8674 - tp: 1994.0000 - fp: 953.0000 - tn: 7975.0000 - fn: 2470.0000 - accuracy: 0.6062 - precision: 0.6766 - recall: 0.4467 - auc: 0.7845 - val_loss: 1.4196 - val_tp: 163.0000 - val_fp: 188.0000 - val_tn: 804.0000 - val_fn: 333.0000 - val_accuracy: 0.4294 - val_precision: 0.4644 - val_recall: 0.3286 - val_auc: 0.5909\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 1.20685\n",
      "Epoch 277/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8684 - tp: 1982.0000 - fp: 894.0000 - tn: 8034.0000 - fn: 2482.0000 - accuracy: 0.5974 - precision: 0.6892 - recall: 0.4440 - auc: 0.7841 - val_loss: 2.3301 - val_tp: 208.0000 - val_fp: 281.0000 - val_tn: 711.0000 - val_fn: 288.0000 - val_accuracy: 0.4214 - val_precision: 0.4254 - val_recall: 0.4194 - val_auc: 0.5952\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 1.20685\n",
      "Epoch 278/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8668 - tp: 2027.0000 - fp: 935.0000 - tn: 7993.0000 - fn: 2437.0000 - accuracy: 0.6055 - precision: 0.6843 - recall: 0.4541 - auc: 0.7868 - val_loss: 2.1866 - val_tp: 228.0000 - val_fp: 261.0000 - val_tn: 731.0000 - val_fn: 268.0000 - val_accuracy: 0.4617 - val_precision: 0.4663 - val_recall: 0.4597 - val_auc: 0.6204\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 1.20685\n",
      "Epoch 279/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8757 - tp: 1975.0000 - fp: 972.0000 - tn: 7956.0000 - fn: 2489.0000 - accuracy: 0.5948 - precision: 0.6702 - recall: 0.4424 - auc: 0.7802 - val_loss: 1.9403 - val_tp: 172.0000 - val_fp: 279.0000 - val_tn: 713.0000 - val_fn: 324.0000 - val_accuracy: 0.3770 - val_precision: 0.3814 - val_recall: 0.3468 - val_auc: 0.5535\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 1.20685\n",
      "Epoch 280/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8716 - tp: 1966.0000 - fp: 946.0000 - tn: 7982.0000 - fn: 2498.0000 - accuracy: 0.6017 - precision: 0.6751 - recall: 0.4404 - auc: 0.7828 - val_loss: 1.7931 - val_tp: 193.0000 - val_fp: 250.0000 - val_tn: 742.0000 - val_fn: 303.0000 - val_accuracy: 0.4415 - val_precision: 0.4357 - val_recall: 0.3891 - val_auc: 0.5997\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 1.20685\n",
      "Epoch 281/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8744 - tp: 1984.0000 - fp: 913.0000 - tn: 8015.0000 - fn: 2480.0000 - accuracy: 0.5966 - precision: 0.6848 - recall: 0.4444 - auc: 0.7822 - val_loss: 2.0949 - val_tp: 197.0000 - val_fp: 290.0000 - val_tn: 702.0000 - val_fn: 299.0000 - val_accuracy: 0.4012 - val_precision: 0.4045 - val_recall: 0.3972 - val_auc: 0.5888\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 1.20685\n",
      "Epoch 282/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8564 - tp: 2092.0000 - fp: 925.0000 - tn: 8003.0000 - fn: 2372.0000 - accuracy: 0.6176 - precision: 0.6934 - recall: 0.4686 - auc: 0.7930 - val_loss: 2.1566 - val_tp: 191.0000 - val_fp: 291.0000 - val_tn: 701.0000 - val_fn: 305.0000 - val_accuracy: 0.3911 - val_precision: 0.3963 - val_recall: 0.3851 - val_auc: 0.5703\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 1.20685\n",
      "Epoch 283/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8740 - tp: 1961.0000 - fp: 968.0000 - tn: 7960.0000 - fn: 2503.0000 - accuracy: 0.5943 - precision: 0.6695 - recall: 0.4393 - auc: 0.7809 - val_loss: 1.8553 - val_tp: 168.0000 - val_fp: 310.0000 - val_tn: 682.0000 - val_fn: 328.0000 - val_accuracy: 0.3488 - val_precision: 0.3515 - val_recall: 0.3387 - val_auc: 0.5607\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 1.20685\n",
      "Epoch 284/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8773 - tp: 1950.0000 - fp: 929.0000 - tn: 7999.0000 - fn: 2514.0000 - accuracy: 0.6004 - precision: 0.6773 - recall: 0.4368 - auc: 0.7805 - val_loss: 1.7827 - val_tp: 197.0000 - val_fp: 279.0000 - val_tn: 713.0000 - val_fn: 299.0000 - val_accuracy: 0.4153 - val_precision: 0.4139 - val_recall: 0.3972 - val_auc: 0.5861\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 1.20685\n",
      "Epoch 285/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8675 - tp: 1960.0000 - fp: 927.0000 - tn: 8001.0000 - fn: 2504.0000 - accuracy: 0.5939 - precision: 0.6789 - recall: 0.4391 - auc: 0.7849 - val_loss: 2.0373 - val_tp: 185.0000 - val_fp: 292.0000 - val_tn: 700.0000 - val_fn: 311.0000 - val_accuracy: 0.3871 - val_precision: 0.3878 - val_recall: 0.3730 - val_auc: 0.5750\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 1.20685\n",
      "Epoch 286/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8765 - tp: 1960.0000 - fp: 920.0000 - tn: 8008.0000 - fn: 2504.0000 - accuracy: 0.5923 - precision: 0.6806 - recall: 0.4391 - auc: 0.7802 - val_loss: 2.2701 - val_tp: 185.0000 - val_fp: 297.0000 - val_tn: 695.0000 - val_fn: 311.0000 - val_accuracy: 0.3851 - val_precision: 0.3838 - val_recall: 0.3730 - val_auc: 0.5786\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 1.20685\n",
      "Epoch 287/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8531 - tp: 2034.0000 - fp: 896.0000 - tn: 8032.0000 - fn: 2430.0000 - accuracy: 0.6116 - precision: 0.6942 - recall: 0.4556 - auc: 0.7961 - val_loss: 1.5379 - val_tp: 191.0000 - val_fp: 226.0000 - val_tn: 766.0000 - val_fn: 305.0000 - val_accuracy: 0.4456 - val_precision: 0.4580 - val_recall: 0.3851 - val_auc: 0.5967\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 1.20685\n",
      "Epoch 288/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8570 - tp: 2032.0000 - fp: 965.0000 - tn: 7963.0000 - fn: 2432.0000 - accuracy: 0.6042 - precision: 0.6780 - recall: 0.4552 - auc: 0.7900 - val_loss: 1.8768 - val_tp: 209.0000 - val_fp: 271.0000 - val_tn: 721.0000 - val_fn: 287.0000 - val_accuracy: 0.4375 - val_precision: 0.4354 - val_recall: 0.4214 - val_auc: 0.6101\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 1.20685\n",
      "Epoch 289/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8690 - tp: 2002.0000 - fp: 966.0000 - tn: 7962.0000 - fn: 2462.0000 - accuracy: 0.5972 - precision: 0.6745 - recall: 0.4485 - auc: 0.7838 - val_loss: 1.7877 - val_tp: 214.0000 - val_fp: 262.0000 - val_tn: 730.0000 - val_fn: 282.0000 - val_accuracy: 0.4395 - val_precision: 0.4496 - val_recall: 0.4315 - val_auc: 0.6021\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 1.20685\n",
      "Epoch 290/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8633 - tp: 1990.0000 - fp: 943.0000 - tn: 7985.0000 - fn: 2474.0000 - accuracy: 0.6044 - precision: 0.6785 - recall: 0.4458 - auc: 0.7881 - val_loss: 2.3325 - val_tp: 205.0000 - val_fp: 287.0000 - val_tn: 705.0000 - val_fn: 291.0000 - val_accuracy: 0.4173 - val_precision: 0.4167 - val_recall: 0.4133 - val_auc: 0.5928\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 1.20685\n",
      "Epoch 291/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8651 - tp: 1985.0000 - fp: 942.0000 - tn: 7986.0000 - fn: 2479.0000 - accuracy: 0.6057 - precision: 0.6782 - recall: 0.4447 - auc: 0.7858 - val_loss: 1.7464 - val_tp: 221.0000 - val_fp: 243.0000 - val_tn: 749.0000 - val_fn: 275.0000 - val_accuracy: 0.4698 - val_precision: 0.4763 - val_recall: 0.4456 - val_auc: 0.6280\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 1.20685\n",
      "Epoch 292/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8661 - tp: 2024.0000 - fp: 951.0000 - tn: 7977.0000 - fn: 2440.0000 - accuracy: 0.5990 - precision: 0.6803 - recall: 0.4534 - auc: 0.7855 - val_loss: 1.9443 - val_tp: 174.0000 - val_fp: 284.0000 - val_tn: 708.0000 - val_fn: 322.0000 - val_accuracy: 0.3669 - val_precision: 0.3799 - val_recall: 0.3508 - val_auc: 0.5421\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 1.20685\n",
      "Epoch 293/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8569 - tp: 2046.0000 - fp: 946.0000 - tn: 7982.0000 - fn: 2418.0000 - accuracy: 0.6064 - precision: 0.6838 - recall: 0.4583 - auc: 0.7905 - val_loss: 1.9046 - val_tp: 218.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 278.0000 - val_accuracy: 0.4415 - val_precision: 0.4561 - val_recall: 0.4395 - val_auc: 0.6152\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 1.20685\n",
      "Epoch 294/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8497 - tp: 2104.0000 - fp: 960.0000 - tn: 7968.0000 - fn: 2360.0000 - accuracy: 0.6084 - precision: 0.6867 - recall: 0.4713 - auc: 0.7949 - val_loss: 2.2701 - val_tp: 200.0000 - val_fp: 283.0000 - val_tn: 709.0000 - val_fn: 296.0000 - val_accuracy: 0.4153 - val_precision: 0.4141 - val_recall: 0.4032 - val_auc: 0.5772\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 1.20685\n",
      "Epoch 295/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8590 - tp: 2061.0000 - fp: 944.0000 - tn: 7984.0000 - fn: 2403.0000 - accuracy: 0.6122 - precision: 0.6859 - recall: 0.4617 - auc: 0.7923 - val_loss: 1.9800 - val_tp: 203.0000 - val_fp: 275.0000 - val_tn: 717.0000 - val_fn: 293.0000 - val_accuracy: 0.4214 - val_precision: 0.4247 - val_recall: 0.4093 - val_auc: 0.5977\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 1.20685\n",
      "Epoch 296/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8582 - tp: 2024.0000 - fp: 945.0000 - tn: 7983.0000 - fn: 2440.0000 - accuracy: 0.6073 - precision: 0.6817 - recall: 0.4534 - auc: 0.7919 - val_loss: 1.9050 - val_tp: 194.0000 - val_fp: 272.0000 - val_tn: 720.0000 - val_fn: 302.0000 - val_accuracy: 0.4113 - val_precision: 0.4163 - val_recall: 0.3911 - val_auc: 0.5890\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 1.20685\n",
      "Epoch 297/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8649 - tp: 2050.0000 - fp: 973.0000 - tn: 7955.0000 - fn: 2414.0000 - accuracy: 0.6006 - precision: 0.6781 - recall: 0.4592 - auc: 0.7859 - val_loss: 1.8508 - val_tp: 168.0000 - val_fp: 286.0000 - val_tn: 706.0000 - val_fn: 328.0000 - val_accuracy: 0.3750 - val_precision: 0.3700 - val_recall: 0.3387 - val_auc: 0.5562\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 1.20685\n",
      "Epoch 298/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8635 - tp: 2021.0000 - fp: 903.0000 - tn: 8025.0000 - fn: 2443.0000 - accuracy: 0.6042 - precision: 0.6912 - recall: 0.4527 - auc: 0.7889 - val_loss: 1.8805 - val_tp: 201.0000 - val_fp: 263.0000 - val_tn: 729.0000 - val_fn: 295.0000 - val_accuracy: 0.4274 - val_precision: 0.4332 - val_recall: 0.4052 - val_auc: 0.5987\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 1.20685\n",
      "Epoch 299/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8599 - tp: 2083.0000 - fp: 942.0000 - tn: 7986.0000 - fn: 2381.0000 - accuracy: 0.6089 - precision: 0.6886 - recall: 0.4666 - auc: 0.7904 - val_loss: 1.9106 - val_tp: 202.0000 - val_fp: 275.0000 - val_tn: 717.0000 - val_fn: 294.0000 - val_accuracy: 0.4194 - val_precision: 0.4235 - val_recall: 0.4073 - val_auc: 0.5866\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 1.20685\n",
      "Epoch 300/500\n",
      "4464/4464 [==============================] - 0s 73us/step - loss: 0.8602 - tp: 2081.0000 - fp: 972.0000 - tn: 7956.0000 - fn: 2383.0000 - accuracy: 0.6026 - precision: 0.6816 - recall: 0.4662 - auc: 0.7888 - val_loss: 1.4580 - val_tp: 183.0000 - val_fp: 232.0000 - val_tn: 760.0000 - val_fn: 313.0000 - val_accuracy: 0.4274 - val_precision: 0.4410 - val_recall: 0.3690 - val_auc: 0.5980\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 1.20685\n",
      "Epoch 301/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8594 - tp: 2047.0000 - fp: 953.0000 - tn: 7975.0000 - fn: 2417.0000 - accuracy: 0.6057 - precision: 0.6823 - recall: 0.4586 - auc: 0.7909 - val_loss: 2.6603 - val_tp: 178.0000 - val_fp: 308.0000 - val_tn: 684.0000 - val_fn: 318.0000 - val_accuracy: 0.3690 - val_precision: 0.3663 - val_recall: 0.3589 - val_auc: 0.5540\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 1.20685\n",
      "Epoch 302/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8594 - tp: 2028.0000 - fp: 926.0000 - tn: 8002.0000 - fn: 2436.0000 - accuracy: 0.6026 - precision: 0.6865 - recall: 0.4543 - auc: 0.7901 - val_loss: 1.5490 - val_tp: 191.0000 - val_fp: 228.0000 - val_tn: 764.0000 - val_fn: 305.0000 - val_accuracy: 0.4274 - val_precision: 0.4558 - val_recall: 0.3851 - val_auc: 0.5840\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 1.20685\n",
      "Epoch 303/500\n",
      "4464/4464 [==============================] - 0s 72us/step - loss: 0.8672 - tp: 2027.0000 - fp: 948.0000 - tn: 7980.0000 - fn: 2437.0000 - accuracy: 0.6010 - precision: 0.6813 - recall: 0.4541 - auc: 0.7847 - val_loss: 2.2661 - val_tp: 191.0000 - val_fp: 293.0000 - val_tn: 699.0000 - val_fn: 305.0000 - val_accuracy: 0.3972 - val_precision: 0.3946 - val_recall: 0.3851 - val_auc: 0.5664\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 1.20685\n",
      "Epoch 304/500\n",
      "4464/4464 [==============================] - 0s 71us/step - loss: 0.8555 - tp: 2062.0000 - fp: 911.0000 - tn: 8017.0000 - fn: 2402.0000 - accuracy: 0.6073 - precision: 0.6936 - recall: 0.4619 - auc: 0.7926 - val_loss: 2.6199 - val_tp: 152.0000 - val_fp: 324.0000 - val_tn: 668.0000 - val_fn: 344.0000 - val_accuracy: 0.3145 - val_precision: 0.3193 - val_recall: 0.3065 - val_auc: 0.4966\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 1.20685\n",
      "Epoch 305/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8662 - tp: 2011.0000 - fp: 962.0000 - tn: 7966.0000 - fn: 2453.0000 - accuracy: 0.6026 - precision: 0.6764 - recall: 0.4505 - auc: 0.7870 - val_loss: 1.9139 - val_tp: 205.0000 - val_fp: 270.0000 - val_tn: 722.0000 - val_fn: 291.0000 - val_accuracy: 0.4274 - val_precision: 0.4316 - val_recall: 0.4133 - val_auc: 0.5910\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 1.20685\n",
      "Epoch 306/500\n",
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8602 - tp: 2005.0000 - fp: 964.0000 - tn: 7964.0000 - fn: 2459.0000 - accuracy: 0.6066 - precision: 0.6753 - recall: 0.4491 - auc: 0.7882 - val_loss: 1.6664 - val_tp: 215.0000 - val_fp: 257.0000 - val_tn: 735.0000 - val_fn: 281.0000 - val_accuracy: 0.4456 - val_precision: 0.4555 - val_recall: 0.4335 - val_auc: 0.6089\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 1.20685\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8502 - tp: 2083.0000 - fp: 915.0000 - tn: 8013.0000 - fn: 2381.0000 - accuracy: 0.6138 - precision: 0.6948 - recall: 0.4666 - auc: 0.7960 - val_loss: 1.9121 - val_tp: 173.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 323.0000 - val_accuracy: 0.4153 - val_precision: 0.4071 - val_recall: 0.3488 - val_auc: 0.5454\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 1.20685\n",
      "Epoch 308/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8521 - tp: 2035.0000 - fp: 960.0000 - tn: 7968.0000 - fn: 2429.0000 - accuracy: 0.6042 - precision: 0.6795 - recall: 0.4559 - auc: 0.7926 - val_loss: 2.2565 - val_tp: 208.0000 - val_fp: 280.0000 - val_tn: 712.0000 - val_fn: 288.0000 - val_accuracy: 0.4234 - val_precision: 0.4262 - val_recall: 0.4194 - val_auc: 0.5783\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 1.20685\n",
      "Epoch 309/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8558 - tp: 2050.0000 - fp: 970.0000 - tn: 7958.0000 - fn: 2414.0000 - accuracy: 0.6048 - precision: 0.6788 - recall: 0.4592 - auc: 0.7908 - val_loss: 1.6693 - val_tp: 169.0000 - val_fp: 236.0000 - val_tn: 756.0000 - val_fn: 327.0000 - val_accuracy: 0.4052 - val_precision: 0.4173 - val_recall: 0.3407 - val_auc: 0.5778\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 1.20685\n",
      "Epoch 310/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8500 - tp: 2108.0000 - fp: 927.0000 - tn: 8001.0000 - fn: 2356.0000 - accuracy: 0.6147 - precision: 0.6946 - recall: 0.4722 - auc: 0.7955 - val_loss: 1.8906 - val_tp: 217.0000 - val_fp: 263.0000 - val_tn: 729.0000 - val_fn: 279.0000 - val_accuracy: 0.4435 - val_precision: 0.4521 - val_recall: 0.4375 - val_auc: 0.6034\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 1.20685\n",
      "Epoch 311/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8649 - tp: 2016.0000 - fp: 982.0000 - tn: 7946.0000 - fn: 2448.0000 - accuracy: 0.5970 - precision: 0.6724 - recall: 0.4516 - auc: 0.7862 - val_loss: 2.1067 - val_tp: 188.0000 - val_fp: 293.0000 - val_tn: 699.0000 - val_fn: 308.0000 - val_accuracy: 0.3911 - val_precision: 0.3909 - val_recall: 0.3790 - val_auc: 0.5626\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 1.20685\n",
      "Epoch 312/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8473 - tp: 2067.0000 - fp: 937.0000 - tn: 7991.0000 - fn: 2397.0000 - accuracy: 0.6107 - precision: 0.6881 - recall: 0.4630 - auc: 0.7960 - val_loss: 1.9013 - val_tp: 176.0000 - val_fp: 274.0000 - val_tn: 718.0000 - val_fn: 320.0000 - val_accuracy: 0.3831 - val_precision: 0.3911 - val_recall: 0.3548 - val_auc: 0.5638\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 1.20685\n",
      "Epoch 313/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8474 - tp: 2057.0000 - fp: 898.0000 - tn: 8030.0000 - fn: 2407.0000 - accuracy: 0.6142 - precision: 0.6961 - recall: 0.4608 - auc: 0.7958 - val_loss: 1.7414 - val_tp: 192.0000 - val_fp: 232.0000 - val_tn: 760.0000 - val_fn: 304.0000 - val_accuracy: 0.4415 - val_precision: 0.4528 - val_recall: 0.3871 - val_auc: 0.5796\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 1.20685\n",
      "Epoch 314/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8543 - tp: 2078.0000 - fp: 959.0000 - tn: 7969.0000 - fn: 2386.0000 - accuracy: 0.6080 - precision: 0.6842 - recall: 0.4655 - auc: 0.7922 - val_loss: 2.7970 - val_tp: 192.0000 - val_fp: 299.0000 - val_tn: 693.0000 - val_fn: 304.0000 - val_accuracy: 0.3911 - val_precision: 0.3910 - val_recall: 0.3871 - val_auc: 0.5809\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 1.20685\n",
      "Epoch 315/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8644 - tp: 2017.0000 - fp: 926.0000 - tn: 8002.0000 - fn: 2447.0000 - accuracy: 0.6057 - precision: 0.6854 - recall: 0.4518 - auc: 0.7866 - val_loss: 1.5810 - val_tp: 187.0000 - val_fp: 228.0000 - val_tn: 764.0000 - val_fn: 309.0000 - val_accuracy: 0.4355 - val_precision: 0.4506 - val_recall: 0.3770 - val_auc: 0.5904\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 1.20685\n",
      "Epoch 316/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8545 - tp: 2064.0000 - fp: 929.0000 - tn: 7999.0000 - fn: 2400.0000 - accuracy: 0.6190 - precision: 0.6896 - recall: 0.4624 - auc: 0.7953 - val_loss: 1.6950 - val_tp: 213.0000 - val_fp: 262.0000 - val_tn: 730.0000 - val_fn: 283.0000 - val_accuracy: 0.4435 - val_precision: 0.4484 - val_recall: 0.4294 - val_auc: 0.6019\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 1.20685\n",
      "Epoch 317/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8598 - tp: 2053.0000 - fp: 916.0000 - tn: 8012.0000 - fn: 2411.0000 - accuracy: 0.6086 - precision: 0.6915 - recall: 0.4599 - auc: 0.7902 - val_loss: 2.0326 - val_tp: 182.0000 - val_fp: 303.0000 - val_tn: 689.0000 - val_fn: 314.0000 - val_accuracy: 0.3710 - val_precision: 0.3753 - val_recall: 0.3669 - val_auc: 0.5652\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 1.20685\n",
      "Epoch 318/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8696 - tp: 2003.0000 - fp: 928.0000 - tn: 8000.0000 - fn: 2461.0000 - accuracy: 0.5979 - precision: 0.6834 - recall: 0.4487 - auc: 0.7836 - val_loss: 1.5838 - val_tp: 192.0000 - val_fp: 242.0000 - val_tn: 750.0000 - val_fn: 304.0000 - val_accuracy: 0.4355 - val_precision: 0.4424 - val_recall: 0.3871 - val_auc: 0.5879\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 1.20685\n",
      "Epoch 319/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8514 - tp: 2086.0000 - fp: 919.0000 - tn: 8009.0000 - fn: 2378.0000 - accuracy: 0.6140 - precision: 0.6942 - recall: 0.4673 - auc: 0.7955 - val_loss: 1.4313 - val_tp: 166.0000 - val_fp: 205.0000 - val_tn: 787.0000 - val_fn: 330.0000 - val_accuracy: 0.4234 - val_precision: 0.4474 - val_recall: 0.3347 - val_auc: 0.5936\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 1.20685\n",
      "Epoch 320/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8527 - tp: 2087.0000 - fp: 950.0000 - tn: 7978.0000 - fn: 2377.0000 - accuracy: 0.6190 - precision: 0.6872 - recall: 0.4675 - auc: 0.7939 - val_loss: 1.6455 - val_tp: 179.0000 - val_fp: 278.0000 - val_tn: 714.0000 - val_fn: 317.0000 - val_accuracy: 0.3851 - val_precision: 0.3917 - val_recall: 0.3609 - val_auc: 0.5654\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 1.20685\n",
      "Epoch 321/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8447 - tp: 2104.0000 - fp: 918.0000 - tn: 8010.0000 - fn: 2360.0000 - accuracy: 0.6185 - precision: 0.6962 - recall: 0.4713 - auc: 0.7979 - val_loss: 1.6924 - val_tp: 205.0000 - val_fp: 230.0000 - val_tn: 762.0000 - val_fn: 291.0000 - val_accuracy: 0.4637 - val_precision: 0.4713 - val_recall: 0.4133 - val_auc: 0.6183\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 1.20685\n",
      "Epoch 322/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8507 - tp: 2074.0000 - fp: 897.0000 - tn: 8031.0000 - fn: 2390.0000 - accuracy: 0.6122 - precision: 0.6981 - recall: 0.4646 - auc: 0.7949 - val_loss: 2.1432 - val_tp: 196.0000 - val_fp: 283.0000 - val_tn: 709.0000 - val_fn: 300.0000 - val_accuracy: 0.4113 - val_precision: 0.4092 - val_recall: 0.3952 - val_auc: 0.5903\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 1.20685\n",
      "Epoch 323/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8571 - tp: 2064.0000 - fp: 959.0000 - tn: 7969.0000 - fn: 2400.0000 - accuracy: 0.6055 - precision: 0.6828 - recall: 0.4624 - auc: 0.7893 - val_loss: 2.4689 - val_tp: 175.0000 - val_fp: 314.0000 - val_tn: 678.0000 - val_fn: 321.0000 - val_accuracy: 0.3548 - val_precision: 0.3579 - val_recall: 0.3528 - val_auc: 0.5410\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 1.20685\n",
      "Epoch 324/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8549 - tp: 2031.0000 - fp: 929.0000 - tn: 7999.0000 - fn: 2433.0000 - accuracy: 0.6129 - precision: 0.6861 - recall: 0.4550 - auc: 0.7917 - val_loss: 1.8584 - val_tp: 198.0000 - val_fp: 246.0000 - val_tn: 746.0000 - val_fn: 298.0000 - val_accuracy: 0.4375 - val_precision: 0.4459 - val_recall: 0.3992 - val_auc: 0.6045\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 1.20685\n",
      "Epoch 325/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8577 - tp: 2048.0000 - fp: 953.0000 - tn: 7975.0000 - fn: 2416.0000 - accuracy: 0.6091 - precision: 0.6824 - recall: 0.4588 - auc: 0.7907 - val_loss: 1.5922 - val_tp: 179.0000 - val_fp: 220.0000 - val_tn: 772.0000 - val_fn: 317.0000 - val_accuracy: 0.4335 - val_precision: 0.4486 - val_recall: 0.3609 - val_auc: 0.6018\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 1.20685\n",
      "Epoch 326/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8502 - tp: 2096.0000 - fp: 914.0000 - tn: 8014.0000 - fn: 2368.0000 - accuracy: 0.6107 - precision: 0.6963 - recall: 0.4695 - auc: 0.7964 - val_loss: 1.5014 - val_tp: 192.0000 - val_fp: 236.0000 - val_tn: 756.0000 - val_fn: 304.0000 - val_accuracy: 0.4294 - val_precision: 0.4486 - val_recall: 0.3871 - val_auc: 0.5887\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 1.20685\n",
      "Epoch 327/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8630 - tp: 2043.0000 - fp: 950.0000 - tn: 7978.0000 - fn: 2421.0000 - accuracy: 0.6055 - precision: 0.6826 - recall: 0.4577 - auc: 0.7890 - val_loss: 2.0938 - val_tp: 225.0000 - val_fp: 259.0000 - val_tn: 733.0000 - val_fn: 271.0000 - val_accuracy: 0.4577 - val_precision: 0.4649 - val_recall: 0.4536 - val_auc: 0.6132\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 1.20685\n",
      "Epoch 328/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8368 - tp: 2131.0000 - fp: 940.0000 - tn: 7988.0000 - fn: 2333.0000 - accuracy: 0.6205 - precision: 0.6939 - recall: 0.4774 - auc: 0.8039 - val_loss: 1.6347 - val_tp: 179.0000 - val_fp: 263.0000 - val_tn: 729.0000 - val_fn: 317.0000 - val_accuracy: 0.4032 - val_precision: 0.4050 - val_recall: 0.3609 - val_auc: 0.5672\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 1.20685\n",
      "Epoch 329/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8526 - tp: 2050.0000 - fp: 960.0000 - tn: 7968.0000 - fn: 2414.0000 - accuracy: 0.6100 - precision: 0.6811 - recall: 0.4592 - auc: 0.7932 - val_loss: 1.8499 - val_tp: 217.0000 - val_fp: 240.0000 - val_tn: 752.0000 - val_fn: 279.0000 - val_accuracy: 0.4738 - val_precision: 0.4748 - val_recall: 0.4375 - val_auc: 0.6179\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 1.20685\n",
      "Epoch 330/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8497 - tp: 2083.0000 - fp: 943.0000 - tn: 7985.0000 - fn: 2381.0000 - accuracy: 0.6104 - precision: 0.6884 - recall: 0.4666 - auc: 0.7946 - val_loss: 1.6192 - val_tp: 197.0000 - val_fp: 231.0000 - val_tn: 761.0000 - val_fn: 299.0000 - val_accuracy: 0.4476 - val_precision: 0.4603 - val_recall: 0.3972 - val_auc: 0.6163\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 1.20685\n",
      "Epoch 331/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8439 - tp: 2114.0000 - fp: 984.0000 - tn: 7944.0000 - fn: 2350.0000 - accuracy: 0.6187 - precision: 0.6824 - recall: 0.4736 - auc: 0.7985 - val_loss: 1.9228 - val_tp: 214.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 282.0000 - val_accuracy: 0.4435 - val_precision: 0.4515 - val_recall: 0.4315 - val_auc: 0.6106\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 1.20685\n",
      "Epoch 332/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8494 - tp: 2107.0000 - fp: 964.0000 - tn: 7964.0000 - fn: 2357.0000 - accuracy: 0.6169 - precision: 0.6861 - recall: 0.4720 - auc: 0.7966 - val_loss: 1.6016 - val_tp: 201.0000 - val_fp: 256.0000 - val_tn: 736.0000 - val_fn: 295.0000 - val_accuracy: 0.4294 - val_precision: 0.4398 - val_recall: 0.4052 - val_auc: 0.5947\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 1.20685\n",
      "Epoch 333/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8478 - tp: 2098.0000 - fp: 969.0000 - tn: 7959.0000 - fn: 2366.0000 - accuracy: 0.6102 - precision: 0.6841 - recall: 0.4700 - auc: 0.7969 - val_loss: 3.3304 - val_tp: 238.0000 - val_fp: 256.0000 - val_tn: 736.0000 - val_fn: 258.0000 - val_accuracy: 0.4819 - val_precision: 0.4818 - val_recall: 0.4798 - val_auc: 0.6237\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 1.20685\n",
      "Epoch 334/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8477 - tp: 2097.0000 - fp: 952.0000 - tn: 7976.0000 - fn: 2367.0000 - accuracy: 0.6095 - precision: 0.6878 - recall: 0.4698 - auc: 0.7951 - val_loss: 1.7750 - val_tp: 186.0000 - val_fp: 258.0000 - val_tn: 734.0000 - val_fn: 310.0000 - val_accuracy: 0.4153 - val_precision: 0.4189 - val_recall: 0.3750 - val_auc: 0.5851\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 1.20685\n",
      "Epoch 335/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8491 - tp: 2101.0000 - fp: 927.0000 - tn: 8001.0000 - fn: 2363.0000 - accuracy: 0.6192 - precision: 0.6939 - recall: 0.4707 - auc: 0.7968 - val_loss: 1.6988 - val_tp: 204.0000 - val_fp: 247.0000 - val_tn: 745.0000 - val_fn: 292.0000 - val_accuracy: 0.4456 - val_precision: 0.4523 - val_recall: 0.4113 - val_auc: 0.6086\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 1.20685\n",
      "Epoch 336/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8486 - tp: 2061.0000 - fp: 949.0000 - tn: 7979.0000 - fn: 2403.0000 - accuracy: 0.6120 - precision: 0.6847 - recall: 0.4617 - auc: 0.7952 - val_loss: 1.9999 - val_tp: 198.0000 - val_fp: 270.0000 - val_tn: 722.0000 - val_fn: 298.0000 - val_accuracy: 0.4214 - val_precision: 0.4231 - val_recall: 0.3992 - val_auc: 0.5850\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 1.20685\n",
      "Epoch 337/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8566 - tp: 2072.0000 - fp: 952.0000 - tn: 7976.0000 - fn: 2392.0000 - accuracy: 0.6046 - precision: 0.6852 - recall: 0.4642 - auc: 0.7916 - val_loss: 1.8967 - val_tp: 203.0000 - val_fp: 263.0000 - val_tn: 729.0000 - val_fn: 293.0000 - val_accuracy: 0.4415 - val_precision: 0.4356 - val_recall: 0.4093 - val_auc: 0.6104\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 1.20685\n",
      "Epoch 338/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8533 - tp: 2099.0000 - fp: 972.0000 - tn: 7956.0000 - fn: 2365.0000 - accuracy: 0.6062 - precision: 0.6835 - recall: 0.4702 - auc: 0.7927 - val_loss: 1.7638 - val_tp: 201.0000 - val_fp: 249.0000 - val_tn: 743.0000 - val_fn: 295.0000 - val_accuracy: 0.4254 - val_precision: 0.4467 - val_recall: 0.4052 - val_auc: 0.6027\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 1.20685\n",
      "Epoch 339/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8338 - tp: 2120.0000 - fp: 900.0000 - tn: 8028.0000 - fn: 2344.0000 - accuracy: 0.6254 - precision: 0.7020 - recall: 0.4749 - auc: 0.8062 - val_loss: 2.0616 - val_tp: 227.0000 - val_fp: 249.0000 - val_tn: 743.0000 - val_fn: 269.0000 - val_accuracy: 0.4738 - val_precision: 0.4769 - val_recall: 0.4577 - val_auc: 0.6153\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 1.20685\n",
      "Epoch 340/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8445 - tp: 2140.0000 - fp: 935.0000 - tn: 7993.0000 - fn: 2324.0000 - accuracy: 0.6147 - precision: 0.6959 - recall: 0.4794 - auc: 0.7983 - val_loss: 1.9224 - val_tp: 216.0000 - val_fp: 270.0000 - val_tn: 722.0000 - val_fn: 280.0000 - val_accuracy: 0.4415 - val_precision: 0.4444 - val_recall: 0.4355 - val_auc: 0.6026\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 1.20685\n",
      "Epoch 341/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8460 - tp: 2099.0000 - fp: 939.0000 - tn: 7989.0000 - fn: 2365.0000 - accuracy: 0.6181 - precision: 0.6909 - recall: 0.4702 - auc: 0.7983 - val_loss: 1.8827 - val_tp: 218.0000 - val_fp: 268.0000 - val_tn: 724.0000 - val_fn: 278.0000 - val_accuracy: 0.4415 - val_precision: 0.4486 - val_recall: 0.4395 - val_auc: 0.6187\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 1.20685\n",
      "Epoch 342/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8548 - tp: 2106.0000 - fp: 960.0000 - tn: 7968.0000 - fn: 2358.0000 - accuracy: 0.6084 - precision: 0.6869 - recall: 0.4718 - auc: 0.7931 - val_loss: 2.0900 - val_tp: 210.0000 - val_fp: 278.0000 - val_tn: 714.0000 - val_fn: 286.0000 - val_accuracy: 0.4294 - val_precision: 0.4303 - val_recall: 0.4234 - val_auc: 0.6078\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 1.20685\n",
      "Epoch 343/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8497 - tp: 2110.0000 - fp: 911.0000 - tn: 8017.0000 - fn: 2354.0000 - accuracy: 0.6163 - precision: 0.6984 - recall: 0.4727 - auc: 0.7967 - val_loss: 1.6036 - val_tp: 172.0000 - val_fp: 227.0000 - val_tn: 765.0000 - val_fn: 324.0000 - val_accuracy: 0.4194 - val_precision: 0.4311 - val_recall: 0.3468 - val_auc: 0.5721\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 1.20685\n",
      "Epoch 344/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8617 - tp: 2107.0000 - fp: 997.0000 - tn: 7931.0000 - fn: 2357.0000 - accuracy: 0.6122 - precision: 0.6788 - recall: 0.4720 - auc: 0.7898 - val_loss: 1.7282 - val_tp: 181.0000 - val_fp: 272.0000 - val_tn: 720.0000 - val_fn: 315.0000 - val_accuracy: 0.3931 - val_precision: 0.3996 - val_recall: 0.3649 - val_auc: 0.5808\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 1.20685\n",
      "Epoch 345/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8486 - tp: 2092.0000 - fp: 999.0000 - tn: 7929.0000 - fn: 2372.0000 - accuracy: 0.6120 - precision: 0.6768 - recall: 0.4686 - auc: 0.7962 - val_loss: 1.6972 - val_tp: 188.0000 - val_fp: 256.0000 - val_tn: 736.0000 - val_fn: 308.0000 - val_accuracy: 0.4214 - val_precision: 0.4234 - val_recall: 0.3790 - val_auc: 0.5851\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 1.20685\n",
      "Epoch 346/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8554 - tp: 2115.0000 - fp: 977.0000 - tn: 7951.0000 - fn: 2349.0000 - accuracy: 0.6073 - precision: 0.6840 - recall: 0.4738 - auc: 0.7914 - val_loss: 2.1026 - val_tp: 175.0000 - val_fp: 308.0000 - val_tn: 684.0000 - val_fn: 321.0000 - val_accuracy: 0.3609 - val_precision: 0.3623 - val_recall: 0.3528 - val_auc: 0.5416\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 1.20685\n",
      "Epoch 347/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8392 - tp: 2127.0000 - fp: 945.0000 - tn: 7983.0000 - fn: 2337.0000 - accuracy: 0.6183 - precision: 0.6924 - recall: 0.4765 - auc: 0.8013 - val_loss: 1.8892 - val_tp: 221.0000 - val_fp: 259.0000 - val_tn: 733.0000 - val_fn: 275.0000 - val_accuracy: 0.4496 - val_precision: 0.4604 - val_recall: 0.4456 - val_auc: 0.6096\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 1.20685\n",
      "Epoch 348/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8433 - tp: 2133.0000 - fp: 924.0000 - tn: 8004.0000 - fn: 2331.0000 - accuracy: 0.6190 - precision: 0.6977 - recall: 0.4778 - auc: 0.8008 - val_loss: 1.6338 - val_tp: 188.0000 - val_fp: 242.0000 - val_tn: 750.0000 - val_fn: 308.0000 - val_accuracy: 0.4173 - val_precision: 0.4372 - val_recall: 0.3790 - val_auc: 0.5859\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 1.20685\n",
      "Epoch 349/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8402 - tp: 2133.0000 - fp: 965.0000 - tn: 7963.0000 - fn: 2331.0000 - accuracy: 0.6185 - precision: 0.6885 - recall: 0.4778 - auc: 0.8008 - val_loss: 2.1653 - val_tp: 191.0000 - val_fp: 285.0000 - val_tn: 707.0000 - val_fn: 305.0000 - val_accuracy: 0.3931 - val_precision: 0.4013 - val_recall: 0.3851 - val_auc: 0.5749\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 1.20685\n",
      "Epoch 350/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8498 - tp: 2105.0000 - fp: 909.0000 - tn: 8019.0000 - fn: 2359.0000 - accuracy: 0.6066 - precision: 0.6984 - recall: 0.4716 - auc: 0.7956 - val_loss: 1.4554 - val_tp: 107.0000 - val_fp: 205.0000 - val_tn: 787.0000 - val_fn: 389.0000 - val_accuracy: 0.3589 - val_precision: 0.3429 - val_recall: 0.2157 - val_auc: 0.5340\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 1.20685\n",
      "Epoch 351/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8529 - tp: 2089.0000 - fp: 1002.0000 - tn: 7926.0000 - fn: 2375.0000 - accuracy: 0.6138 - precision: 0.6758 - recall: 0.4680 - auc: 0.7926 - val_loss: 2.0399 - val_tp: 183.0000 - val_fp: 295.0000 - val_tn: 697.0000 - val_fn: 313.0000 - val_accuracy: 0.3770 - val_precision: 0.3828 - val_recall: 0.3690 - val_auc: 0.5622\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 1.20685\n",
      "Epoch 352/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8601 - tp: 2075.0000 - fp: 979.0000 - tn: 7949.0000 - fn: 2389.0000 - accuracy: 0.6022 - precision: 0.6794 - recall: 0.4648 - auc: 0.7888 - val_loss: 1.6917 - val_tp: 198.0000 - val_fp: 227.0000 - val_tn: 765.0000 - val_fn: 298.0000 - val_accuracy: 0.4496 - val_precision: 0.4659 - val_recall: 0.3992 - val_auc: 0.6129\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 1.20685\n",
      "Epoch 353/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8538 - tp: 2094.0000 - fp: 974.0000 - tn: 7954.0000 - fn: 2370.0000 - accuracy: 0.6134 - precision: 0.6825 - recall: 0.4691 - auc: 0.7937 - val_loss: 2.0191 - val_tp: 198.0000 - val_fp: 258.0000 - val_tn: 734.0000 - val_fn: 298.0000 - val_accuracy: 0.4315 - val_precision: 0.4342 - val_recall: 0.3992 - val_auc: 0.5718\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 1.20685\n",
      "Epoch 354/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8483 - tp: 2068.0000 - fp: 948.0000 - tn: 7980.0000 - fn: 2396.0000 - accuracy: 0.6118 - precision: 0.6857 - recall: 0.4633 - auc: 0.7956 - val_loss: 1.7081 - val_tp: 210.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 286.0000 - val_accuracy: 0.4355 - val_precision: 0.4468 - val_recall: 0.4234 - val_auc: 0.6029\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 1.20685\n",
      "Epoch 355/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8522 - tp: 2096.0000 - fp: 963.0000 - tn: 7965.0000 - fn: 2368.0000 - accuracy: 0.6093 - precision: 0.6852 - recall: 0.4695 - auc: 0.7932 - val_loss: 2.7563 - val_tp: 171.0000 - val_fp: 323.0000 - val_tn: 669.0000 - val_fn: 325.0000 - val_accuracy: 0.3448 - val_precision: 0.3462 - val_recall: 0.3448 - val_auc: 0.5594\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 1.20685\n",
      "Epoch 356/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8510 - tp: 2100.0000 - fp: 939.0000 - tn: 7989.0000 - fn: 2364.0000 - accuracy: 0.6142 - precision: 0.6910 - recall: 0.4704 - auc: 0.7944 - val_loss: 2.1502 - val_tp: 205.0000 - val_fp: 285.0000 - val_tn: 707.0000 - val_fn: 291.0000 - val_accuracy: 0.4153 - val_precision: 0.4184 - val_recall: 0.4133 - val_auc: 0.5926\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 1.20685\n",
      "Epoch 357/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8489 - tp: 2090.0000 - fp: 963.0000 - tn: 7965.0000 - fn: 2374.0000 - accuracy: 0.6073 - precision: 0.6846 - recall: 0.4682 - auc: 0.7960 - val_loss: 2.0563 - val_tp: 195.0000 - val_fp: 291.0000 - val_tn: 701.0000 - val_fn: 301.0000 - val_accuracy: 0.3972 - val_precision: 0.4012 - val_recall: 0.3931 - val_auc: 0.5685\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 1.20685\n",
      "Epoch 358/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8415 - tp: 2083.0000 - fp: 934.0000 - tn: 7994.0000 - fn: 2381.0000 - accuracy: 0.6136 - precision: 0.6904 - recall: 0.4666 - auc: 0.7985 - val_loss: 2.2374 - val_tp: 221.0000 - val_fp: 250.0000 - val_tn: 742.0000 - val_fn: 275.0000 - val_accuracy: 0.4617 - val_precision: 0.4692 - val_recall: 0.4456 - val_auc: 0.6160\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 1.20685\n",
      "Epoch 359/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8484 - tp: 2127.0000 - fp: 952.0000 - tn: 7976.0000 - fn: 2337.0000 - accuracy: 0.6192 - precision: 0.6908 - recall: 0.4765 - auc: 0.7968 - val_loss: 1.9885 - val_tp: 161.0000 - val_fp: 295.0000 - val_tn: 697.0000 - val_fn: 335.0000 - val_accuracy: 0.3548 - val_precision: 0.3531 - val_recall: 0.3246 - val_auc: 0.5370\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 1.20685\n",
      "Epoch 360/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8574 - tp: 2094.0000 - fp: 968.0000 - tn: 7960.0000 - fn: 2370.0000 - accuracy: 0.6120 - precision: 0.6839 - recall: 0.4691 - auc: 0.7921 - val_loss: 1.7762 - val_tp: 204.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 292.0000 - val_accuracy: 0.4335 - val_precision: 0.4397 - val_recall: 0.4113 - val_auc: 0.5738\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 1.20685\n",
      "Epoch 361/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8361 - tp: 2109.0000 - fp: 925.0000 - tn: 8003.0000 - fn: 2355.0000 - accuracy: 0.6149 - precision: 0.6951 - recall: 0.4724 - auc: 0.8022 - val_loss: 1.5207 - val_tp: 206.0000 - val_fp: 235.0000 - val_tn: 757.0000 - val_fn: 290.0000 - val_accuracy: 0.4617 - val_precision: 0.4671 - val_recall: 0.4153 - val_auc: 0.6302\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 1.20685\n",
      "Epoch 362/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8408 - tp: 2180.0000 - fp: 932.0000 - tn: 7996.0000 - fn: 2284.0000 - accuracy: 0.6216 - precision: 0.7005 - recall: 0.4884 - auc: 0.8020 - val_loss: 2.0597 - val_tp: 174.0000 - val_fp: 306.0000 - val_tn: 686.0000 - val_fn: 322.0000 - val_accuracy: 0.3649 - val_precision: 0.3625 - val_recall: 0.3508 - val_auc: 0.5513\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 1.20685\n",
      "Epoch 363/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8499 - tp: 2116.0000 - fp: 910.0000 - tn: 8018.0000 - fn: 2348.0000 - accuracy: 0.6149 - precision: 0.6993 - recall: 0.4740 - auc: 0.7950 - val_loss: 1.8793 - val_tp: 213.0000 - val_fp: 270.0000 - val_tn: 722.0000 - val_fn: 283.0000 - val_accuracy: 0.4375 - val_precision: 0.4410 - val_recall: 0.4294 - val_auc: 0.5887\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 1.20685\n",
      "Epoch 364/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8377 - tp: 2135.0000 - fp: 904.0000 - tn: 8024.0000 - fn: 2329.0000 - accuracy: 0.6223 - precision: 0.7025 - recall: 0.4783 - auc: 0.8028 - val_loss: 1.6325 - val_tp: 212.0000 - val_fp: 243.0000 - val_tn: 749.0000 - val_fn: 284.0000 - val_accuracy: 0.4516 - val_precision: 0.4659 - val_recall: 0.4274 - val_auc: 0.6146\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 1.20685\n",
      "Epoch 365/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8448 - tp: 2115.0000 - fp: 944.0000 - tn: 7984.0000 - fn: 2349.0000 - accuracy: 0.6190 - precision: 0.6914 - recall: 0.4738 - auc: 0.8001 - val_loss: 2.5816 - val_tp: 221.0000 - val_fp: 267.0000 - val_tn: 725.0000 - val_fn: 275.0000 - val_accuracy: 0.4496 - val_precision: 0.4529 - val_recall: 0.4456 - val_auc: 0.6096\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 1.20685\n",
      "Epoch 366/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8416 - tp: 2130.0000 - fp: 1004.0000 - tn: 7924.0000 - fn: 2334.0000 - accuracy: 0.6116 - precision: 0.6796 - recall: 0.4772 - auc: 0.7994 - val_loss: 1.8907 - val_tp: 170.0000 - val_fp: 285.0000 - val_tn: 707.0000 - val_fn: 326.0000 - val_accuracy: 0.3649 - val_precision: 0.3736 - val_recall: 0.3427 - val_auc: 0.5396\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 1.20685\n",
      "Epoch 367/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8419 - tp: 2143.0000 - fp: 956.0000 - tn: 7972.0000 - fn: 2321.0000 - accuracy: 0.6239 - precision: 0.6915 - recall: 0.4801 - auc: 0.7996 - val_loss: 1.6008 - val_tp: 217.0000 - val_fp: 242.0000 - val_tn: 750.0000 - val_fn: 279.0000 - val_accuracy: 0.4718 - val_precision: 0.4728 - val_recall: 0.4375 - val_auc: 0.6182\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 1.20685\n",
      "Epoch 368/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8454 - tp: 2113.0000 - fp: 947.0000 - tn: 7981.0000 - fn: 2351.0000 - accuracy: 0.6140 - precision: 0.6905 - recall: 0.4733 - auc: 0.7975 - val_loss: 1.9902 - val_tp: 226.0000 - val_fp: 252.0000 - val_tn: 740.0000 - val_fn: 270.0000 - val_accuracy: 0.4617 - val_precision: 0.4728 - val_recall: 0.4556 - val_auc: 0.6257\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 1.20685\n",
      "Epoch 369/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8463 - tp: 2149.0000 - fp: 989.0000 - tn: 7939.0000 - fn: 2315.0000 - accuracy: 0.6122 - precision: 0.6848 - recall: 0.4814 - auc: 0.7971 - val_loss: 1.7547 - val_tp: 213.0000 - val_fp: 242.0000 - val_tn: 750.0000 - val_fn: 283.0000 - val_accuracy: 0.4798 - val_precision: 0.4681 - val_recall: 0.4294 - val_auc: 0.6051\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 1.20685\n",
      "Epoch 370/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8512 - tp: 2093.0000 - fp: 978.0000 - tn: 7950.0000 - fn: 2371.0000 - accuracy: 0.6060 - precision: 0.6815 - recall: 0.4689 - auc: 0.7944 - val_loss: 1.5824 - val_tp: 189.0000 - val_fp: 220.0000 - val_tn: 772.0000 - val_fn: 307.0000 - val_accuracy: 0.4698 - val_precision: 0.4621 - val_recall: 0.3810 - val_auc: 0.6037\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 1.20685\n",
      "Epoch 371/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8426 - tp: 2135.0000 - fp: 948.0000 - tn: 7980.0000 - fn: 2329.0000 - accuracy: 0.6185 - precision: 0.6925 - recall: 0.4783 - auc: 0.7986 - val_loss: 1.7812 - val_tp: 191.0000 - val_fp: 258.0000 - val_tn: 734.0000 - val_fn: 305.0000 - val_accuracy: 0.4173 - val_precision: 0.4254 - val_recall: 0.3851 - val_auc: 0.5871\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 1.20685\n",
      "Epoch 372/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8471 - tp: 2127.0000 - fp: 945.0000 - tn: 7983.0000 - fn: 2337.0000 - accuracy: 0.6169 - precision: 0.6924 - recall: 0.4765 - auc: 0.7970 - val_loss: 1.8833 - val_tp: 192.0000 - val_fp: 282.0000 - val_tn: 710.0000 - val_fn: 304.0000 - val_accuracy: 0.4032 - val_precision: 0.4051 - val_recall: 0.3871 - val_auc: 0.5679\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 1.20685\n",
      "Epoch 373/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8295 - tp: 2164.0000 - fp: 918.0000 - tn: 8010.0000 - fn: 2300.0000 - accuracy: 0.6284 - precision: 0.7021 - recall: 0.4848 - auc: 0.8065 - val_loss: 1.9632 - val_tp: 209.0000 - val_fp: 251.0000 - val_tn: 741.0000 - val_fn: 287.0000 - val_accuracy: 0.4355 - val_precision: 0.4543 - val_recall: 0.4214 - val_auc: 0.6016\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 1.20685\n",
      "Epoch 374/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8390 - tp: 2134.0000 - fp: 962.0000 - tn: 7966.0000 - fn: 2330.0000 - accuracy: 0.6248 - precision: 0.6893 - recall: 0.4780 - auc: 0.8012 - val_loss: 1.5351 - val_tp: 159.0000 - val_fp: 225.0000 - val_tn: 767.0000 - val_fn: 337.0000 - val_accuracy: 0.4052 - val_precision: 0.4141 - val_recall: 0.3206 - val_auc: 0.5553\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 1.20685\n",
      "Epoch 375/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8354 - tp: 2119.0000 - fp: 958.0000 - tn: 7970.0000 - fn: 2345.0000 - accuracy: 0.6145 - precision: 0.6887 - recall: 0.4747 - auc: 0.8027 - val_loss: 1.4350 - val_tp: 179.0000 - val_fp: 215.0000 - val_tn: 777.0000 - val_fn: 317.0000 - val_accuracy: 0.4294 - val_precision: 0.4543 - val_recall: 0.3609 - val_auc: 0.5912\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 1.20685\n",
      "Epoch 376/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8391 - tp: 2180.0000 - fp: 929.0000 - tn: 7999.0000 - fn: 2284.0000 - accuracy: 0.6205 - precision: 0.7012 - recall: 0.4884 - auc: 0.8026 - val_loss: 1.7016 - val_tp: 212.0000 - val_fp: 225.0000 - val_tn: 767.0000 - val_fn: 284.0000 - val_accuracy: 0.4657 - val_precision: 0.4851 - val_recall: 0.4274 - val_auc: 0.6277\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 1.20685\n",
      "Epoch 377/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8426 - tp: 2125.0000 - fp: 986.0000 - tn: 7942.0000 - fn: 2339.0000 - accuracy: 0.6129 - precision: 0.6831 - recall: 0.4760 - auc: 0.7991 - val_loss: 1.6547 - val_tp: 195.0000 - val_fp: 272.0000 - val_tn: 720.0000 - val_fn: 301.0000 - val_accuracy: 0.4133 - val_precision: 0.4176 - val_recall: 0.3931 - val_auc: 0.5871\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 1.20685\n",
      "Epoch 378/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8322 - tp: 2179.0000 - fp: 896.0000 - tn: 8032.0000 - fn: 2285.0000 - accuracy: 0.6272 - precision: 0.7086 - recall: 0.4881 - auc: 0.8066 - val_loss: 1.7468 - val_tp: 190.0000 - val_fp: 243.0000 - val_tn: 749.0000 - val_fn: 306.0000 - val_accuracy: 0.4294 - val_precision: 0.4388 - val_recall: 0.3831 - val_auc: 0.5998\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 1.20685\n",
      "Epoch 379/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 70us/step - loss: 0.8417 - tp: 2189.0000 - fp: 959.0000 - tn: 7969.0000 - fn: 2275.0000 - accuracy: 0.6190 - precision: 0.6954 - recall: 0.4904 - auc: 0.8007 - val_loss: 1.8299 - val_tp: 206.0000 - val_fp: 264.0000 - val_tn: 728.0000 - val_fn: 290.0000 - val_accuracy: 0.4375 - val_precision: 0.4383 - val_recall: 0.4153 - val_auc: 0.5995\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 1.20685\n",
      "Epoch 380/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8345 - tp: 2177.0000 - fp: 972.0000 - tn: 7956.0000 - fn: 2287.0000 - accuracy: 0.6214 - precision: 0.6913 - recall: 0.4877 - auc: 0.8034 - val_loss: 1.5985 - val_tp: 158.0000 - val_fp: 214.0000 - val_tn: 778.0000 - val_fn: 338.0000 - val_accuracy: 0.3891 - val_precision: 0.4247 - val_recall: 0.3185 - val_auc: 0.5567\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 1.20685\n",
      "Epoch 381/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8332 - tp: 2150.0000 - fp: 963.0000 - tn: 7965.0000 - fn: 2314.0000 - accuracy: 0.6109 - precision: 0.6907 - recall: 0.4816 - auc: 0.8027 - val_loss: 2.5197 - val_tp: 171.0000 - val_fp: 314.0000 - val_tn: 678.0000 - val_fn: 325.0000 - val_accuracy: 0.3548 - val_precision: 0.3526 - val_recall: 0.3448 - val_auc: 0.5341\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 1.20685\n",
      "Epoch 382/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8307 - tp: 2168.0000 - fp: 965.0000 - tn: 7963.0000 - fn: 2296.0000 - accuracy: 0.6207 - precision: 0.6920 - recall: 0.4857 - auc: 0.8056 - val_loss: 1.9167 - val_tp: 210.0000 - val_fp: 255.0000 - val_tn: 737.0000 - val_fn: 286.0000 - val_accuracy: 0.4456 - val_precision: 0.4516 - val_recall: 0.4234 - val_auc: 0.6051\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 1.20685\n",
      "Epoch 383/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8355 - tp: 2168.0000 - fp: 962.0000 - tn: 7966.0000 - fn: 2296.0000 - accuracy: 0.6212 - precision: 0.6927 - recall: 0.4857 - auc: 0.8029 - val_loss: 3.4291 - val_tp: 144.0000 - val_fp: 344.0000 - val_tn: 648.0000 - val_fn: 352.0000 - val_accuracy: 0.2984 - val_precision: 0.2951 - val_recall: 0.2903 - val_auc: 0.4794\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 1.20685\n",
      "Epoch 384/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8518 - tp: 2090.0000 - fp: 999.0000 - tn: 7929.0000 - fn: 2374.0000 - accuracy: 0.6044 - precision: 0.6766 - recall: 0.4682 - auc: 0.7922 - val_loss: 1.9689 - val_tp: 183.0000 - val_fp: 285.0000 - val_tn: 707.0000 - val_fn: 313.0000 - val_accuracy: 0.3911 - val_precision: 0.3910 - val_recall: 0.3690 - val_auc: 0.5878\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 1.20685\n",
      "Epoch 385/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8370 - tp: 2121.0000 - fp: 958.0000 - tn: 7970.0000 - fn: 2343.0000 - accuracy: 0.6120 - precision: 0.6889 - recall: 0.4751 - auc: 0.8000 - val_loss: 1.5243 - val_tp: 184.0000 - val_fp: 241.0000 - val_tn: 751.0000 - val_fn: 312.0000 - val_accuracy: 0.4194 - val_precision: 0.4329 - val_recall: 0.3710 - val_auc: 0.6030\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 1.20685\n",
      "Epoch 386/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8407 - tp: 2134.0000 - fp: 965.0000 - tn: 7963.0000 - fn: 2330.0000 - accuracy: 0.6136 - precision: 0.6886 - recall: 0.4780 - auc: 0.8003 - val_loss: 1.9308 - val_tp: 225.0000 - val_fp: 246.0000 - val_tn: 746.0000 - val_fn: 271.0000 - val_accuracy: 0.4778 - val_precision: 0.4777 - val_recall: 0.4536 - val_auc: 0.6259\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 1.20685\n",
      "Epoch 387/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8332 - tp: 2199.0000 - fp: 963.0000 - tn: 7965.0000 - fn: 2265.0000 - accuracy: 0.6221 - precision: 0.6954 - recall: 0.4926 - auc: 0.8037 - val_loss: 1.7528 - val_tp: 194.0000 - val_fp: 234.0000 - val_tn: 758.0000 - val_fn: 302.0000 - val_accuracy: 0.4516 - val_precision: 0.4533 - val_recall: 0.3911 - val_auc: 0.6008\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 1.20685\n",
      "Epoch 388/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8353 - tp: 2153.0000 - fp: 969.0000 - tn: 7959.0000 - fn: 2311.0000 - accuracy: 0.6147 - precision: 0.6896 - recall: 0.4823 - auc: 0.8025 - val_loss: 1.9969 - val_tp: 159.0000 - val_fp: 290.0000 - val_tn: 702.0000 - val_fn: 337.0000 - val_accuracy: 0.3508 - val_precision: 0.3541 - val_recall: 0.3206 - val_auc: 0.5288\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 1.20685\n",
      "Epoch 389/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8304 - tp: 2201.0000 - fp: 934.0000 - tn: 7994.0000 - fn: 2263.0000 - accuracy: 0.6369 - precision: 0.7021 - recall: 0.4931 - auc: 0.8080 - val_loss: 1.7337 - val_tp: 205.0000 - val_fp: 256.0000 - val_tn: 736.0000 - val_fn: 291.0000 - val_accuracy: 0.4435 - val_precision: 0.4447 - val_recall: 0.4133 - val_auc: 0.6094\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 1.20685\n",
      "Epoch 390/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8283 - tp: 2157.0000 - fp: 937.0000 - tn: 7991.0000 - fn: 2307.0000 - accuracy: 0.6219 - precision: 0.6972 - recall: 0.4832 - auc: 0.8058 - val_loss: 1.5726 - val_tp: 170.0000 - val_fp: 220.0000 - val_tn: 772.0000 - val_fn: 326.0000 - val_accuracy: 0.4133 - val_precision: 0.4359 - val_recall: 0.3427 - val_auc: 0.5672\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 1.20685\n",
      "Epoch 391/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8248 - tp: 2206.0000 - fp: 923.0000 - tn: 8005.0000 - fn: 2258.0000 - accuracy: 0.6290 - precision: 0.7050 - recall: 0.4942 - auc: 0.8091 - val_loss: 2.3361 - val_tp: 215.0000 - val_fp: 274.0000 - val_tn: 718.0000 - val_fn: 281.0000 - val_accuracy: 0.4415 - val_precision: 0.4397 - val_recall: 0.4335 - val_auc: 0.6110\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 1.20685\n",
      "Epoch 392/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8269 - tp: 2177.0000 - fp: 933.0000 - tn: 7995.0000 - fn: 2287.0000 - accuracy: 0.6228 - precision: 0.7000 - recall: 0.4877 - auc: 0.8093 - val_loss: 2.3752 - val_tp: 209.0000 - val_fp: 280.0000 - val_tn: 712.0000 - val_fn: 287.0000 - val_accuracy: 0.4274 - val_precision: 0.4274 - val_recall: 0.4214 - val_auc: 0.6151\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 1.20685\n",
      "Epoch 393/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8304 - tp: 2166.0000 - fp: 901.0000 - tn: 8027.0000 - fn: 2298.0000 - accuracy: 0.6248 - precision: 0.7062 - recall: 0.4852 - auc: 0.8062 - val_loss: 1.7649 - val_tp: 203.0000 - val_fp: 267.0000 - val_tn: 725.0000 - val_fn: 293.0000 - val_accuracy: 0.4274 - val_precision: 0.4319 - val_recall: 0.4093 - val_auc: 0.5961\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 1.20685\n",
      "Epoch 394/500\n",
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8307 - tp: 2179.0000 - fp: 985.0000 - tn: 7943.0000 - fn: 2285.0000 - accuracy: 0.6142 - precision: 0.6887 - recall: 0.4881 - auc: 0.8054 - val_loss: 1.6008 - val_tp: 205.0000 - val_fp: 260.0000 - val_tn: 732.0000 - val_fn: 291.0000 - val_accuracy: 0.4395 - val_precision: 0.4409 - val_recall: 0.4133 - val_auc: 0.6138\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 1.20685\n",
      "Epoch 395/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8271 - tp: 2202.0000 - fp: 930.0000 - tn: 7998.0000 - fn: 2262.0000 - accuracy: 0.6268 - precision: 0.7031 - recall: 0.4933 - auc: 0.8084 - val_loss: 1.7937 - val_tp: 207.0000 - val_fp: 241.0000 - val_tn: 751.0000 - val_fn: 289.0000 - val_accuracy: 0.4536 - val_precision: 0.4621 - val_recall: 0.4173 - val_auc: 0.6135\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 1.20685\n",
      "Epoch 396/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8194 - tp: 2219.0000 - fp: 943.0000 - tn: 7985.0000 - fn: 2245.0000 - accuracy: 0.6277 - precision: 0.7018 - recall: 0.4971 - auc: 0.8117 - val_loss: 1.8218 - val_tp: 206.0000 - val_fp: 258.0000 - val_tn: 734.0000 - val_fn: 290.0000 - val_accuracy: 0.4476 - val_precision: 0.4440 - val_recall: 0.4153 - val_auc: 0.5950\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 1.20685\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464/4464 [==============================] - 0s 67us/step - loss: 0.8330 - tp: 2209.0000 - fp: 954.0000 - tn: 7974.0000 - fn: 2255.0000 - accuracy: 0.6261 - precision: 0.6984 - recall: 0.4948 - auc: 0.8057 - val_loss: 1.8704 - val_tp: 183.0000 - val_fp: 281.0000 - val_tn: 711.0000 - val_fn: 313.0000 - val_accuracy: 0.3931 - val_precision: 0.3944 - val_recall: 0.3690 - val_auc: 0.5505\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 1.20685\n",
      "Epoch 398/500\n",
      "4464/4464 [==============================] - 0s 66us/step - loss: 0.8304 - tp: 2178.0000 - fp: 970.0000 - tn: 7958.0000 - fn: 2286.0000 - accuracy: 0.6221 - precision: 0.6919 - recall: 0.4879 - auc: 0.8057 - val_loss: 2.1378 - val_tp: 194.0000 - val_fp: 292.0000 - val_tn: 700.0000 - val_fn: 302.0000 - val_accuracy: 0.3972 - val_precision: 0.3992 - val_recall: 0.3911 - val_auc: 0.5496\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 1.20685\n",
      "Epoch 399/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8390 - tp: 2169.0000 - fp: 979.0000 - tn: 7949.0000 - fn: 2295.0000 - accuracy: 0.6125 - precision: 0.6890 - recall: 0.4859 - auc: 0.7991 - val_loss: 2.1526 - val_tp: 193.0000 - val_fp: 271.0000 - val_tn: 721.0000 - val_fn: 303.0000 - val_accuracy: 0.4133 - val_precision: 0.4159 - val_recall: 0.3891 - val_auc: 0.5791\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 1.20685\n",
      "Epoch 400/500\n",
      "4464/4464 [==============================] - 0s 69us/step - loss: 0.8336 - tp: 2178.0000 - fp: 965.0000 - tn: 7963.0000 - fn: 2286.0000 - accuracy: 0.6263 - precision: 0.6930 - recall: 0.4879 - auc: 0.8040 - val_loss: 1.5147 - val_tp: 194.0000 - val_fp: 230.0000 - val_tn: 762.0000 - val_fn: 302.0000 - val_accuracy: 0.4435 - val_precision: 0.4575 - val_recall: 0.3911 - val_auc: 0.6060\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 1.20685\n",
      "Epoch 401/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8270 - tp: 2165.0000 - fp: 925.0000 - tn: 8003.0000 - fn: 2299.0000 - accuracy: 0.6263 - precision: 0.7006 - recall: 0.4850 - auc: 0.8083 - val_loss: 2.1081 - val_tp: 209.0000 - val_fp: 271.0000 - val_tn: 721.0000 - val_fn: 287.0000 - val_accuracy: 0.4294 - val_precision: 0.4354 - val_recall: 0.4214 - val_auc: 0.5968\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 1.20685\n",
      "Epoch 402/500\n",
      "4464/4464 [==============================] - 0s 68us/step - loss: 0.8360 - tp: 2166.0000 - fp: 961.0000 - tn: 7967.0000 - fn: 2298.0000 - accuracy: 0.6160 - precision: 0.6927 - recall: 0.4852 - auc: 0.8012 - val_loss: 2.3618 - val_tp: 228.0000 - val_fp: 249.0000 - val_tn: 743.0000 - val_fn: 268.0000 - val_accuracy: 0.4798 - val_precision: 0.4780 - val_recall: 0.4597 - val_auc: 0.6289\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 1.20685\n",
      "Epoch 403/500\n",
      " 128/4464 [..............................] - ETA: 0s - loss: 0.8510 - tp: 61.0000 - fp: 31.0000 - tn: 225.0000 - fn: 67.0000 - accuracy: 0.5625 - precision: 0.6630 - recall: 0.4766 - auc: 0.7885"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(x=X_train, \n",
    "                    y=Y_train, \n",
    "                    batch_size=128, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    shuffle=True,\n",
    "                    callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "plotted_metrics = ['auc']\n",
    "\n",
    "fig = plt.figure(figsize=(18, 4 * len(plotted_metrics)))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "plotted_metrics = ['auc']\n",
    "\n",
    "for idx, metric in enumerate(plotted_metrics):\n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+1)\n",
    "    plt.title(metric)\n",
    "    plt.plot(history_dict[metric])\n",
    "    \n",
    "    plt.subplot(len(plotted_metrics), 2, 2*idx+2)\n",
    "    plt.title('val_{}'.format(metric))\n",
    "    plt.plot(history_dict['val_{}'.format(metric)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_down_while_steady:  14\n",
      "pred_steady_while_steady:  349\n",
      "pred_up_while_steady:  291\n"
     ]
    }
   ],
   "source": [
    "pred_down_while_steady = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 0 and x[1] == 1])\n",
    "print('pred_down_while_steady: ', pred_down_while_steady)\n",
    "\n",
    "pred_steady_while_steady = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 1 and x[1] == 1])\n",
    "print('pred_steady_while_steady: ', pred_steady_while_steady)\n",
    "\n",
    "pred_up_while_steady = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 2 and x[1] == 1])\n",
    "print('pred_up_while_steady: ', pred_up_while_steady)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_down_while_down:  43\n",
      "pred_steady_while_down:  159\n",
      "pred_up_while_down:  341\n"
     ]
    }
   ],
   "source": [
    "pred_down_while_down = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 0 and x[1] == 0])\n",
    "print('pred_down_while_down: ', pred_down_while_down)\n",
    "\n",
    "pred_steady_while_down = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 1 and x[1] == 0])\n",
    "print('pred_steady_while_down: ', pred_steady_while_down)\n",
    "\n",
    "pred_up_while_down = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 2 and x[1] == 0])\n",
    "print('pred_up_while_down: ', pred_up_while_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_down_while_up:  29\n",
      "pred_steady_while_up:  240\n",
      "pred_up_while_up:  518\n"
     ]
    }
   ],
   "source": [
    "pred_down_while_up = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 0 and x[1] == 2])\n",
    "print('pred_down_while_up: ', pred_down_while_up)\n",
    "\n",
    "pred_steady_while_up = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 1 and x[1] == 2])\n",
    "print('pred_steady_while_up: ', pred_steady_while_up)\n",
    "\n",
    "pred_up_while_up = sum([1 for x in zip(np.argmax(preds, axis=1), labels) if x[0] == 2 and x[1] == 2])\n",
    "print('pred_up_while_up: ', pred_up_while_up)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
